{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove-embedding data\n",
    "def load_emb_vectors(fname):\n",
    "    data = {}\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            data[word] = embedding\n",
    "    return data\n",
    "fname = '../data/glove.6B.50d.txt'\n",
    "words_emb_dict = load_emb_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_emb_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is the output of \"Consolidated Data Cleaning\"\n",
    "data_all = pd.read_csv('../data/cleaned_recipe_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'subtitle', 'carbs', 'fat', 'protein', 'chef_id', 'calories',\n",
       "       'cooking_tips', 'story', 'ingredients_display', 'step_one', 'step_two',\n",
       "       'step_three', 'step_four', 'step_five', 'step_six', 'recipe_tags',\n",
       "       'tag_cuisine_african', 'tag_cuisine_mexican',\n",
       "       'tag_cuisine_middle-eastern', 'tag_cuisine_latin-american',\n",
       "       'tag_cuisine_french', 'tag_cuisine_italian', 'tag_cuisine_nordic',\n",
       "       'tag_cuisine_american', 'tag_cuisine_asian',\n",
       "       'tag_cuisine_mediterranean', 'tag_cuisine_indian',\n",
       "       'tag_cuisine_european'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intruction = data_all[['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cuisine_tags = data_all[['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "       'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "       'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "       'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "       'tag_cuisine_middle-eastern']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preheat oven to 425°F. Pat chicken dry with paper towel and season all over with .5 teaspoon salt and black pepper as desired. Heat 1 tablespoon olive oil in a medium pan over medium-high heat. When oil is shimmering, add chicken and sear until cooked through and no longer pink, about 6 minutes per side. Transfer chicken to a plate or cutting board, and set aside to rest. Wipe pan clean and reserve for cooking salsa.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_intruction.step_one[1083]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and remove punctuation\n",
    "def tokenizer(sent):\n",
    "    #print(sent)\n",
    "    if pd.isnull(sent):\n",
    "        words = []\n",
    "    else:\n",
    "        table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        sent = sent.translate(table)\n",
    "        tokens = word_tokenize(sent)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        #table = str.maketrans('', '', string.punctuation)\n",
    "        #stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "        #re.findall(r'\\d+', 'sdfa')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.findall(r'\\d+', 'sdfa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新写，带上ID\n",
    "def tokenize_dataset(step_n):\n",
    "    \"\"\"returns tokenization for each step, training set tokenizatoin\"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in step_n:\n",
    "        tokens = tokenizer(sample)\n",
    "        token_dataset.append(tokens)\n",
    "    return token_dataset\n",
    "\n",
    "def all_tokens_list(train_data):\n",
    "    \"\"\"returns all tokens of instruction (all steps) for creating vocabulary\"\"\"\n",
    "    all_tokens = []\n",
    "    for columns in train_data.columns:\n",
    "        for sample in train_data[columns]:\n",
    "            all_tokens += sample[:]\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one has been tokenized.\n",
      "step_two has been tokenized.\n",
      "step_three has been tokenized.\n",
      "step_four has been tokenized.\n",
      "step_five has been tokenized.\n",
      "step_six has been tokenized.\n"
     ]
    }
   ],
   "source": [
    "# tokenize each steps\n",
    "data_instruction_tokenized = pd.DataFrame()\n",
    "for steps in data_intruction.columns:\n",
    "    data_instruction_tokenized[steps] = tokenize_dataset(data_intruction[steps])\n",
    "    print(steps, 'has been tokenized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step_one</th>\n",
       "      <th>step_two</th>\n",
       "      <th>step_three</th>\n",
       "      <th>step_four</th>\n",
       "      <th>step_five</th>\n",
       "      <th>step_six</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[preheat, oven, to, place, butter, in, a, smal...</td>\n",
       "      <td>[on, a, baking, sheet, toss, carrots, green, b...</td>\n",
       "      <td>[while, vegetables, roast, mince, garlic, and,...</td>\n",
       "      <td>[pat, steaks, dry, with, paper, towel, and, se...</td>\n",
       "      <td>[once, roasted, remove, vegetables, from, oven...</td>\n",
       "      <td>[once, steaks, have, rested, find, the, direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[in, a, small, pot, combine, rice, cups, water...</td>\n",
       "      <td>[rinse, bell, pepper, and, halve, lengthwise, ...</td>\n",
       "      <td>[pat, steaks, dry, with, paper, towel, and, se...</td>\n",
       "      <td>[while, steaks, sear, heat, teaspoons, canola,...</td>\n",
       "      <td>[return, pan, from, steaks, to, medium, heat, ...</td>\n",
       "      <td>[once, rested, cut, steaks, against, the, grai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[in, a, small, pot, combine, rice, cups, water...</td>\n",
       "      <td>[while, rice, cooks, halve, bok, choy, lengthw...</td>\n",
       "      <td>[heat, sesame, oil, in, a, medium, nonstick, p...</td>\n",
       "      <td>[add, cooked, rice, and, tablespoon, canola, o...</td>\n",
       "      <td>[pat, pork, chops, dry, with, a, paper, towel,...</td>\n",
       "      <td>[once, rested, cut, pork, into, inch, slices, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[preheat, oven, to, in, a, small, pot, combine...</td>\n",
       "      <td>[rinse, all, produce, halve, cucumber, lengthw...</td>\n",
       "      <td>[on, half, of, baking, sheet, toss, chickpeas,...</td>\n",
       "      <td>[while, chickpeas, and, tomatoes, roast, place...</td>\n",
       "      <td>[while, feta, bakes, in, a, large, bowl, whisk...</td>\n",
       "      <td>[divide, quinoa, between, serving, bowls, then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[preheat, oven, to, pat, chicken, dry, with, p...</td>\n",
       "      <td>[while, chicken, cooks, halve, lime, cut, half...</td>\n",
       "      <td>[return, pan, from, chicken, to, medium, high,...</td>\n",
       "      <td>[stack, tortillas, wrap, in, foil, and, place,...</td>\n",
       "      <td>[stir, chipotle, paste, and, teaspoon, salt, i...</td>\n",
       "      <td>[divide, warmed, tortillas, between, serving, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            step_one  \\\n",
       "0  [preheat, oven, to, place, butter, in, a, smal...   \n",
       "1  [in, a, small, pot, combine, rice, cups, water...   \n",
       "2  [in, a, small, pot, combine, rice, cups, water...   \n",
       "3  [preheat, oven, to, in, a, small, pot, combine...   \n",
       "4  [preheat, oven, to, pat, chicken, dry, with, p...   \n",
       "\n",
       "                                            step_two  \\\n",
       "0  [on, a, baking, sheet, toss, carrots, green, b...   \n",
       "1  [rinse, bell, pepper, and, halve, lengthwise, ...   \n",
       "2  [while, rice, cooks, halve, bok, choy, lengthw...   \n",
       "3  [rinse, all, produce, halve, cucumber, lengthw...   \n",
       "4  [while, chicken, cooks, halve, lime, cut, half...   \n",
       "\n",
       "                                          step_three  \\\n",
       "0  [while, vegetables, roast, mince, garlic, and,...   \n",
       "1  [pat, steaks, dry, with, paper, towel, and, se...   \n",
       "2  [heat, sesame, oil, in, a, medium, nonstick, p...   \n",
       "3  [on, half, of, baking, sheet, toss, chickpeas,...   \n",
       "4  [return, pan, from, chicken, to, medium, high,...   \n",
       "\n",
       "                                           step_four  \\\n",
       "0  [pat, steaks, dry, with, paper, towel, and, se...   \n",
       "1  [while, steaks, sear, heat, teaspoons, canola,...   \n",
       "2  [add, cooked, rice, and, tablespoon, canola, o...   \n",
       "3  [while, chickpeas, and, tomatoes, roast, place...   \n",
       "4  [stack, tortillas, wrap, in, foil, and, place,...   \n",
       "\n",
       "                                           step_five  \\\n",
       "0  [once, roasted, remove, vegetables, from, oven...   \n",
       "1  [return, pan, from, steaks, to, medium, heat, ...   \n",
       "2  [pat, pork, chops, dry, with, a, paper, towel,...   \n",
       "3  [while, feta, bakes, in, a, large, bowl, whisk...   \n",
       "4  [stir, chipotle, paste, and, teaspoon, salt, i...   \n",
       "\n",
       "                                            step_six  \n",
       "0  [once, steaks, have, rested, find, the, direct...  \n",
       "1  [once, rested, cut, steaks, against, the, grai...  \n",
       "2  [once, rested, cut, pork, into, inch, slices, ...  \n",
       "3  [divide, quinoa, between, serving, bowls, then...  \n",
       "4  [divide, warmed, tortillas, between, serving, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instruction_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_instruction_tokenized.shape[0] == data_cuisine_tags.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, test_data, y_train, test_tags = train_test_split(data_instruction_tokenized, data_cuisine_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All tokens from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form all tokens list\n",
    "all_train_tokens = all_tokens_list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide which tag to predict for trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.sum()/data_cuisine_tags.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tag: tag_cuisine_american, which 27.3525% are 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary and indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3191"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_counter.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index 0 for unk and 1 for pad\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2517 ; token tap\n",
      "Token tap; token id 2517\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = len(list(set(all_train_tokens)))\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print(\"Token id {}; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print(\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emb_weight(words_emb_dict, id2token):\n",
    "    vocab_size = len(id2token)\n",
    "    emb_dim = len(words_emb_dict['a'])\n",
    "    emb_weight = np.zeros([vocab_size, emb_dim])\n",
    "    for i in range(2,vocab_size):\n",
    "        emb = words_emb_dict.get(id2token[i], None)\n",
    "        if emb is not None:\n",
    "            emb_weight[i] = emb\n",
    "    return emb_weight\n",
    "\n",
    "emb_weight = build_emb_weight(words_emb_dict, id2token)\n",
    "\n",
    "# dump data about vocab and embedding\n",
    "# pkl.dump(token2id, open(\"hw2_data/token2id.p\", \"wb\"))\n",
    "# pkl.dump(id2token, open(\"hw2_data/id2token.p\", \"wb\"))\n",
    "# pkl.dump(emb_weight, open(\"hw2_data/embedding_weight.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053554650798621983"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.sum(emb_weight,1)==0)/emb_weight.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct data strcuture for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 2031\n",
      "Val dataset size is 226\n",
      "Test dataset size is 251\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    \"\"\"returns [[[step1 indices],[step2 indices],...,[step6 indices]],[],[],...]\"\"\"\n",
    "    recipie_indices_data = []\n",
    "    UNK_IDX = 1\n",
    "    for recipie in tokens_data.iterrows():\n",
    "        step_indices_data = []\n",
    "        for step in recipie[1]:\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in step]\n",
    "            step_indices_data.append(index_list)\n",
    "        recipie_indices_data.append(step_indices_data)\n",
    "    return recipie_indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data, token2id)\n",
    "val_data_indices = token2index_dataset(val_data, token2id)\n",
    "test_data_indices = token2index_dataset(test_data, token2id)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, tags_list, max_sent_len):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data_list: list of recipie tokens \n",
    "        @param target_list: list of single tag, i.e. 'tag_cuisine_american'\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.tags_list = tags_list\n",
    "        assert (len(self.data_list) == len(self.tags_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when  you call recipie[i]\n",
    "        \"\"\"\n",
    "        recipie = self.data_list[key]\n",
    "        step1_idx = recipie[0][:max_sent_len[0]]\n",
    "        step2_idx = recipie[1][:max_sent_len[1]]\n",
    "        step3_idx = recipie[2][:max_sent_len[2]]\n",
    "        step4_idx = recipie[3][:max_sent_len[3]]       \n",
    "        step5_idx = recipie[4][:max_sent_len[4]]\n",
    "        step6_idx = recipie[5][:max_sent_len[5]]\n",
    "        label = self.tags_list[key]\n",
    "        return [[step1_idx, step2_idx, step3_idx, step4_idx, step5_idx, step6_idx], \n",
    "                [len(step1_idx),len(step2_idx), len(step3_idx),len(step4_idx), len(step5_idx),len(step6_idx)], \n",
    "                label]\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    steps_dict = defaultdict(list)\n",
    "    label_dict = defaultdict(list)\n",
    "    length_dict = defaultdict(list)\n",
    "    max_sent_len = []\n",
    "    for datum in batch:\n",
    "        for idx, task_label in enumerate(datum[-1]):\n",
    "            label_dict[idx].append(task_label)\n",
    "        for i in range(6):\n",
    "            length_dict[i].append(datum[1][i])\n",
    "    \n",
    "    # padding\n",
    "    for i in range(6):\n",
    "        max_sent_len.append(max(length_dict[i]))\n",
    "    \n",
    "    for datum in batch:\n",
    "        for i, step in enumerate(datum[0]):\n",
    "            padded_vec = np.pad(np.array(step), \n",
    "                                pad_width=((0, max_sent_len[i]-datum[1][i])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "            steps_dict[i].append(padded_vec)\n",
    "    \n",
    "    for key in length_dict.keys():\n",
    "        length_dict[key] = torch.LongTensor(length_dict[key])\n",
    "        steps_dict[key] = torch.from_numpy(np.array(steps_dict[key]).astype(np.int))\n",
    "    for key in label_dict.keys():\n",
    "        label_dict[key] = torch.LongTensor(label_dict[key])\n",
    "        \n",
    "    return [steps_dict, length_dict, label_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "def create_dataset_obj(train,val,test,train_targets,val_targets,test_targets,\n",
    "                       BATCH_SIZE,max_sent_len,collate_func):\n",
    "    collate_func=partial(collate_func)\n",
    "    train_dataset = IntructionDataset(train, train_targets, max_sent_len)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = IntructionDataset(val, val_targets, max_sent_len)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=False)\n",
    "\n",
    "    test_dataset = IntructionDataset(test, test_targets, max_sent_len)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, trainable=False):\n",
    "    vocab_size, emb_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if trainable == False:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, vocab_size, emb_dim\n",
    "\n",
    "class two_stage_RNN(nn.Module):\n",
    "    def __init__(self, rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, cuda_on, num_tasks, num_classes):\n",
    "        \n",
    "        super(two_stage_RNN, self).__init__()\n",
    "        \n",
    "        self.num_tasks = num_tasks\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        self.embedding, vocab_size, emb_dim = create_emb_layer(weights_matrix, trainable=False)\n",
    "        \n",
    "        # module for steps in the fisrt stage\n",
    "#         self.hidden_stage1, self.hidden_stage2 = self.init_hidden(batch_size, cuda_on)\n",
    "        rnn_common = rnn_1(emb_dim, hidden_dim1, num_layers=1, \n",
    "                           batch_first=True, bidirectional=bi)\n",
    "        self.rnn_each_step = nn.ModuleList([])\n",
    "        for i in range(6):\n",
    "            self.rnn_each_step.append(rnn_common)\n",
    "        \n",
    "        # module for the second stage\n",
    "        if bi:\n",
    "            self.steps_rnn = rnn_2(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=False)\n",
    "        else:\n",
    "            self.steps_rnn = rnn_2(hidden_dim1, hidden_dim2, num_layers=1, batch_first=False)\n",
    "        \n",
    "        # module for interaction\n",
    "        self.classifiers_mlt = nn.ModuleList([])\n",
    "        for i in range(self.num_tasks):\n",
    "            self.classifiers_mlt.append(nn.Linear(hidden_dim2, num_classes))\n",
    "        #self.linear = nn.Linear(hidden_dim2, num_classes)\n",
    "        \n",
    "#     def init_hidden(self, batch_size, cuda_on):\n",
    "#         # Function initializes the activation of recurrent neural net at timestep 0\n",
    "#         # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "#         hidden_stage1 = {}\n",
    "#         for i in range(6):\n",
    "#             hidden_stage1[str(i)] = Variable(torch.zeros(1, batch_size, self.hidden_dim1), \n",
    "#                                              requires_grad=True)\n",
    "#         hidden_stage2 = Variable(torch.zeros(1, batch_size, self.hidden_dim2), \n",
    "#                                  requires_grad=True)\n",
    "#         if torch.cuda.is_available() and cuda_on:\n",
    "#             for i in range(6):\n",
    "#                 hidden_stage1[str(i)] = hidden_stage1[str(i)].cuda()\n",
    "#             hidden_stage2 = hidden_stage2.cuda()\n",
    "#         return hidden_stage1, hidden_stage2\n",
    "\n",
    "    def forward(self, steps, lengths):\n",
    "        # first stage\n",
    "        output_each_step = []\n",
    "        for i in range(6):\n",
    "            #print('1', steps[str(i)].size())\n",
    "            rnn_input = steps[i]\n",
    "            emb = self.embedding(rnn_input) # embedding\n",
    "            #print('2', emb.size())\n",
    "            #(torch.ones(emb.size(0))*torch.max(lengths[str(i)])).numpy()\n",
    "            #emb = nn.utils.rnn.pack_padded_sequence(emb, np.array([80,]*10), batch_first=True)\n",
    "            #print('3', emb.size())\n",
    "            #torch.ones(emb.size(0))*step[i][1]\n",
    "            output, _ = self.rnn_each_step[i](emb) #, self.hidden_stage1[str(i)]\n",
    "            if bi:\n",
    "                output_size = output.size()\n",
    "                output = output.view(output_size[0], output_size[1], 2, self.hidden_dim1)\n",
    "            #print('3', output.size())\n",
    "            # undo packing\n",
    "            #output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "            if bi:\n",
    "                output_each_step.append(torch.cat((output[:,-1,0,:],output[:,0,1,:]),1))\n",
    "            else:\n",
    "                output_each_step.append(output[:,-1,:])\n",
    "            #print('4', output_each_step[-1].size())\n",
    "        \n",
    "        #second stage\n",
    "        output1 = torch.stack(output_each_step, 0)\n",
    "        #print('5', output1.size())\n",
    "        output, _ = self.steps_rnn(output1) #, self.hidden_stage2\n",
    "        output = output[-1,:,:]\n",
    "        logits = {}\n",
    "        for i in range(self.num_tasks):\n",
    "            logits[i] = self.classifiers_mlt[i](output)\n",
    "        #logits = torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    logits_all_dict = defaultdict(list)\n",
    "    labels_all_dict = defaultdict(list)\n",
    "    model.eval()\n",
    "    for steps_batch, lengths_batch, labels_batch in loader:\n",
    "        for step_id in range(6):\n",
    "            lengths_batch[step_id] = lengths_batch[step_id].cuda()\n",
    "            steps_batch[step_id] = steps_batch[step_id].cuda() \n",
    "        logits = model(steps_batch, lengths_batch)\n",
    "        for i in labels_batch.keys():\n",
    "            logits_all_dict[i].extend(list(logits[i].cpu().detach().numpy()))\n",
    "            labels_all_dict[i].extend(list(labels_batch[i].numpy()))\n",
    "    auc = []\n",
    "    acc = []\n",
    "    for i in labels_all_dict.keys():\n",
    "        logits_all_dict[i] = np.array(logits_all_dict[i])\n",
    "        labels_all_dict[i] = np.array(labels_all_dict[i])\n",
    "        auc.append(roc_auc_score(labels_all_dict[i], logits_all_dict[i])) \n",
    "        predicts = (logits_all_dict[i] > 0.5).astype(int)\n",
    "        acc.append(np.mean(predicts==labels_all_dict[i]))\n",
    "    return auc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_cuisine_indian            0.023525  85% auc\n",
    "tag_cuisine_nordic            0.000399\n",
    "tag_cuisine_european          0.012360\n",
    "tag_cuisine_asian             0.182217  98% auc\n",
    "tag_cuisine_mexican           0.013557\n",
    "tag_cuisine_latin-american    0.094896  90% auc\n",
    "tag_cuisine_french            0.077352  72% auc\n",
    "tag_cuisine_italian           0.233254  80% auc\n",
    "tag_cuisine_african           0.003987\n",
    "tag_cuisine_mediterranean     0.076555  88% auc\n",
    "tag_cuisine_american          0.273525  80% auc\n",
    "tag_cuisine_middle-eastern    0.046252  87% auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.sum()/data_cuisine_tags.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_predicted = ['tag_cuisine_american', 'tag_cuisine_italian', 'tag_cuisine_asian', \n",
    "                 'tag_cuisine_latin-american','tag_cuisine_mediterranean']\n",
    "# train_targets = list(train_tags[tag_predicted])\n",
    "# val_targets = list(val_tags[tag_predicted])\n",
    "# test_targets = list(test_tags[tag_predicted])\n",
    "train_targets = []\n",
    "val_targets = []\n",
    "test_targets = []\n",
    "for row in train_tags[tag_predicted].iterrows():\n",
    "    train_targets.append(list(row[1].values))\n",
    "    \n",
    "for row in val_tags[tag_predicted].iterrows():\n",
    "    val_targets.append(list(row[1].values))\n",
    "    \n",
    "for row in test_tags[tag_predicted].iterrows():\n",
    "    test_targets.append(list(row[1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_tags[tag_predicted].value_counts())\n",
    "# print(val_tags[tag_predicted].value_counts())\n",
    "# print(test_tags[tag_predicted].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_types = {\n",
    "    'rnn': nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "}\n",
    "\n",
    "params = dict(\n",
    "    rnn1_type = 'gru',\n",
    "    rnn2_type = 'gru',\n",
    "    bi = False,\n",
    "    hidden_dim1 = 30,\n",
    "    hidden_dim2 = 30,\n",
    "    num_classes = 1,\n",
    "    \n",
    "    num_epochs = 20,\n",
    "    batch_size = 50,\n",
    "    learning_rate = 0.01,\n",
    "    cuda_on = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = params['batch_size']\n",
    "max_sent_len = np.array([94, 86, 87, 90, 98, 91])\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataset_obj(train_data_indices, val_data_indices,\n",
    "                                                           test_data_indices, train_targets,\n",
    "                                                           val_targets, test_targets,\n",
    "                                                           BATCH_SIZE, max_sent_len, \n",
    "                                                           collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20, Step:1/41, TrainLoss:0.390651, ValAUC:[0.67574229691876742, 0.86794258373205746, 0.9897161661867544, 0.97322767187834658, 0.59490291262135919] ValAcc:[0.77433628318584069, 0.73165478894196878, 0.72601613282167754, 0.87708512804448269, 0.91150442477876104]\n",
      "1/20, Step:11/41, TrainLoss:0.457195, ValAUC:[0.70509803921568626, 0.87698032961190853, 0.98916769505004809, 0.971728421503534, 0.59126213592233012] ValAcc:[0.77433628318584069, 0.7543660427598089, 0.72311849009319451, 0.87356096796930061, 0.91150442477876104]\n",
      "1/20, Step:21/41, TrainLoss:0.517338, ValAUC:[0.70442577030812326, 0.88293460925039868, 0.99012751953928424, 0.97451274362818596, 0.60097087378640779] ValAcc:[0.77433628318584069, 0.75663716814159288, 0.72022084736471137, 0.84889184744302604, 0.91150442477876104]\n",
      "1/20, Step:31/41, TrainLoss:0.367780, ValAUC:[0.69254901960784321, 0.89611908559276987, 0.99040175510763739, 0.9779396016277575, 0.57742718446601948] ValAcc:[0.77433628318584069, 0.7543660427598089, 0.70283499099381319, 0.82774688699193355, 0.91150442477876104]\n",
      "1/20, Step:41/41, TrainLoss:0.456639, ValAUC:[0.69221288515406165, 0.89994683678894205, 0.98820787056081172, 0.97965303062754328, 0.60266990291262135] ValAcc:[0.77433628318584069, 0.75663716814159288, 0.70573263372229622, 0.83479520714229771, 0.91150442477876104]\n",
      "Epoch: [1/20], trainAUC: [0.75101555497730355, 0.91081518336951128, 0.99190955816108939, 0.96508281198834944, 0.6995753715498938], trainAcc: [0.72082717872968982, 0.76661742983751846, 0.70420593067425363, 0.84442834732255645, 0.92762186115214185]\n",
      "Epoch: [1/20], ValAUC: [0.69221288515406165, 0.89994683678894205, 0.98820787056081172, 0.97965303062754328, 0.60266990291262135], ValAcc: [0.77433628318584069, 0.75663716814159288, 0.70573263372229622, 0.83479520714229771, 0.91150442477876104]\n",
      "2/20, Step:1/41, TrainLoss:0.202802, ValAUC:[0.6921008403361345, 0.90058479532163749, 0.98807075277663514, 0.98050974512743627, 0.60364077669902916] ValAcc:[0.77433628318584069, 0.75663716814159288, 0.70863027645077925, 0.83479520714229771, 0.91150442477876104]\n",
      "2/20, Step:11/41, TrainLoss:0.250243, ValAUC:[0.70005602240896359, 0.90494417862838916, 0.98779651720828199, 0.97901049475262369, 0.59490291262135919] ValAcc:[0.77433628318584069, 0.74301041585088889, 0.72022084736471137, 0.82774688699193355, 0.91150442477876104]\n",
      "2/20, Step:21/41, TrainLoss:0.175493, ValAUC:[0.70352941176470585, 0.91589580010632643, 0.98656245715069246, 0.97644035125294493, 0.60339805825242721] ValAcc:[0.77433628318584069, 0.7180280366512648, 0.70863027645077925, 0.86651264781893644, 0.91150442477876104]\n",
      "2/20, Step:31/41, TrainLoss:0.250199, ValAUC:[0.70599439775910366, 0.91951089845826683, 0.98642533936651589, 0.97494110087813235, 0.62014563106796117] ValAcc:[0.77433628318584069, 0.68623228130628866, 0.71732320463622834, 0.86298848774375436, 0.91150442477876104]\n",
      "2/20, Step:41/41, TrainLoss:0.624435, ValAUC:[0.7124929971988796, 0.9166400850611377, 0.98669957493486904, 0.98350824587706143, 0.654126213592233] ValAcc:[0.77433628318584069, 0.66806327825201661, 0.72601613282167754, 0.84536768736784396, 0.91150442477876104]\n",
      "Epoch: [2/20], trainAUC: [0.76283960254816363, 0.92513461731285684, 0.98690589615456914, 0.96650178952024768, 0.75339774975807727], trainAcc: [0.71887006931701902, 0.66842450146801391, 0.71543803686871221, 0.85084004430587346, 0.92762186115214185]\n",
      "Epoch: [2/20], ValAUC: [0.7124929971988796, 0.9166400850611377, 0.98669957493486904, 0.98350824587706143, 0.654126213592233], ValAcc: [0.77433628318584069, 0.66806327825201661, 0.72601613282167754, 0.84536768736784396, 0.91150442477876104]\n",
      "3/20, Step:1/41, TrainLoss:0.464407, ValAUC:[0.71439775910364145, 0.9152578415736311, 0.98669957493486904, 0.98457913900192762, 0.65703883495145632] ValAcc:[0.77433628318584069, 0.67487665439736866, 0.72311849009319451, 0.84889184744302604, 0.91150442477876104]\n",
      "3/20, Step:11/41, TrainLoss:0.238210, ValAUC:[0.72044817927170868, 0.91419457735247212, 0.98807075277663514, 0.9856500321267937, 0.67305825242718442] ValAcc:[0.77433628318584069, 0.69304565745164071, 0.71732320463622834, 0.82069856684156939, 0.91150442477876104]\n",
      "3/20, Step:21/41, TrainLoss:0.169992, ValAUC:[0.72784313725490191, 0.91004784688995222, 0.98875634169751814, 0.98886271150139216, 0.69320388349514561] ValAcc:[0.77433628318584069, 0.66579215287023263, 0.71732320463622834, 0.85241600751820812, 0.91150442477876104]\n",
      "3/20, Step:31/41, TrainLoss:0.241395, ValAUC:[0.73523809523809525, 0.9009037745879851, 0.98656245715069235, 0.98457913900192762, 0.72014563106796114] ValAcc:[0.77433628318584069, 0.71121466050591275, 0.71152791917926228, 0.87003680789411852, 0.91150442477876104]\n",
      "3/20, Step:41/41, TrainLoss:0.272286, ValAUC:[0.72638655462184865, 0.91185539606592236, 0.98834498834498841, 0.98822017562647246, 0.74781553398058243] ValAcc:[0.77433628318584069, 0.6816900305427207, 0.7318114182786436, 0.84184352729266188, 0.91150442477876104]\n",
      "Epoch: [3/20], trainAUC: [0.76970875377068459, 0.93299350422347427, 0.99179127210976947, 0.96545622712832269, 0.78868235192166036], trainAcc: [0.72082717872968982, 0.67577584369888588, 0.71668604866809649, 0.84763419581421495, 0.92762186115214185]\n",
      "Epoch: [3/20], ValAUC: [0.72638655462184865, 0.91185539606592236, 0.98834498834498841, 0.98822017562647246, 0.74781553398058243], ValAcc: [0.77433628318584069, 0.6816900305427207, 0.7318114182786436, 0.84184352729266188, 0.91150442477876104]\n",
      "4/20, Step:1/41, TrainLoss:0.166414, ValAUC:[0.72257703081232494, 0.91206804891015425, 0.9883449883449883, 0.98864853287641896, 0.75339805825242723] ValAcc:[0.77433628318584069, 0.67260552901558457, 0.72891377555016057, 0.83479520714229771, 0.91150442477876104]\n",
      "4/20, Step:11/41, TrainLoss:0.448662, ValAUC:[0.73109243697478987, 0.91653375863902187, 0.9883449883449883, 0.98372242450203473, 0.71262135922330105] ValAcc:[0.77433628318584069, 0.69304565745164071, 0.69993734826533005, 0.78545696608974858, 0.91150442477876104]\n",
      "4/20, Step:21/41, TrainLoss:0.466281, ValAUC:[0.745546218487395, 0.92174375332270064, 0.99067599067599066, 0.98629256800171339, 0.74320388349514555] ValAcc:[0.76219750959354682, 0.67260552901558457, 0.71442556190774531, 0.84184352729266188, 0.91150442477876104]\n",
      "4/20, Step:31/41, TrainLoss:0.025282, ValAUC:[0.73669467787114851, 0.90717703349282308, 0.98820787056081172, 0.98050974512743627, 0.74077669902912624] ValAcc:[0.77433628318584069, 0.68396115592450468, 0.70863027645077925, 0.84536768736784396, 0.91150442477876104]\n",
      "4/20, Step:41/41, TrainLoss:0.064673, ValAUC:[0.7267226890756302, 0.90707070707070714, 0.98999040175510766, 0.95523666738059543, 0.69684466019417479] ValAcc:[0.77433628318584069, 0.71348578588769673, 0.70573263372229622, 0.84536768736784396, 0.91150442477876104]\n",
      "Epoch: [4/20], trainAUC: [0.77363484353466128, 0.92760745672869771, 0.99272621802225713, 0.95387748536499894, 0.76715123416670283], trainAcc: [0.72082717872968982, 0.70596885643282448, 0.6985898775770244, 0.85204223749024532, 0.92762186115214185]\n",
      "Epoch: [4/20], ValAUC: [0.7267226890756302, 0.90707070707070714, 0.98999040175510766, 0.95523666738059543, 0.69684466019417479], ValAcc: [0.77433628318584069, 0.71348578588769673, 0.70573263372229622, 0.84536768736784396, 0.91150442477876104]\n",
      "5/20, Step:1/41, TrainLoss:0.253493, ValAUC:[0.7258263305322129, 0.90940988835725678, 0.99053887289181408, 0.95180980938102389, 0.6762135922330097] ValAcc:[0.77433628318584069, 0.71348578588769673, 0.69414206280836399, 0.84889184744302604, 0.91150442477876104]\n",
      "5/20, Step:11/41, TrainLoss:0.500021, ValAUC:[0.73075630252100843, 0.90334928229665068, 0.98491704374057321, 0.87748982651531371, 0.71723300970873793] ValAcc:[0.77433628318584069, 0.63399639752525649, 0.71152791917926228, 0.86298848774375436, 0.91150442477876104]\n",
      "5/20, Step:21/41, TrainLoss:0.317691, ValAUC:[0.74196078431372536, 0.8926103136629453, 0.97518168106403402, 0.86892268151638463, 0.74368932038834956] ValAcc:[0.77190852846738189, 0.65897877672488059, 0.72891377555016057, 0.86298848774375436, 0.91150442477876104]\n",
      "5/20, Step:31/41, TrainLoss:0.156932, ValAUC:[0.72784313725490191, 0.92418926103136623, 0.98382010146716037, 0.96658813450417647, 0.76165048543689318] ValAcc:[0.76948077374892321, 0.6816900305427207, 0.70573263372229622, 0.78545696608974858, 0.91150442477876104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/20, Step:41/41, TrainLoss:0.608378, ValAUC:[0.7366946778711484, 0.92982456140350855, 0.98697381050322219, 0.9638038123795245, 0.73422330097087374] ValAcc:[0.75248649071971185, 0.67941890516093661, 0.68834677735139793, 0.81365024669120523, 0.91150442477876104]\n",
      "Epoch: [5/20], trainAUC: [0.78613592775705698, 0.94176564799232543, 0.98930726503205069, 0.96022554274454386, 0.80709013966520793], trainAcc: [0.70517030342832332, 0.67446310401480158, 0.68330173303456687, 0.83641372609341036, 0.92762186115214185]\n",
      "Epoch: [5/20], ValAUC: [0.7366946778711484, 0.92982456140350855, 0.98697381050322219, 0.9638038123795245, 0.73422330097087374], ValAcc: [0.75248649071971185, 0.67941890516093661, 0.68834677735139793, 0.81365024669120523, 0.91150442477876104]\n",
      "6/20, Step:1/41, TrainLoss:0.293409, ValAUC:[0.74151260504201677, 0.93003721424774055, 0.98738516385575203, 0.96915827800385523, 0.74126213592233015] ValAcc:[0.74034771712741798, 0.67941890516093661, 0.68834677735139793, 0.82069856684156939, 0.91150442477876104]\n",
      "6/20, Step:11/41, TrainLoss:0.527255, ValAUC:[0.7249299719887955, 0.92886762360446573, 0.98573975044563278, 0.98307988862711504, 0.74393203883495151] ValAcc:[0.7330644529720417, 0.67033440363380059, 0.74629963192105886, 0.82069856684156939, 0.91150442477876104]\n",
      "6/20, Step:21/41, TrainLoss:0.145188, ValAUC:[0.72571428571428565, 0.93088782562466776, 0.98272315919374731, 0.97901049475262369, 0.73470873786407764] ValAcc:[0.77433628318584069, 0.6816900305427207, 0.75789020283499098, 0.83479520714229771, 0.91150442477876104]\n",
      "6/20, Step:31/41, TrainLoss:0.198623, ValAUC:[0.73322128851540624, 0.92886762360446573, 0.9883449883449883, 0.98158063825230246, 0.72742718446601939] ValAcc:[0.77433628318584069, 0.68850340668807264, 0.72311849009319451, 0.84184352729266188, 0.91150442477876104]\n",
      "6/20, Step:41/41, TrainLoss:0.443113, ValAUC:[0.74464985994397759, 0.9232323232323234, 0.98916769505004798, 0.96680231312914966, 0.71067961165048543] ValAcc:[0.77190852846738189, 0.67033440363380059, 0.69993734826533005, 0.84536768736784396, 0.91150442477876104]\n",
      "Epoch: [6/20], trainAUC: [0.80366178043773662, 0.94745521111951192, 0.99266302410442875, 0.96649317224778675, 0.81785389314961654], trainAcc: [0.71952243912124259, 0.67183762464663299, 0.69796587167733226, 0.85885466553501966, 0.92762186115214185]\n",
      "Epoch: [6/20], ValAUC: [0.74464985994397759, 0.9232323232323234, 0.98916769505004798, 0.96680231312914966, 0.71067961165048543], ValAcc: [0.77190852846738189, 0.67033440363380059, 0.69993734826533005, 0.84536768736784396, 0.91150442477876104]\n",
      "7/20, Step:1/41, TrainLoss:0.422625, ValAUC:[0.74644257703081229, 0.92344497607655507, 0.98916769505004809, 0.96680231312914966, 0.71067961165048543] ValAcc:[0.77190852846738189, 0.67033440363380059, 0.69993734826533005, 0.84536768736784396, 0.91150442477876104]\n",
      "7/20, Step:11/41, TrainLoss:0.448085, ValAUC:[0.75137254901960782, 0.92971823498139283, 0.9869738105032223, 0.98136645962732916, 0.73276699029126213] ValAcc:[0.76705301903046441, 0.64308089905239252, 0.69993734826533005, 0.82069856684156939, 0.91150442477876104]\n",
      "7/20, Step:21/41, TrainLoss:0.256725, ValAUC:[0.73983193277310921, 0.92610313662945243, 0.98327163033045384, 0.98179481687727554, 0.74587378640776703] ValAcc:[0.77433628318584069, 0.65670765134309661, 0.71152791917926228, 0.80307776646565898, 0.91150442477876104]\n",
      "7/20, Step:31/41, TrainLoss:0.218479, ValAUC:[0.73714285714285721, 0.92312599681020746, 0.98094062799945148, 0.97965303062754328, 0.73033980582524272] ValAcc:[0.67479833972903125, 0.69985903359699275, 0.72311849009319451, 0.79602944631529482, 0.91150442477876104]\n",
      "7/20, Step:41/41, TrainLoss:0.177207, ValAUC:[0.73120448179271713, 0.93024986709197244, 0.98477992595639663, 0.9787963161276505, 0.74101941747572808] ValAcc:[0.76948077374892321, 0.6816900305427207, 0.72891377555016057, 0.83831936721747979, 0.91150442477876104]\n",
      "Epoch: [7/20], trainAUC: [0.81808555237516989, 0.945047410767759, 0.99487319087155757, 0.97499554774256181, 0.82821684937244533], trainAcc: [0.70951943545648066, 0.68207699418249046, 0.71231800737025153, 0.85084004430587346, 0.92762186115214185]\n",
      "Epoch: [7/20], ValAUC: [0.73120448179271713, 0.93024986709197244, 0.98477992595639663, 0.9787963161276505, 0.74101941747572808], ValAcc: [0.76948077374892321, 0.6816900305427207, 0.72891377555016057, 0.83831936721747979, 0.91150442477876104]\n",
      "8/20, Step:1/41, TrainLoss:0.169106, ValAUC:[0.72974789915966387, 0.93035619351408827, 0.98491704374057321, 0.97901049475262369, 0.74174757281553405] ValAcc:[0.77190852846738189, 0.67714777977915264, 0.72891377555016057, 0.84536768736784396, 0.91150442477876104]\n",
      "8/20, Step:11/41, TrainLoss:0.592498, ValAUC:[0.71294117647058819, 0.92450824029771395, 0.98697381050322219, 0.97237095737845358, 0.73907766990291257] ValAcc:[0.76948077374892321, 0.66352102748844854, 0.71732320463622834, 0.84536768736784396, 0.91150442477876104]\n",
      "8/20, Step:21/41, TrainLoss:0.263601, ValAUC:[0.70935574229691867, 0.9332270069112174, 0.98711092828739888, 0.94217177125722851, 0.75582524271844664] ValAcc:[0.76948077374892321, 0.65897877672488059, 0.69124442007988096, 0.84536768736784396, 0.91150442477876104]\n",
      "8/20, Step:31/41, TrainLoss:0.163992, ValAUC:[0.71686274509803916, 0.92812333864965446, 0.9876593994241053, 0.95009638038123789, 0.73713592233009706] ValAcc:[0.77190852846738189, 0.68396115592450468, 0.69993734826533005, 0.84184352729266188, 0.91150442477876104]\n",
      "8/20, Step:41/41, TrainLoss:0.154156, ValAUC:[0.73053221288515402, 0.93343965975544929, 0.98848210612916498, 0.97793960162775762, 0.7439320388349514] ValAcc:[0.76462526431200561, 0.68396115592450468, 0.73470906100712663, 0.84889184744302604, 0.91150442477876104]\n",
      "Epoch: [8/20], trainAUC: [0.81980464721812629, 0.94556094837795279, 0.99332412970632666, 0.97643463224353555, 0.85260409896442646], trainAcc: [0.7108241750649279, 0.68023915862477247, 0.72230210176532583, 0.85725174128919035, 0.92762186115214185]\n",
      "Epoch: [8/20], ValAUC: [0.73053221288515402, 0.93343965975544929, 0.98848210612916498, 0.97793960162775762, 0.7439320388349514], ValAcc: [0.76462526431200561, 0.68396115592450468, 0.73470906100712663, 0.84889184744302604, 0.91150442477876104]\n",
      "9/20, Step:1/41, TrainLoss:0.221570, ValAUC:[0.73120448179271702, 0.93429027113237639, 0.98820787056081172, 0.97815378025273081, 0.74563106796116507] ValAcc:[0.76462526431200561, 0.68396115592450468, 0.73470906100712663, 0.84889184744302604, 0.91150442477876104]\n",
      "9/20, Step:11/41, TrainLoss:0.157304, ValAUC:[0.73938375350140051, 0.934609250398724, 0.9890305772658714, 0.96937245662882854, 0.73932038834951463] ValAcc:[0.76705301903046441, 0.64989427519774456, 0.71732320463622834, 0.84536768736784396, 0.91150442477876104]\n",
      "9/20, Step:21/41, TrainLoss:0.166647, ValAUC:[0.74700280112044815, 0.92652844231791609, 0.98930481283422467, 0.965088884129364, 0.73592233009708741] ValAcc:[0.74520322656433546, 0.66352102748844854, 0.72601613282167754, 0.83831936721747979, 0.91150442477876104]\n",
      "9/20, Step:31/41, TrainLoss:0.434141, ValAUC:[0.74588235294117644, 0.92737905369484319, 0.98916769505004798, 0.9648747055043907, 0.73665048543689315] ValAcc:[0.77190852846738189, 0.66124990210666457, 0.71732320463622834, 0.83831936721747979, 0.91150442477876104]\n",
      "9/20, Step:41/41, TrainLoss:0.325823, ValAUC:[0.74812324929971985, 0.93248272195640614, 0.98752228163992861, 0.9644463482544442, 0.7339805825242719] ValAcc:[0.77190852846738189, 0.68396115592450468, 0.69414206280836399, 0.83479520714229771, 0.91150442477876104]\n",
      "Epoch: [9/20], trainAUC: [0.82782066479698535, 0.95298895148898821, 0.99513406832720852, 0.97508746531547841, 0.8672927769834049], trainAcc: [0.71560822029590099, 0.67735113131978697, 0.69266182152994904, 0.84923712006004426, 0.92762186115214185]\n",
      "Epoch: [9/20], ValAUC: [0.74812324929971985, 0.93248272195640614, 0.98752228163992861, 0.9644463482544442, 0.7339805825242719], ValAcc: [0.77190852846738189, 0.68396115592450468, 0.69414206280836399, 0.83479520714229771, 0.91150442477876104]\n",
      "10/20, Step:1/41, TrainLoss:0.016120, ValAUC:[0.7480112044817927, 0.93205741626794247, 0.98724804607157546, 0.96551724137931028, 0.73131067961165053] ValAcc:[0.77190852846738189, 0.68396115592450468, 0.69414206280836399, 0.83479520714229771, 0.91150442477876104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/20, Step:11/41, TrainLoss:0.128805, ValAUC:[0.74543417366946785, 0.92939925571504522, 0.98711092828739888, 0.96894409937888193, 0.73834951456310682] ValAcc:[0.77190852846738189, 0.66124990210666457, 0.69993734826533005, 0.82069856684156939, 0.91150442477876104]\n",
      "10/20, Step:21/41, TrainLoss:0.099155, ValAUC:[0.7471148459383754, 0.92971823498139283, 0.98615110379816251, 0.971728421503534, 0.70970873786407762] ValAcc:[0.75976975487508813, 0.66124990210666457, 0.71152791917926228, 0.82069856684156939, 0.91150442477876104]\n",
      "10/20, Step:31/41, TrainLoss:0.591588, ValAUC:[0.74577030812324929, 0.9323763955342903, 0.98807075277663514, 0.96851574212893554, 0.69927184466019421] ValAcc:[0.66994283029211377, 0.69985903359699275, 0.70863027645077925, 0.82069856684156939, 0.91150442477876104]\n",
      "10/20, Step:41/41, TrainLoss:0.045927, ValAUC:[0.73266106442577028, 0.93173843700159487, 0.98642533936651589, 0.96187620475476554, 0.72621359223300974] ValAcc:[0.77190852846738189, 0.66352102748844854, 0.72311849009319451, 0.81012608661602314, 0.91150442477876104]\n",
      "Epoch: [10/20], trainAUC: [0.82972528213876118, 0.95411358530550749, 0.99430606596796878, 0.9766558089033659, 0.85944292791426546], trainAcc: [0.72082717872968982, 0.65792258399533954, 0.71637404571825047, 0.83400933972466651, 0.92762186115214185]\n",
      "Epoch: [10/20], ValAUC: [0.73266106442577028, 0.93173843700159487, 0.98642533936651589, 0.96187620475476554, 0.72621359223300974], ValAcc: [0.77190852846738189, 0.66352102748844854, 0.72311849009319451, 0.81012608661602314, 0.91150442477876104]\n",
      "11/20, Step:1/41, TrainLoss:0.073570, ValAUC:[0.7355742296918768, 0.93163211057947903, 0.98615110379816262, 0.95973441850503316, 0.72718446601941755] ValAcc:[0.77190852846738189, 0.66124990210666457, 0.72311849009319451, 0.81012608661602314, 0.91150442477876104]\n",
      "11/20, Step:11/41, TrainLoss:0.168666, ValAUC:[0.7351260504201681, 0.93088782562466776, 0.98820787056081183, 0.95673591775540801, 0.72281553398058263] ValAcc:[0.74277547184587678, 0.65443652596131252, 0.69703970553684702, 0.82422272691675147, 0.91150442477876104]\n",
      "11/20, Step:21/41, TrainLoss:0.357273, ValAUC:[0.73355742296918769, 0.9216374269005847, 0.98669957493486915, 0.95844934675519378, 0.72548543689320388] ValAcc:[0.73791996240895918, 0.66806327825201661, 0.71152791917926228, 0.80660192654084106, 0.91150442477876104]\n",
      "11/20, Step:31/41, TrainLoss:0.371266, ValAUC:[0.72705882352941176, 0.92291334396597557, 0.98807075277663514, 0.94859713000642532, 0.73665048543689315] ValAcc:[0.72578118881666531, 0.64080977367060854, 0.71442556190774531, 0.81717440676638731, 0.91150442477876104]\n",
      "11/20, Step:41/41, TrainLoss:0.579570, ValAUC:[0.72627450980392161, 0.92514619883040938, 0.98875634169751803, 0.94923966588134501, 0.73276699029126213] ValAcc:[0.68450935860286632, 0.64762314981596047, 0.72891377555016057, 0.81365024669120523, 0.91150442477876104]\n",
      "Epoch: [11/20], trainAUC: [0.82414153680091751, 0.95538455701622449, 0.99449078665085211, 0.97157449057557621, 0.86538628190129563], trainAcc: [0.62819066652993805, 0.65030869382765077, 0.71606204276840435, 0.83601299503195303, 0.92593748159073508]\n",
      "Epoch: [11/20], ValAUC: [0.72627450980392161, 0.92514619883040938, 0.98875634169751803, 0.94923966588134501, 0.73276699029126213], ValAcc: [0.68450935860286632, 0.64762314981596047, 0.72891377555016057, 0.81365024669120523, 0.91150442477876104]\n",
      "12/20, Step:1/41, TrainLoss:0.051097, ValAUC:[0.72918767507002802, 0.92450824029771395, 0.98793363499245856, 0.94688370100663954, 0.72742718446601939] ValAcc:[0.67965384916594873, 0.64989427519774456, 0.7318114182786436, 0.81717440676638731, 0.91150442477876104]\n",
      "12/20, Step:11/41, TrainLoss:0.059643, ValAUC:[0.75977591036414571, 0.92046783625730999, 0.98313451254627726, 0.94538445063182697, 0.69684466019417468] ValAcc:[0.75248649071971185, 0.63626752290704047, 0.72891377555016057, 0.82422272691675147, 0.91150442477876104]\n",
      "12/20, Step:21/41, TrainLoss:0.202309, ValAUC:[0.7586554621848739, 0.92440191387559811, 0.98903057726587129, 0.9648747055043907, 0.66674757281553398] ValAcc:[0.73791996240895918, 0.70213015897877673, 0.72022084736471137, 0.86298848774375436, 0.91150442477876104]\n",
      "12/20, Step:31/41, TrainLoss:0.150622, ValAUC:[0.75249299719887952, 0.92737905369484319, 0.98779651720828188, 0.94752623688155935, 0.68907766990291264] ValAcc:[0.76705301903046441, 0.66352102748844854, 0.72022084736471137, 0.85241600751820812, 0.91150442477876104]\n",
      "12/20, Step:41/41, TrainLoss:0.383680, ValAUC:[0.76504201680672268, 0.92121212121212115, 0.98135198135198132, 0.94452773613193408, 0.70825242718446602] ValAcc:[0.71607016994283024, 0.67033440363380059, 0.71442556190774531, 0.82774688699193355, 0.91150442477876104]\n",
      "Epoch: [12/20], trainAUC: [0.84450925684987621, 0.95669075821998917, 0.99330954649452008, 0.97136193118820702, 0.86249043141672799], trainAcc: [0.6540680020974744, 0.66816195353119701, 0.69890188052687041, 0.84803492687567228, 0.92677967137143846]\n",
      "Epoch: [12/20], ValAUC: [0.76504201680672268, 0.92121212121212115, 0.98135198135198132, 0.94452773613193408, 0.70825242718446602], ValAcc: [0.71607016994283024, 0.67033440363380059, 0.71442556190774531, 0.82774688699193355, 0.91150442477876104]\n",
      "13/20, Step:1/41, TrainLoss:0.161909, ValAUC:[0.76414565826330538, 0.92185007974481659, 0.98135198135198132, 0.94559862925680016, 0.71213592233009704] ValAcc:[0.71849792466128903, 0.67033440363380059, 0.71442556190774531, 0.82422272691675147, 0.91150442477876104]\n",
      "13/20, Step:11/41, TrainLoss:0.062730, ValAUC:[0.75249299719887952, 0.92854864433811801, 0.98121486356780474, 0.95352323838080955, 0.72378640776699033] ValAcc:[0.74277547184587678, 0.65670765134309661, 0.68834677735139793, 0.81717440676638731, 0.90786279270107295]\n",
      "13/20, Step:21/41, TrainLoss:0.364494, ValAUC:[0.7357983193277311, 0.92567783094098888, 0.98327163033045384, 0.95844934675519389, 0.70097087378640777] ValAcc:[0.74763098128279426, 0.65216540057952854, 0.72311849009319451, 0.82422272691675147, 0.91150442477876104]\n",
      "13/20, Step:31/41, TrainLoss:0.442935, ValAUC:[0.72481792717086835, 0.91493886230728338, 0.98190045248868785, 0.96080531162989935, 0.73131067961165042] ValAcc:[0.75248649071971185, 0.69758790821520866, 0.7318114182786436, 0.78545696608974858, 0.91150442477876104]\n",
      "13/20, Step:41/41, TrainLoss:0.184702, ValAUC:[0.71226890756302519, 0.91653375863902187, 0.98094062799945148, 0.96830156350396224, 0.74344660194174761] ValAcc:[0.71607016994283024, 0.66806327825201661, 0.72601613282167754, 0.7995536063904769, 0.90057952854569656]\n",
      "Epoch: [13/20], trainAUC: [0.84712223282350785, 0.95828828023164758, 0.99396903173955031, 0.97114649937668385, 0.87311697502780294], trainAcc: [0.65515528510451371, 0.67420055607798479, 0.70857397197209859, 0.8275976427413495, 0.92088434290651477]\n",
      "Epoch: [13/20], ValAUC: [0.71226890756302519, 0.91653375863902187, 0.98094062799945148, 0.96830156350396224, 0.74344660194174761], ValAcc: [0.71607016994283024, 0.66806327825201661, 0.72601613282167754, 0.7995536063904769, 0.90057952854569656]\n",
      "14/20, Step:1/41, TrainLoss:0.168257, ValAUC:[0.71204481792717089, 0.91653375863902187, 0.98148909913615789, 0.96723067037909616, 0.74684466019417473] ValAcc:[0.71121466050591275, 0.66806327825201661, 0.72601613282167754, 0.7995536063904769, 0.90422116062338476]\n",
      "14/20, Step:11/41, TrainLoss:0.356919, ValAUC:[0.71843137254901956, 0.91653375863902176, 0.98477992595639652, 0.96508888412936389, 0.75825242718446606] ValAcc:[0.74277547184587678, 0.66124990210666457, 0.70863027645077925, 0.84536768736784396, 0.91150442477876104]\n",
      "14/20, Step:21/41, TrainLoss:0.093106, ValAUC:[0.71170868347338934, 0.9166400850611377, 0.98203757027286442, 0.96251874062968512, 0.75072815533980575] ValAcc:[0.77433628318584069, 0.64989427519774456, 0.72601613282167754, 0.81365024669120523, 0.91150442477876104]\n",
      "14/20, Step:31/41, TrainLoss:0.335886, ValAUC:[0.73378151260504199, 0.92631578947368431, 0.9808035102152749, 0.95330905975583635, 0.72208737864077666] ValAcc:[0.77433628318584069, 0.69304565745164071, 0.72311849009319451, 0.82774688699193355, 0.91150442477876104]\n",
      "14/20, Step:41/41, TrainLoss:0.228024, ValAUC:[0.72929971988795517, 0.93397129186602879, 0.97943233237350891, 0.95973441850503316, 0.7203883495145631] ValAcc:[0.77190852846738189, 0.66579215287023263, 0.72601613282167754, 0.82774688699193355, 0.91150442477876104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14/20], trainAUC: [0.85243733194552862, 0.96234915679563371, 0.9955294354028531, 0.97949950881546965, 0.87344194578043532], trainAcc: [0.71495585049167742, 0.6707874328993656, 0.71668604866809649, 0.84322615413818458, 0.92762186115214185]\n",
      "Epoch: [14/20], ValAUC: [0.72929971988795517, 0.93397129186602879, 0.97943233237350891, 0.95973441850503316, 0.7203883495145631], ValAcc: [0.77190852846738189, 0.66579215287023263, 0.72601613282167754, 0.82774688699193355, 0.91150442477876104]\n",
      "15/20, Step:1/41, TrainLoss:0.545185, ValAUC:[0.72985994397759102, 0.93524720893141944, 0.97943233237350891, 0.96059113300492605, 0.71480582524271852] ValAcc:[0.77190852846738189, 0.66352102748844854, 0.72601613282167754, 0.82774688699193355, 0.91150442477876104]\n",
      "15/20, Step:11/41, TrainLoss:0.504530, ValAUC:[0.72257703081232494, 0.93173843700159487, 0.97847250788427265, 0.96423216962947089, 0.71262135922330105] ValAcc:[0.65780405669981989, 0.64989427519774456, 0.72601613282167754, 0.82422272691675147, 0.91150442477876104]\n",
      "15/20, Step:21/41, TrainLoss:0.285378, ValAUC:[0.73064425770308128, 0.93280170122275397, 0.98011792129439201, 0.96615977725423008, 0.70242718446601948] ValAcc:[0.76705301903046441, 0.65897877672488059, 0.70283499099381319, 0.82069856684156939, 0.91150442477876104]\n",
      "15/20, Step:31/41, TrainLoss:0.358348, ValAUC:[0.74621848739495789, 0.92780435938330674, 0.9862882215823392, 0.95480831013064893, 0.71237864077669899] ValAcc:[0.76948077374892321, 0.66352102748844854, 0.71152791917926228, 0.82422272691675147, 0.90786279270107295]\n",
      "15/20, Step:41/41, TrainLoss:0.074394, ValAUC:[0.75753501400560219, 0.91504518872939922, 0.99012751953928424, 0.95566502463054182, 0.71165048543689324] ValAcc:[0.7330644529720417, 0.66124990210666457, 0.71732320463622834, 0.83831936721747979, 0.89693789646800848]\n",
      "Epoch: [15/20], trainAUC: [0.85662122570137145, 0.96279630036123776, 0.994756525177105, 0.98018314576403598, 0.867263890694282], trainAcc: [0.66341863595801265, 0.66973724115209821, 0.69734186577764012, 0.84883638899858693, 0.92425310202932831]\n",
      "Epoch: [15/20], ValAUC: [0.75753501400560219, 0.91504518872939922, 0.99012751953928424, 0.95566502463054182, 0.71165048543689324], ValAcc: [0.7330644529720417, 0.66124990210666457, 0.71732320463622834, 0.83831936721747979, 0.89693789646800848]\n",
      "16/20, Step:1/41, TrainLoss:0.310985, ValAUC:[0.75473389355742282, 0.91568314726209465, 0.98999040175510766, 0.95630756050546151, 0.71213592233009715] ValAcc:[0.73549220769050039, 0.66124990210666457, 0.71732320463622834, 0.83831936721747979, 0.89693789646800848]\n",
      "16/20, Step:11/41, TrainLoss:0.112362, ValAUC:[0.75719887955182075, 0.91281233386496541, 0.98916769505004798, 0.94923966588134501, 0.71868932038834954] ValAcc:[0.7330644529720417, 0.64989427519774456, 0.71732320463622834, 0.82422272691675147, 0.90422116062338476]\n",
      "16/20, Step:21/41, TrainLoss:0.011656, ValAUC:[0.74352941176470588, 0.90707070707070714, 0.98903057726587129, 0.94431355750696078, 0.73567961165048545] ValAcc:[0.74277547184587678, 0.66352102748844854, 0.71732320463622834, 0.81717440676638731, 0.90786279270107295]\n",
      "16/20, Step:31/41, TrainLoss:0.051168, ValAUC:[0.74577030812324929, 0.90207336523126003, 0.9876593994241053, 0.94324266438209459, 0.73058252427184456] ValAcc:[0.7330644529720417, 0.66124990210666457, 0.71732320463622834, 0.82422272691675147, 0.90422116062338476]\n",
      "16/20, Step:41/41, TrainLoss:0.044895, ValAUC:[0.73490196078431369, 0.9031366294524189, 0.98834498834498841, 0.9428143071321482, 0.73009708737864076] ValAcc:[0.73063669825358291, 0.64762314981596047, 0.72601613282167754, 0.82774688699193355, 0.91150442477876104]\n",
      "Epoch: [16/20], trainAUC: [0.8466728828750687, 0.96031533106238609, 0.99596369104331539, 0.97891640671228075, 0.85956569464303767], trainAcc: [0.65276326248902716, 0.65109633763810126, 0.71325401621978968, 0.8452298094454711, 0.92635857648108677]\n",
      "Epoch: [16/20], ValAUC: [0.73490196078431369, 0.9031366294524189, 0.98834498834498841, 0.9428143071321482, 0.73009708737864076], ValAcc: [0.73063669825358291, 0.64762314981596047, 0.72601613282167754, 0.82774688699193355, 0.91150442477876104]\n",
      "17/20, Step:1/41, TrainLoss:0.099448, ValAUC:[0.73322128851540613, 0.90388091440723017, 0.98834498834498841, 0.9428143071321482, 0.72815533980582525] ValAcc:[0.72820894353512411, 0.65216540057952854, 0.72601613282167754, 0.83127104706711563, 0.91150442477876104]\n",
      "17/20, Step:11/41, TrainLoss:0.151309, ValAUC:[0.73871148459383751, 0.91451355661881983, 0.98889345948169471, 0.94859713000642532, 0.74077669902912624] ValAcc:[0.68208160388440753, 0.67033440363380059, 0.71442556190774531, 0.82422272691675147, 0.91150442477876104]\n",
      "17/20, Step:21/41, TrainLoss:0.248125, ValAUC:[0.7369187675070028, 0.91504518872939922, 0.98532839709310305, 0.95009638038123789, 0.77063106796116498] ValAcc:[0.75491424543817054, 0.65897877672488059, 0.71152791917926228, 0.80660192654084106, 0.91150442477876104]\n",
      "17/20, Step:31/41, TrainLoss:0.222227, ValAUC:[0.74028011204481781, 0.91153641679957476, 0.98368298368298368, 0.9286785178839152, 0.75995145631067973] ValAcc:[0.75491424543817054, 0.64308089905239252, 0.73760670373560966, 0.7995536063904769, 0.91150442477876104]\n",
      "17/20, Step:41/41, TrainLoss:0.153654, ValAUC:[0.76605042016806713, 0.91440723019670389, 0.98820787056081172, 0.92418076675947736, 0.71699029126213598] ValAcc:[0.70635915106899527, 0.65216540057952854, 0.7318114182786436, 0.84184352729266188, 0.91150442477876104]\n",
      "Epoch: [17/20], trainAUC: [0.86149781709891005, 0.96016763818768647, 0.99248802556274995, 0.97898247246781445, 0.86332813380129125], trainAcc: [0.641455519215818, 0.66448628241576102, 0.71169400147055939, 0.85164150642878811, 0.92762186115214185]\n",
      "Epoch: [17/20], ValAUC: [0.76605042016806713, 0.91440723019670389, 0.98820787056081172, 0.92418076675947736, 0.71699029126213598], ValAcc: [0.70635915106899527, 0.65216540057952854, 0.7318114182786436, 0.84184352729266188, 0.91150442477876104]\n",
      "18/20, Step:1/41, TrainLoss:0.048324, ValAUC:[0.76302521008403357, 0.91366294524189251, 0.98848210612916487, 0.92418076675947736, 0.7128640776699029] ValAcc:[0.70150364163207768, 0.65216540057952854, 0.73470906100712663, 0.84184352729266188, 0.91150442477876104]\n",
      "18/20, Step:11/41, TrainLoss:0.277281, ValAUC:[0.77210084033613446, 0.91493886230728338, 0.98382010146716026, 0.93810237738273716, 0.74514563106796117] ValAcc:[0.76948077374892321, 0.64762314981596047, 0.73760670373560966, 0.82069856684156939, 0.91150442477876104]\n",
      "18/20, Step:21/41, TrainLoss:0.133620, ValAUC:[0.76560224089635853, 0.92238171185539608, 0.98861922391334156, 0.95309488113086327, 0.74878640776699035] ValAcc:[0.76948077374892321, 0.64308089905239252, 0.72311849009319451, 0.81012608661602314, 0.91150442477876104]\n",
      "18/20, Step:31/41, TrainLoss:0.551257, ValAUC:[0.77523809523809517, 0.92185007974481659, 0.99053887289181408, 0.94902548725637181, 0.74757281553398069] ValAcc:[0.76219750959354682, 0.65443652596131252, 0.72311849009319451, 0.82422272691675147, 0.8896546323126322]\n",
      "18/20, Step:41/41, TrainLoss:0.022423, ValAUC:[0.77277310924369746, 0.92291334396597557, 0.99053887289181408, 0.94559862925680016, 0.70970873786407762] ValAcc:[0.60682120761218572, 0.68623228130628866, 0.72601613282167754, 0.83127104706711563, 0.8896546323126322]\n",
      "Epoch: [18/20], trainAUC: [0.86138096201848469, 0.96354831453975376, 0.99607225495343088, 0.98021761485387982, 0.88572222944379442], trainAcc: [0.5651282521216564, 0.68969088435017933, 0.70763796312256044, 0.84883638899858693, 0.92046324801616308]\n",
      "Epoch: [18/20], ValAUC: [0.77277310924369746, 0.92291334396597557, 0.99053887289181408, 0.94559862925680016, 0.70970873786407762], ValAcc: [0.60682120761218572, 0.68623228130628866, 0.72601613282167754, 0.83127104706711563, 0.8896546323126322]\n",
      "19/20, Step:1/41, TrainLoss:0.252778, ValAUC:[0.77098039215686265, 0.92333864965443913, 0.99067599067599066, 0.9430284857571215, 0.70970873786407762] ValAcc:[0.59468243401989196, 0.68396115592450468, 0.72601613282167754, 0.83127104706711563, 0.8896546323126322]\n",
      "19/20, Step:11/41, TrainLoss:0.156845, ValAUC:[0.77703081232492988, 0.92068048910154177, 0.99204716851775676, 0.94260012850717501, 0.75485436893203883] ValAcc:[0.66751507557365497, 0.66579215287023263, 0.7318114182786436, 0.82422272691675147, 0.87872973607956772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20, Step:21/41, TrainLoss:0.058050, ValAUC:[0.78700280112044818, 0.90898458266879312, 0.99163581516522692, 0.96358963375455131, 0.77451456310679612] ValAcc:[0.75976975487508813, 0.66352102748844854, 0.72311849009319451, 0.82422272691675147, 0.87872973607956772]\n",
      "19/20, Step:31/41, TrainLoss:0.136764, ValAUC:[0.78151260504201669, 0.91153641679957476, 0.98683669271904573, 0.96744484900406924, 0.75339805825242712] ValAcc:[0.74763098128279426, 0.67260552901558457, 0.72601613282167754, 0.82774688699193355, 0.90057952854569656]\n",
      "19/20, Step:41/41, TrainLoss:0.444593, ValAUC:[0.77669467787114843, 0.92089314194577343, 0.98395721925133683, 0.96894409937888204, 0.73907766990291268] ValAcc:[0.74520322656433546, 0.66579215287023263, 0.71732320463622834, 0.83831936721747979, 0.91150442477876104]\n",
      "Epoch: [19/20], trainAUC: [0.87233763167278633, 0.96473934240086301, 0.99516971617829109, 0.98274822053323674, 0.88050103268483626], trainAcc: [0.67820568485374766, 0.66474883035257792, 0.70326992182471548, 0.84603127156838576, 0.92762186115214185]\n",
      "Epoch: [19/20], ValAUC: [0.77669467787114843, 0.92089314194577343, 0.98395721925133683, 0.96894409937888204, 0.73907766990291268], ValAcc: [0.74520322656433546, 0.66579215287023263, 0.71732320463622834, 0.83831936721747979, 0.91150442477876104]\n",
      "20/20, Step:1/41, TrainLoss:0.502671, ValAUC:[0.77747899159663858, 0.92142477405635292, 0.98423145481969021, 0.96787320625401585, 0.73956310679611648] ValAcc:[0.74034771712741798, 0.65670765134309661, 0.71732320463622834, 0.83479520714229771, 0.91150442477876104]\n",
      "20/20, Step:11/41, TrainLoss:0.186824, ValAUC:[0.77535014005602243, 0.91196172248803831, 0.98395721925133695, 0.96915827800385523, 0.75315533980582527] ValAcc:[0.67722609444749005, 0.63853864828882445, 0.70863027645077925, 0.83127104706711563, 0.90422116062338476]\n",
      "20/20, Step:21/41, TrainLoss:0.026276, ValAUC:[0.78028011204481795, 0.91685273790536947, 0.98807075277663514, 0.95180980938102377, 0.75315533980582527] ValAcc:[0.68693711332132512, 0.65443652596131252, 0.71442556190774531, 0.85241600751820812, 0.86780483984650325]\n",
      "20/20, Step:31/41, TrainLoss:0.043065, ValAUC:[0.76963585434173665, 0.91440723019670389, 0.99108734402852039, 0.94110087813236243, 0.75072815533980575] ValAcc:[0.72820894353512411, 0.65216540057952854, 0.72311849009319451, 0.85946432766857228, 0.86416320776881506]\n",
      "20/20, Step:41/41, TrainLoss:0.055466, ValAUC:[0.76324929971988797, 0.90845295055821373, 0.9876593994241053, 0.94666952238166624, 0.73737864077669912] ValAcc:[0.70150364163207768, 0.65670765134309661, 0.72601613282167754, 0.84889184744302604, 0.89329626439032028]\n",
      "Epoch: [20/20], trainAUC: [0.8744337949711356, 0.97106168142240423, 0.99546786184189218, 0.97940759124255328, 0.89609240723890404], trainAcc: [0.64189043241863375, 0.65555965256398785, 0.70451793362409976, 0.86165978296522072, 0.91751558378370124]\n",
      "Epoch: [20/20], ValAUC: [0.76324929971988797, 0.90845295055821373, 0.9876593994241053, 0.94666952238166624, 0.73737864077669912], ValAcc: [0.70150364163207768, 0.65670765134309661, 0.72601613282167754, 0.84889184744302604, 0.89329626439032028]\n"
     ]
    }
   ],
   "source": [
    "# #build model\n",
    "# num_tasks = len(tag_predicted)\n",
    "# rnn1_type = params['rnn1_type'] \n",
    "# rnn_1 = rnn_types[rnn1_type]\n",
    "# rnn2_type = params['rnn2_type']\n",
    "# rnn_2 = rnn_types[rnn2_type]\n",
    "# bi = params['bi']\n",
    "# hidden_dim1 = params['hidden_dim1']\n",
    "# hidden_dim2 = params['hidden_dim2']\n",
    "# num_classes = params['num_classes']\n",
    "# batch_size = params['batch_size']\n",
    "# cuda_on = params['cuda_on']\n",
    "\n",
    "\n",
    "# weights_matrix = torch.from_numpy(emb_weight)\n",
    "# model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n",
    "#                       cuda_on, num_tasks, num_classes)\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# print('The number of train parameters', sum([np.prod(p.size()) for p in model_parameters]))\n",
    "# model = model.cuda()\n",
    "\n",
    "# #parameter for training\n",
    "# learning_rate = params['learning_rate']\n",
    "# num_epochs = params['num_epochs'] # number epoch to train\n",
    "\n",
    "# # Criterion and Optimizer\n",
    "# #pos_weight=torch.Tensor([40,]).cuda()\n",
    "# criterion = nn.BCEWithLogitsLoss() #torch.nn.BCELoss(); torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (steps_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "        for step_id in range(6):\n",
    "            lengths_batch[step_id] = lengths_batch[step_id].cuda()\n",
    "            steps_batch[step_id] = steps_batch[step_id].cuda() \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(steps_batch, lengths_batch)\n",
    "        task_id = np.random.randint(0, num_tasks)\n",
    "#         loss_list = []\n",
    "#         for task_id in range(num_tasks):\n",
    "#             loss_list.append(criterion(logits[task_id], \n",
    "#                                   labels_batch[task_id].view(-1,1).float().cuda()))\n",
    "#         loss= torch.mean(torch.stack(loss_list))\n",
    "        loss = criterion(logits[task_id], labels_batch[task_id].view(-1,1).float().cuda())\n",
    "        train_loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i % 10 == 0:\n",
    "            # validate\n",
    "#             print('---------------------')\n",
    "#             for p in model.parameters():\n",
    "#                 if p.requires_grad:\n",
    "#                     print(p.name, p.size(), p.requires_grad, torch.mean(torch.abs(p.data)), torch.mean(torch.abs(p.grad)))\n",
    "#                     break\n",
    "            val_auc, val_acc = test_model(val_loader, model)\n",
    "            print('{}/{}, Step:{}/{}, TrainLoss:{:.6f}, ValAUC:{} ValAcc:{}'.format(\n",
    "                epoch+1, num_epochs, i+1, len(train_loader), loss, val_auc, val_acc))\n",
    "    val_auc, val_acc = test_model(val_loader, model)\n",
    "    train_auc, train_acc = test_model(train_loader, model)\n",
    "    print('Epoch: [{}/{}], trainAUC: {}, trainAcc: {}'.format(epoch+1, num_epochs, train_auc, train_acc))\n",
    "    print('Epoch: [{}/{}], ValAUC: {}, ValAcc: {}'.format(epoch+1, num_epochs, val_auc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2affd74fbb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7100715637207031,\n",
       " 0.6749559640884399,\n",
       " 0.6341803669929504,\n",
       " 0.5763193368911743,\n",
       " 0.6072262525558472,\n",
       " 0.5393203496932983,\n",
       " 0.6776454448699951,\n",
       " 0.6320223808288574,\n",
       " 0.5687660574913025,\n",
       " 0.5386589765548706,\n",
       " 0.5262254476547241,\n",
       " 0.5568784475326538,\n",
       " 0.5461165904998779,\n",
       " 0.5277289748191833,\n",
       " 0.5762420892715454,\n",
       " 0.6197364926338196,\n",
       " 0.5988755226135254,\n",
       " 0.5976908802986145,\n",
       " 0.5332546234130859,\n",
       " 0.5413956642150879,\n",
       " 0.537192702293396,\n",
       " 0.5248972177505493,\n",
       " 0.5829717516899109,\n",
       " 0.5485081076622009,\n",
       " 0.601547360420227,\n",
       " 0.5548056960105896,\n",
       " 0.5823066830635071,\n",
       " 0.5304849147796631,\n",
       " 0.5699131488800049,\n",
       " 0.6373076438903809,\n",
       " 0.5215262174606323,\n",
       " 0.5432432889938354,\n",
       " 0.6002408266067505,\n",
       " 0.6445224285125732,\n",
       " 0.5340147018432617,\n",
       " 0.5818507671356201,\n",
       " 0.5948092937469482,\n",
       " 0.5200140476226807,\n",
       " 0.6017743349075317,\n",
       " 0.5819497108459473,\n",
       " 0.557530403137207,\n",
       " 0.5826940536499023,\n",
       " 0.4906589090824127,\n",
       " 0.6161730885505676,\n",
       " 0.4993214011192322,\n",
       " 0.5104201436042786,\n",
       " 0.6048842668533325,\n",
       " 0.6451091766357422,\n",
       " 0.6239080429077148,\n",
       " 0.5886887907981873,\n",
       " 0.6231103539466858,\n",
       " 0.5674463510513306,\n",
       " 0.6088069677352905,\n",
       " 0.5751285552978516,\n",
       " 0.5694952011108398,\n",
       " 0.6068483591079712,\n",
       " 0.5873565077781677,\n",
       " 0.5974433422088623,\n",
       " 0.5034798979759216,\n",
       " 0.6147557497024536,\n",
       " 0.5463549494743347,\n",
       " 0.5272601842880249,\n",
       " 0.5858535170555115,\n",
       " 0.6397316455841064,\n",
       " 0.5831691026687622,\n",
       " 0.6068019270896912,\n",
       " 0.5734202861785889,\n",
       " 0.5607274770736694,\n",
       " 0.5541373491287231,\n",
       " 0.5289605855941772,\n",
       " 0.5474565029144287,\n",
       " 0.5933987498283386,\n",
       " 0.5369550585746765,\n",
       " 0.5516397953033447,\n",
       " 0.5142149329185486,\n",
       " 0.5365140438079834,\n",
       " 0.5671335458755493,\n",
       " 0.5582720041275024,\n",
       " 0.5584119558334351,\n",
       " 0.5394453406333923,\n",
       " 0.567331075668335,\n",
       " 0.60164475440979,\n",
       " 0.5298082828521729,\n",
       " 0.6054612398147583,\n",
       " 0.579448938369751,\n",
       " 0.623115062713623,\n",
       " 0.5876255631446838,\n",
       " 0.5655551552772522,\n",
       " 0.5876389741897583,\n",
       " 0.5448936223983765,\n",
       " 0.5431663393974304,\n",
       " 0.5790743827819824,\n",
       " 0.5905948877334595,\n",
       " 0.5813479423522949,\n",
       " 0.5563054084777832,\n",
       " 0.511850118637085,\n",
       " 0.5779426097869873,\n",
       " 0.5920140743255615,\n",
       " 0.5777086019515991,\n",
       " 0.5524810552597046,\n",
       " 0.5316634774208069,\n",
       " 0.5141267776489258,\n",
       " 0.5572898387908936,\n",
       " 0.6477819681167603,\n",
       " 0.6257128119468689,\n",
       " 0.54570472240448,\n",
       " 0.5363187789916992,\n",
       " 0.5330865979194641,\n",
       " 0.536898136138916,\n",
       " 0.5010237693786621,\n",
       " 0.5669236183166504,\n",
       " 0.566774845123291,\n",
       " 0.5115805268287659,\n",
       " 0.4847031235694885,\n",
       " 0.5025815963745117,\n",
       " 0.5213242769241333,\n",
       " 0.55522221326828,\n",
       " 0.5021257996559143,\n",
       " 0.5012511610984802,\n",
       " 0.5136439204216003,\n",
       " 0.4899212121963501,\n",
       " 0.47215574979782104,\n",
       " 0.5018271207809448,\n",
       " 0.596174955368042,\n",
       " 0.5858838558197021,\n",
       " 0.48754093050956726,\n",
       " 0.531826913356781,\n",
       " 0.5158947706222534,\n",
       " 0.6683620810508728,\n",
       " 0.5401691198348999,\n",
       " 0.4885708689689636,\n",
       " 0.5207306146621704,\n",
       " 0.5185732245445251,\n",
       " 0.5009615421295166,\n",
       " 0.49347853660583496,\n",
       " 0.5011639595031738,\n",
       " 0.6085021495819092,\n",
       " 0.4461435377597809,\n",
       " 0.5336381793022156,\n",
       " 0.5655086040496826,\n",
       " 0.49737173318862915,\n",
       " 0.40604013204574585,\n",
       " 0.4765891432762146,\n",
       " 0.478426456451416,\n",
       " 0.46656346321105957,\n",
       " 0.45940959453582764,\n",
       " 0.5327610969543457,\n",
       " 0.40149998664855957,\n",
       " 0.5490773916244507,\n",
       " 0.48826199769973755,\n",
       " 0.4420959949493408,\n",
       " 0.5352367162704468,\n",
       " 0.4285525679588318,\n",
       " 0.4507502317428589,\n",
       " 0.45143139362335205,\n",
       " 0.5713268518447876,\n",
       " 0.49986398220062256,\n",
       " 0.46691644191741943,\n",
       " 0.494051456451416,\n",
       " 0.3952498137950897,\n",
       " 0.46842455863952637,\n",
       " 0.4631209075450897,\n",
       " 0.503314733505249,\n",
       " 0.45602476596832275,\n",
       " 0.4800335168838501,\n",
       " 0.418146014213562,\n",
       " 0.43735969066619873,\n",
       " 0.480445921421051,\n",
       " 0.3944692611694336,\n",
       " 0.5515776872634888,\n",
       " 0.45990628004074097,\n",
       " 0.435478150844574,\n",
       " 0.4090864062309265,\n",
       " 0.42868125438690186,\n",
       " 0.5297888517379761,\n",
       " 0.456997811794281,\n",
       " 0.4116542339324951,\n",
       " 0.44785410165786743,\n",
       " 0.5314208269119263,\n",
       " 0.4833412766456604,\n",
       " 0.44661664962768555,\n",
       " 0.42646217346191406,\n",
       " 0.480352520942688,\n",
       " 0.43101873993873596,\n",
       " 0.5481611490249634,\n",
       " 0.4723608195781708,\n",
       " 0.4932071566581726,\n",
       " 0.5842937231063843,\n",
       " 0.44157710671424866,\n",
       " 0.46841758489608765,\n",
       " 0.4120844006538391,\n",
       " 0.4438856244087219,\n",
       " 0.4426928758621216,\n",
       " 0.4222630560398102,\n",
       " 0.45331159234046936,\n",
       " 0.4360257387161255,\n",
       " 0.39001837372779846,\n",
       " 0.43590614199638367,\n",
       " 0.4491400718688965,\n",
       " 0.5985204577445984,\n",
       " 0.40367263555526733,\n",
       " 0.43435418605804443,\n",
       " 0.43444883823394775,\n",
       " 0.5216434001922607,\n",
       " 0.4611862897872925,\n",
       " 0.42208537459373474,\n",
       " 0.39396435022354126,\n",
       " 0.45631468296051025,\n",
       " 0.3650629222393036,\n",
       " 0.5189090371131897,\n",
       " 0.4218336343765259,\n",
       " 0.4760410785675049,\n",
       " 0.44825279712677,\n",
       " 0.42422136664390564,\n",
       " 0.35823002457618713,\n",
       " 0.46835857629776,\n",
       " 0.44186437129974365,\n",
       " 0.4070664048194885,\n",
       " 0.44918516278266907,\n",
       " 0.3544119596481323,\n",
       " 0.381698340177536,\n",
       " 0.3722638487815857,\n",
       " 0.3699530363082886,\n",
       " 0.40345585346221924,\n",
       " 0.4737169146537781,\n",
       " 0.3632839620113373,\n",
       " 0.5369389653205872,\n",
       " 0.40416330099105835,\n",
       " 0.4765399098396301,\n",
       " 0.3748249113559723,\n",
       " 0.3746587336063385,\n",
       " 0.5062022805213928,\n",
       " 0.41522443294525146,\n",
       " 0.43683576583862305,\n",
       " 0.5231176614761353,\n",
       " 0.5671533346176147,\n",
       " 0.33199557662010193,\n",
       " 0.49427300691604614,\n",
       " 0.3883933126926422,\n",
       " 0.4090508818626404,\n",
       " 0.41095447540283203,\n",
       " 0.4461597204208374,\n",
       " 0.5368213653564453,\n",
       " 0.3990114629268646,\n",
       " 0.44688355922698975,\n",
       " 0.4396247863769531,\n",
       " 0.4371325373649597,\n",
       " 0.4206934869289398,\n",
       " 0.48475706577301025,\n",
       " 0.5440869331359863,\n",
       " 0.5016602277755737,\n",
       " 0.34432870149612427,\n",
       " 0.41650694608688354,\n",
       " 0.4019814729690552,\n",
       " 0.3700694739818573,\n",
       " 0.43515273928642273,\n",
       " 0.37328240275382996,\n",
       " 0.4700707793235779,\n",
       " 0.4519144594669342,\n",
       " 0.5248615145683289,\n",
       " 0.40749359130859375,\n",
       " 0.4405127763748169,\n",
       " 0.34939655661582947,\n",
       " 0.5092895030975342,\n",
       " 0.3613603711128235,\n",
       " 0.37973612546920776,\n",
       " 0.3399985432624817,\n",
       " 0.466608464717865,\n",
       " 0.3488134443759918,\n",
       " 0.43827110528945923,\n",
       " 0.3887307345867157,\n",
       " 0.3377021253108978,\n",
       " 0.4447859525680542,\n",
       " 0.407694935798645,\n",
       " 0.4310078024864197,\n",
       " 0.3918902277946472,\n",
       " 0.37090182304382324,\n",
       " 0.33654695749282837,\n",
       " 0.33428817987442017,\n",
       " 0.416090190410614,\n",
       " 0.37873196601867676,\n",
       " 0.4648405909538269,\n",
       " 0.36228787899017334,\n",
       " 0.3799748122692108,\n",
       " 0.3076571822166443,\n",
       " 0.3092000186443329,\n",
       " 0.3018059730529785,\n",
       " 0.38819003105163574,\n",
       " 0.3343237340450287,\n",
       " 0.26994115114212036,\n",
       " 0.4622507393360138,\n",
       " 0.45316147804260254,\n",
       " 0.3994640111923218,\n",
       " 0.3804630637168884,\n",
       " 0.33806657791137695,\n",
       " 0.4074353277683258,\n",
       " 0.387999951839447,\n",
       " 0.3182836174964905,\n",
       " 0.3798612952232361,\n",
       " 0.3530019521713257,\n",
       " 0.4822617173194885,\n",
       " 0.3789186179637909,\n",
       " 0.4227488040924072,\n",
       " 0.37844690680503845,\n",
       " 0.41488251090049744,\n",
       " 0.3642747104167938,\n",
       " 0.300918847322464,\n",
       " 0.49874112010002136,\n",
       " 0.3535082936286926,\n",
       " 0.39993613958358765,\n",
       " 0.40075939893722534,\n",
       " 0.3590765595436096,\n",
       " 0.33989405632019043,\n",
       " 0.3657627999782562,\n",
       " 0.3382331132888794,\n",
       " 0.3285221755504608,\n",
       " 0.30565744638442993,\n",
       " 0.4886300563812256,\n",
       " 0.4019714593887329,\n",
       " 0.31115418672561646,\n",
       " 0.3699873685836792,\n",
       " 0.389313668012619,\n",
       " 0.37716618180274963,\n",
       " 0.38791167736053467,\n",
       " 0.4708409905433655,\n",
       " 0.3390911817550659,\n",
       " 0.4026763439178467,\n",
       " 0.31688207387924194,\n",
       " 0.3386542499065399,\n",
       " 0.20041990280151367,\n",
       " 0.3295047879219055,\n",
       " 0.43028637766838074,\n",
       " 0.3999561071395874,\n",
       " 0.4033728539943695,\n",
       " 0.32184654474258423,\n",
       " 0.39586204290390015,\n",
       " 0.32589191198349,\n",
       " 0.33422935009002686,\n",
       " 0.33042141795158386,\n",
       " 0.33229437470436096,\n",
       " 0.3031957447528839,\n",
       " 0.2965598404407501,\n",
       " 0.30190932750701904,\n",
       " 0.338239461183548,\n",
       " 0.26966553926467896,\n",
       " 0.3594525456428528,\n",
       " 0.44345444440841675,\n",
       " 0.2673111855983734,\n",
       " 0.3836970329284668,\n",
       " 0.30900853872299194,\n",
       " 0.29871833324432373,\n",
       " 0.28295084834098816,\n",
       " 0.29167744517326355,\n",
       " 0.3692891597747803,\n",
       " 0.3901318311691284,\n",
       " 0.4603157937526703,\n",
       " 0.269746333360672,\n",
       " 0.3979123830795288,\n",
       " 0.42263421416282654,\n",
       " 0.34054476022720337,\n",
       " 0.3914770483970642,\n",
       " 0.31031227111816406,\n",
       " 0.4100898504257202,\n",
       " 0.31917649507522583,\n",
       " 0.3301731050014496,\n",
       " 0.41470369696617126,\n",
       " 0.3223824203014374,\n",
       " 0.3466315269470215,\n",
       " 0.3026476502418518,\n",
       " 0.2799414098262787,\n",
       " 0.2965233325958252,\n",
       " 0.3022438585758209,\n",
       " 0.2789672017097473,\n",
       " 0.2886400520801544,\n",
       " 0.23665709793567657,\n",
       " 0.3227141499519348,\n",
       " 0.30948606133461,\n",
       " 0.22539108991622925,\n",
       " 0.3014911413192749,\n",
       " 0.2886882424354553,\n",
       " 0.26716238260269165,\n",
       " 0.308165967464447,\n",
       " 0.370551735162735,\n",
       " 0.31012311577796936,\n",
       " 0.27660441398620605,\n",
       " 0.3345194458961487,\n",
       " 0.20185486972332,\n",
       " 0.3407728970050812,\n",
       " 0.45096540451049805,\n",
       " 0.361034095287323,\n",
       " 0.2920716106891632,\n",
       " 0.29429104924201965,\n",
       " 0.4174748361110687,\n",
       " 0.20192405581474304,\n",
       " 0.2992776036262512,\n",
       " 0.29445838928222656,\n",
       " 0.20181342959403992,\n",
       " 0.311365008354187,\n",
       " 0.3143739104270935,\n",
       " 0.37519150972366333,\n",
       " 0.19379723072052002,\n",
       " 0.19302290678024292,\n",
       " 0.43836402893066406,\n",
       " 0.36160629987716675,\n",
       " 0.22502461075782776,\n",
       " 0.355185329914093,\n",
       " 0.3732718825340271,\n",
       " 0.33374854922294617,\n",
       " 0.2908865809440613,\n",
       " 0.287395715713501,\n",
       " 0.3876451253890991,\n",
       " 0.20090338587760925,\n",
       " 0.2558603882789612,\n",
       " 0.20700553059577942,\n",
       " 0.24178791046142578,\n",
       " 0.2168862223625183,\n",
       " 0.2904616892337799,\n",
       " 0.24709069728851318,\n",
       " 0.2362968474626541,\n",
       " 0.2814742624759674,\n",
       " 0.2938682436943054,\n",
       " 0.29407966136932373,\n",
       " 0.2732306122779846,\n",
       " 0.1638937145471573,\n",
       " 0.25299984216690063,\n",
       " 0.27548277378082275,\n",
       " 0.186512753367424,\n",
       " 0.25219136476516724,\n",
       " 0.25912967324256897,\n",
       " 0.33503586053848267,\n",
       " 0.19273939728736877,\n",
       " 0.21113860607147217,\n",
       " 0.2905210852622986,\n",
       " 0.2633650004863739,\n",
       " 0.30048662424087524,\n",
       " 0.23229141533374786,\n",
       " 0.24089930951595306,\n",
       " 0.19646766781806946,\n",
       " 0.42952820658683777,\n",
       " 0.19048872590065002,\n",
       " 0.2842569053173065,\n",
       " 0.2446400374174118,\n",
       " 0.3348771929740906,\n",
       " 0.3755759000778198,\n",
       " 0.22775687277317047,\n",
       " 0.21282151341438293,\n",
       " 0.3108727037906647,\n",
       " 0.3068651258945465,\n",
       " 0.2759617567062378,\n",
       " 0.2654692530632019,\n",
       " 0.3140003979206085,\n",
       " 0.2824988067150116,\n",
       " 0.1883280873298645,\n",
       " 0.27525120973587036,\n",
       " 0.25349244475364685,\n",
       " 0.1710507720708847,\n",
       " 0.27093079686164856,\n",
       " 0.29035985469818115,\n",
       " 0.16807004809379578,\n",
       " 0.1989787220954895,\n",
       " 0.2550216019153595,\n",
       " 0.18475231528282166,\n",
       " 0.2056931108236313,\n",
       " 0.25980058312416077,\n",
       " 0.26299595832824707,\n",
       " 0.20035061240196228,\n",
       " 0.16951234638690948,\n",
       " 0.24487751722335815,\n",
       " 0.22578129172325134,\n",
       " 0.17455711960792542,\n",
       " 0.3356592357158661,\n",
       " 0.277640163898468,\n",
       " 0.30636459589004517,\n",
       " 0.27955907583236694,\n",
       " 0.21466714143753052,\n",
       " 0.2755502462387085,\n",
       " 0.20605593919754028,\n",
       " 0.2102499008178711,\n",
       " 0.1992020159959793,\n",
       " 0.1315734088420868,\n",
       " 0.1726948320865631,\n",
       " 0.22750291228294373,\n",
       " 0.3158162236213684,\n",
       " 0.12605997920036316,\n",
       " 0.24578572809696198,\n",
       " 0.1804061233997345,\n",
       " 0.22694239020347595,\n",
       " 0.13387633860111237,\n",
       " 0.3153451085090637,\n",
       " 0.2999098598957062,\n",
       " 0.3013380169868469,\n",
       " 0.3065987229347229,\n",
       " 0.21209008991718292,\n",
       " 0.1546137034893036,\n",
       " 0.1207660436630249,\n",
       " 0.1541997641324997,\n",
       " 0.22506654262542725,\n",
       " 0.23614808917045593,\n",
       " 0.16323024034500122,\n",
       " 0.17977109551429749,\n",
       " 0.24258458614349365,\n",
       " 0.2875869870185852,\n",
       " 0.21928125619888306,\n",
       " 0.23189039528369904,\n",
       " 0.27614685893058777,\n",
       " 0.1780942678451538,\n",
       " 0.20646801590919495,\n",
       " 0.12756063044071198,\n",
       " 0.21334819495677948,\n",
       " 0.2471495270729065,\n",
       " 0.25901666283607483,\n",
       " 0.2024323046207428,\n",
       " 0.21799981594085693,\n",
       " 0.2203463315963745,\n",
       " 0.16281434893608093,\n",
       " 0.29989731311798096,\n",
       " 0.39235061407089233,\n",
       " 0.1779526025056839,\n",
       " 0.18530620634555817,\n",
       " 0.2642379701137543,\n",
       " 0.31414198875427246,\n",
       " 0.41653209924697876,\n",
       " 0.10604234784841537,\n",
       " 0.29884982109069824,\n",
       " 0.2705899477005005,\n",
       " 0.2023090124130249,\n",
       " 0.2307179868221283,\n",
       " 0.18938037753105164,\n",
       " 0.2624036967754364,\n",
       " 0.20498837530612946,\n",
       " 0.2014218270778656,\n",
       " 0.23930415511131287,\n",
       " 0.1763330101966858,\n",
       " 0.17417973279953003,\n",
       " 0.27188003063201904,\n",
       " 0.23033498227596283,\n",
       " 0.14079329371452332,\n",
       " 0.17494229972362518,\n",
       " 0.18144437670707703,\n",
       " 0.2142091542482376,\n",
       " 0.15341173112392426,\n",
       " 0.21077629923820496,\n",
       " 0.1892404854297638,\n",
       " 0.13450822234153748,\n",
       " 0.22403419017791748,\n",
       " 0.22648584842681885,\n",
       " 0.2560180127620697,\n",
       " 0.20178131759166718,\n",
       " 0.19012825191020966,\n",
       " 0.14205767214298248,\n",
       " 0.1675061285495758,\n",
       " 0.20571565628051758,\n",
       " 0.1173904687166214,\n",
       " 0.16439096629619598,\n",
       " 0.2821193337440491,\n",
       " 0.11547167599201202,\n",
       " 0.12858378887176514,\n",
       " 0.14802142977714539,\n",
       " 0.24619624018669128,\n",
       " 0.3870893120765686,\n",
       " 0.3116684854030609,\n",
       " 0.2014872133731842,\n",
       " 0.2800274193286896,\n",
       " 0.38548576831817627,\n",
       " 0.14083896577358246,\n",
       " 0.20684897899627686,\n",
       " 0.1972467005252838,\n",
       " 0.2065093070268631,\n",
       " 0.23705972731113434,\n",
       " 0.25524717569351196,\n",
       " 0.22151227295398712,\n",
       " 0.1704484075307846,\n",
       " 0.14983698725700378,\n",
       " 0.1808142066001892,\n",
       " 0.19265539944171906,\n",
       " 0.16840845346450806,\n",
       " 0.16029143333435059,\n",
       " 0.08932407200336456,\n",
       " 0.16780953109264374,\n",
       " 0.24994893372058868,\n",
       " 0.12093751132488251,\n",
       " 0.13640472292900085,\n",
       " 0.1105072945356369,\n",
       " 0.05097649246454239,\n",
       " 0.1327478140592575,\n",
       " 0.1610841155052185,\n",
       " 0.09258925914764404,\n",
       " 0.13938197493553162,\n",
       " 0.15967926383018494,\n",
       " 0.1176161915063858,\n",
       " 0.16102324426174164,\n",
       " 0.1466507911682129,\n",
       " 0.1246955543756485,\n",
       " 0.2154480516910553,\n",
       " 0.23259657621383667,\n",
       " 0.2117747664451599,\n",
       " 0.20350651443004608,\n",
       " 0.1892869770526886,\n",
       " 0.12698134779930115,\n",
       " 0.26685065031051636,\n",
       " 0.21660290658473969,\n",
       " 0.2728712558746338,\n",
       " 0.14531415700912476,\n",
       " 0.18242309987545013,\n",
       " 0.1477794349193573,\n",
       " 0.1365681141614914,\n",
       " 0.14708809554576874,\n",
       " 0.20866450667381287,\n",
       " 0.13910514116287231,\n",
       " 0.16182845830917358,\n",
       " 0.3541862964630127,\n",
       " 0.1575690656900406,\n",
       " 0.3184506893157959,\n",
       " 0.19797059893608093,\n",
       " 0.319863498210907,\n",
       " 0.09808985888957977,\n",
       " 0.16651326417922974,\n",
       " 0.11996246874332428,\n",
       " 0.19084994494915009,\n",
       " 0.19905787706375122,\n",
       " 0.15507467091083527,\n",
       " 0.1328166425228119,\n",
       " 0.21945089101791382,\n",
       " 0.1683368682861328,\n",
       " 0.1600249707698822,\n",
       " 0.2690584063529968,\n",
       " 0.23310250043869019,\n",
       " 0.17342570424079895,\n",
       " 0.11726856231689453,\n",
       " 0.2029840648174286,\n",
       " 0.1549449861049652,\n",
       " 0.2102019041776657,\n",
       " 0.11043306440114975,\n",
       " 0.08588724583387375,\n",
       " 0.15421956777572632,\n",
       " 0.12647464871406555,\n",
       " 0.11905165761709213,\n",
       " 0.10569623112678528,\n",
       " 0.10801392793655396,\n",
       " 0.13750222325325012,\n",
       " 0.131611630320549,\n",
       " 0.12260577082633972,\n",
       " 0.204187273979187,\n",
       " 0.213753342628479,\n",
       " 0.08813253045082092,\n",
       " 0.09794609248638153,\n",
       " 0.21501314640045166,\n",
       " 0.07025809586048126,\n",
       " 0.21854087710380554,\n",
       " 0.1378164142370224,\n",
       " 0.1475614756345749,\n",
       " 0.10896725952625275,\n",
       " 0.1581697165966034,\n",
       " 0.12206318974494934,\n",
       " 0.3124585747718811,\n",
       " 0.27486729621887207,\n",
       " 0.16602040827274323,\n",
       " 0.12527963519096375,\n",
       " 0.14931318163871765,\n",
       " 0.10026441514492035,\n",
       " 0.16787868738174438,\n",
       " 0.12510472536087036,\n",
       " 0.2283109724521637,\n",
       " 0.13604561984539032,\n",
       " 0.13303089141845703,\n",
       " 0.08940926939249039,\n",
       " 0.16757544875144958,\n",
       " 0.19671617448329926,\n",
       " 0.1437237560749054,\n",
       " 0.12817233800888062,\n",
       " 0.09734287858009338,\n",
       " 0.16397498548030853,\n",
       " 0.08701705187559128,\n",
       " 0.11471347510814667,\n",
       " 0.15356487035751343,\n",
       " 0.13280782103538513,\n",
       " 0.12743531167507172,\n",
       " 0.1918013095855713,\n",
       " 0.17070966958999634,\n",
       " 0.10185673832893372,\n",
       " 0.1193477064371109,\n",
       " 0.1318347007036209,\n",
       " 0.16956883668899536,\n",
       " 0.12174543738365173,\n",
       " 0.1280653476715088,\n",
       " 0.04884409159421921,\n",
       " 0.12354984134435654,\n",
       " 0.11793062090873718,\n",
       " 0.07809159904718399,\n",
       " 0.2090354561805725,\n",
       " 0.22198575735092163,\n",
       " 0.21786093711853027,\n",
       " 0.13275080919265747,\n",
       " 0.19661733508110046,\n",
       " 0.15989205241203308,\n",
       " 0.18620222806930542,\n",
       " 0.24407723546028137,\n",
       " 0.13417205214500427,\n",
       " 0.13513150811195374,\n",
       " 0.05579324811697006,\n",
       " 0.0464559905230999,\n",
       " 0.0933295413851738,\n",
       " 0.16807621717453003,\n",
       " 0.14080819487571716,\n",
       " 0.1100018322467804,\n",
       " 0.09614881873130798,\n",
       " 0.09912033379077911,\n",
       " 0.10723567008972168,\n",
       " 0.16654834151268005,\n",
       " 0.14438733458518982,\n",
       " 0.09732193499803543,\n",
       " 0.09720177948474884,\n",
       " 0.19972771406173706,\n",
       " 0.1136961355805397,\n",
       " 0.17884135246276855,\n",
       " 0.1324848234653473,\n",
       " 0.05552511289715767,\n",
       " 0.10621877014636993,\n",
       " 0.22040240466594696,\n",
       " 0.08197945356369019,\n",
       " 0.09510564804077148,\n",
       " 0.11508685350418091,\n",
       " 0.08260108530521393,\n",
       " 0.09070881456136703,\n",
       " 0.09764014184474945,\n",
       " 0.12064307928085327,\n",
       " 0.08190134167671204,\n",
       " 0.1632469892501831,\n",
       " 0.07285431027412415,\n",
       " 0.15646344423294067,\n",
       " 0.2437644898891449,\n",
       " 0.07493839412927628,\n",
       " 0.1800563782453537,\n",
       " 0.08742226660251617,\n",
       " 0.12060505151748657,\n",
       " 0.10016284137964249,\n",
       " 0.07301681488752365,\n",
       " 0.04985540360212326,\n",
       " 0.10106661915779114,\n",
       " 0.050937216728925705,\n",
       " 0.09350757300853729,\n",
       " 0.060232676565647125,\n",
       " 0.1380826234817505,\n",
       " 0.09119828790426254,\n",
       " 0.06494274735450745,\n",
       " 0.1085609719157219,\n",
       " 0.13145926594734192,\n",
       " 0.17299817502498627,\n",
       " 0.05866877734661102,\n",
       " 0.18869292736053467,\n",
       " 0.10144733637571335,\n",
       " 0.1762184500694275,\n",
       " 0.18219295144081116,\n",
       " 0.07589654624462128,\n",
       " 0.0335240475833416,\n",
       " 0.11967110633850098,\n",
       " 0.14392979443073273,\n",
       " 0.08784845471382141,\n",
       " 0.10588796436786652,\n",
       " 0.11745090037584305,\n",
       " 0.11272485554218292,\n",
       " 0.09594525396823883,\n",
       " 0.1019887775182724,\n",
       " 0.11897825449705124,\n",
       " 0.08216599375009537,\n",
       " 0.13871511816978455,\n",
       " 0.12265832722187042,\n",
       " 0.06726928055286407,\n",
       " 0.09950275719165802,\n",
       " 0.13872097432613373,\n",
       " 0.06299330294132233,\n",
       " 0.19372081756591797,\n",
       " 0.2783392071723938,\n",
       " 0.11531983315944672,\n",
       " 0.11673925817012787,\n",
       " 0.05598132684826851,\n",
       " 0.1432340145111084,\n",
       " 0.2690355181694031,\n",
       " 0.11306744068861008,\n",
       " 0.08483463525772095,\n",
       " 0.07669878005981445,\n",
       " 0.16762016713619232,\n",
       " 0.12902340292930603,\n",
       " 0.051510073244571686,\n",
       " 0.16805577278137207,\n",
       " 0.07566897571086884,\n",
       " 0.08474374562501907,\n",
       " 0.06152709573507309,\n",
       " 0.050425149500370026,\n",
       " 0.03764869645237923,\n",
       " 0.08432643115520477,\n",
       " 0.08504734188318253,\n",
       " 0.1385367214679718,\n",
       " 0.09890937805175781,\n",
       " 0.15608465671539307,\n",
       " 0.10454084724187851,\n",
       " 0.06362934410572052,\n",
       " 0.09527598321437836,\n",
       " 0.06693705916404724,\n",
       " 0.10677279531955719,\n",
       " 0.10475024580955505,\n",
       " 0.16798709332942963,\n",
       " 0.20778019726276398,\n",
       " 0.10643758624792099,\n",
       " 0.11981654912233353,\n",
       " 0.0659116804599762,\n",
       " 0.14526869356632233,\n",
       " 0.07674182951450348,\n",
       " 0.05280803516507149,\n",
       " 0.09906876087188721,\n",
       " 0.11765377223491669,\n",
       " 0.1413479745388031,\n",
       " 0.1208956241607666,\n",
       " 0.04215330258011818,\n",
       " 0.1139298602938652,\n",
       " 0.09947231411933899,\n",
       " 0.0725971907377243,\n",
       " 0.07375523447990417,\n",
       " 0.11436136066913605,\n",
       " 0.26204386353492737]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_stage_RNN(\n",
      "  (embedding): Embedding(3193, 50)\n",
      "  (rnn_each_step): ModuleList(\n",
      "    (0): GRU(50, 30, batch_first=True, bidirectional=True)\n",
      "    (1): GRU(50, 30, batch_first=True, bidirectional=True)\n",
      "    (2): GRU(50, 30, batch_first=True, bidirectional=True)\n",
      "    (3): GRU(50, 30, batch_first=True, bidirectional=True)\n",
      "    (4): GRU(50, 30, batch_first=True, bidirectional=True)\n",
      "    (5): GRU(50, 30, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (steps_rnn): GRU(60, 30)\n",
      "  (linear): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([3193, 50])\n",
      "rnn_each_step.0.weight_ih_l0 torch.Size([90, 50])\n",
      "rnn_each_step.0.weight_hh_l0 torch.Size([90, 30])\n",
      "rnn_each_step.0.bias_ih_l0 torch.Size([90])\n",
      "rnn_each_step.0.bias_hh_l0 torch.Size([90])\n",
      "rnn_each_step.1.weight_ih_l0 torch.Size([90, 50])\n",
      "rnn_each_step.1.weight_hh_l0 torch.Size([90, 30])\n",
      "rnn_each_step.1.bias_ih_l0 torch.Size([90])\n",
      "rnn_each_step.1.bias_hh_l0 torch.Size([90])\n",
      "rnn_each_step.2.weight_ih_l0 torch.Size([90, 50])\n",
      "rnn_each_step.2.weight_hh_l0 torch.Size([90, 30])\n",
      "rnn_each_step.2.bias_ih_l0 torch.Size([90])\n",
      "rnn_each_step.2.bias_hh_l0 torch.Size([90])\n",
      "rnn_each_step.3.weight_ih_l0 torch.Size([90, 50])\n",
      "rnn_each_step.3.weight_hh_l0 torch.Size([90, 30])\n",
      "rnn_each_step.3.bias_ih_l0 torch.Size([90])\n",
      "rnn_each_step.3.bias_hh_l0 torch.Size([90])\n",
      "rnn_each_step.4.weight_ih_l0 torch.Size([90, 50])\n",
      "rnn_each_step.4.weight_hh_l0 torch.Size([90, 30])\n",
      "rnn_each_step.4.bias_ih_l0 torch.Size([90])\n",
      "rnn_each_step.4.bias_hh_l0 torch.Size([90])\n",
      "rnn_each_step.5.weight_ih_l0 torch.Size([90, 50])\n",
      "rnn_each_step.5.weight_hh_l0 torch.Size([90, 30])\n",
      "rnn_each_step.5.bias_ih_l0 torch.Size([90])\n",
      "rnn_each_step.5.bias_hh_l0 torch.Size([90])\n",
      "steps_rnn.weight_ih_l0 torch.Size([90, 30])\n",
      "steps_rnn.weight_hh_l0 torch.Size([90, 30])\n",
      "steps_rnn.bias_ih_l0 torch.Size([90])\n",
      "steps_rnn.bias_hh_l0 torch.Size([90])\n",
      "linear.weight torch.Size([1, 30])\n",
      "linear.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, val in model.state_dict().items():\n",
    "    print(key, val.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_all_dict = defaultdict(list)\n",
    "labels_all_dict = defaultdict(list)\n",
    "model.eval()\n",
    "for steps_batch, lengths_batch, labels_batch in test_loader:\n",
    "    for step_id in range(6):\n",
    "        lengths_batch[step_id] = lengths_batch[step_id].cuda()\n",
    "        steps_batch[step_id] = steps_batch[step_id].cuda() \n",
    "    logits = model(steps_batch, lengths_batch)\n",
    "    for i in labels_batch.keys():\n",
    "        logits_all_dict[i].extend(list(logits[i].cpu().detach().numpy()))\n",
    "        labels_all_dict[i].extend(list(labels_batch[i].numpy()))\n",
    "auc = []\n",
    "acc = []\n",
    "for i in labels_all_dict.keys():\n",
    "    logits_all_dict[i] = np.array(logits_all_dict[i])\n",
    "    labels_all_dict[i] = np.array(labels_all_dict[i])\n",
    "    auc.append(roc_auc_score(labels_all_dict[i], logits_all_dict[i])) \n",
    "    predicts = (logits_all_dict[i] > 0.5).astype(int)\n",
    "    acc.append(np.mean(predicts==labels_all_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.77354548376727739,\n",
       " 0.93626373626373627,\n",
       " 0.97762460233297988,\n",
       " 0.87709401709401713,\n",
       " 0.84637168141592933]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2affd74fb4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for i in labels_all_dict.keys():\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels_all_dict[i], logits_all_dict[i], pos_label=1)\n",
    "    plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 50])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([90, 50])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([90, 60])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
