{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from embedding import load_emb_vectors, build_emb_weight\n",
    "from loadData import create_dataset_obj, collate_func\n",
    "from model import create_emb_layer, two_stage_RNN, test_model\n",
    "from preprocess import tokenize_dataset, all_tokens_list, build_vocab, token2index_dataset \n",
    "from importlib import reload\n",
    "# reload(loadData)\n",
    "# from loadData import create_dataset_obj, collate_func\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../../data/glove.6B.50d.txt'\n",
    "words_emb_dict = load_emb_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']\n",
    "steps_aug = ['step_one_sp', 'step_two_sp', 'step_three_sp',\n",
    "             'step_four_sp', 'step_five_sp', 'step_six_sp']\n",
    "tags = ['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "        'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "        'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "        'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "        'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "        'tag_cuisine_middle-eastern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug = pd.read_csv('../data/recipe_data_with_aug.csv', index_col=0)\n",
    "data_with_aug_tags = data_with_aug[steps+steps_aug+tags]\n",
    "print(data_with_aug_tags.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing original instruction data')\n",
    "# tokenize each steps on original datasets\n",
    "steps_token = []\n",
    "for step in steps:\n",
    "    steps_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')\n",
    "\n",
    "# tokenize each steps on augmented datasets\n",
    "print('Processing augmented instruction data')\n",
    "steps_aug_token = []\n",
    "for step in steps_aug:\n",
    "    steps_aug_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags = data_with_aug_tags[steps_token+steps_aug_token+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(data_with_aug_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "test_data = test_data[steps_token+tags]\n",
    "#train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2ori_colname = dict(zip(steps_aug_token+tags, steps_token+tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_predicted = ['tag_cuisine_american', 'tag_cuisine_italian', 'tag_cuisine_asian', \n",
    "                 'tag_cuisine_latin-american','tag_cuisine_mediterranean']\n",
    "\n",
    "test_targets = []\n",
    "for row in test_data[tags_predicted].iterrows():\n",
    "    test_targets.append(list(row[1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_types = {\n",
    "    'rnn': nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "    }\n",
    "\n",
    "params = dict(\n",
    "    rnn1_type = 'gru',\n",
    "    rnn2_type = 'gru',\n",
    "    bi = False,\n",
    "    hidden_dim1 = 30,\n",
    "    hidden_dim2 = 30,\n",
    "    num_classes = 1,\n",
    "    \n",
    "    multi_task_train = 'mean_loss' #{'mean_loss', 'random_selection'}\n",
    "    num_epochs = 20,\n",
    "    batch_size = 50,\n",
    "    learning_rate = 0.01,\n",
    "    \n",
    "    add_data_aug = True,\n",
    "    cuda_on = True \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "k = 1 \n",
    "val_auc_kf = []\n",
    "for train_index, val_index in kf.split(train):\n",
    "    print('===================== This is the Kfold {} ====================='.format(k))\n",
    "    k += 1\n",
    "    val_data = train_val_data[steps_token+tags].iloc[val_index]\n",
    "    train_data = train_val_data.iloc[train_index]\n",
    "    \n",
    "    if params['add_data_aug']:\n",
    "        ##### add augmentation to training set by index #####\n",
    "        train_org = train_data[steps_token+tags]\n",
    "        train_aug = train_data[steps_aug_token+tags]\n",
    "        train_aug.rename(index=str, columns=aug2ori_colname, inplace=True)\n",
    "        train_data = pd.concat([train_org, train_aug], axis=0, ignore_index=False)\n",
    "        ##### add augmentation to training set by index #####\n",
    "    else:\n",
    "        train_data = train_data[steps_token+tags]\n",
    "    \n",
    "    train_targets = []\n",
    "    for row in train_data[tags_predicted].iterrows():\n",
    "        train_targets.append(list(row[1].values))\n",
    "    val_targets = []\n",
    "    for row in val_data[tags_predicted].iterrows():\n",
    "        val_targets.append(list(row[1].values))\n",
    "    \n",
    "    train_X = train_data[steps_token]\n",
    "    val_X = val_data[steps_token]\n",
    "    test_X = test_data[steps_token]\n",
    "    all_train_tokens = all_tokens_list(train_X)\n",
    "    max_vocab_size = len(list(set(all_train_tokens)))\n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    emb_weight = build_emb_weight(words_emb_dict, id2token)\n",
    "    train_data_indices = token2index_dataset(train_X, token2id)\n",
    "    val_data_indices = token2index_dataset(val_X, token2id)\n",
    "    test_data_indices = token2index_dataset(test_X, token2id)\n",
    "\n",
    "    # batchify datasets: \n",
    "    batch_size = params['batch_size']\n",
    "    max_sent_len = np.array([94, 86, 87, 90, 98, 91])\n",
    "    train_loader, val_loader, test_loader = create_dataset_obj(train_data_indices, val_data_indices,\n",
    "                                                           test_data_indices, train_targets,\n",
    "                                                           val_targets, test_targets,\n",
    "                                                           batch_size, max_sent_len, \n",
    "                                                           collate_func)\n",
    "    \n",
    "    val_auc, val_acc = model_train(params, tags_predicted, emb_weight, train_loader, val_loader, test_loader)\n",
    "    val_auc_kf.append(val_auc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "def train_model(params, tags_predicted, emb_weight, train_loader, val_loader, test_loader):\n",
    "    num_tasks = len(tag_predicted)\n",
    "    rnn1_type = params['rnn1_type'] \n",
    "    rnn_1 = rnn_types[rnn1_type]\n",
    "    rnn2_type = params['rnn2_type']\n",
    "    rnn_2 = rnn_types[rnn2_type]\n",
    "    bi = params['bi']\n",
    "    hidden_dim1 = params['hidden_dim1']\n",
    "    hidden_dim2 = params['hidden_dim2']\n",
    "    \n",
    "    multi_task_train = params['multi_task_train'] \n",
    "    num_classes = params['num_classes']\n",
    "    batch_size = params['batch_size']\n",
    "    cuda_on = params['cuda_on']\n",
    "\n",
    "    weights_matrix = torch.from_numpy(emb_weight)\n",
    "    model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n",
    "                          cuda_on, num_tasks, num_classes)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print('The number of train parameters', sum([np.prod(p.size()) for p in model_parameters]))\n",
    "    model = model.to(device)\n",
    "\n",
    "    #parameter for training\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs'] # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    #pos_weight=torch.Tensor([40,]).cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss() #torch.nn.BCELoss(); torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (steps_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "            for step_id in range(6):\n",
    "                lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "                steps_batch[step_id] = steps_batch[step_id].to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(steps_batch, lengths_batch)\n",
    "            if multi_task_train == 'mean_loss'\n",
    "                loss_list = []\n",
    "                for task_id in range(num_tasks):\n",
    "                    loss_list.append(criterion(logits[task_id], \n",
    "                                          labels_batch[task_id].view(-1,1).float().to(device)))\n",
    "                loss= torch.mean(torch.stack(loss_list))\n",
    "            elif multi_task_train == 'random_selection':\n",
    "                task_id = np.random.randint(0, num_tasks)\n",
    "                loss = criterion(logits[task_id], labels_batch[task_id].view(-1,1).float().to(device))\n",
    "            else:\n",
    "                print('multi-task-train-method Error')\n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i % 10 == 0:\n",
    "                # validate\n",
    "#                 print('---------------------')\n",
    "#                 for p in model.parameters():\n",
    "#                     if p.requires_grad:\n",
    "#                         print(p.name, p.size(), p.requires_grad, torch.mean(torch.abs(p.data)), torch.mean(torch.abs(p.grad)))\n",
    "#                         break\n",
    "                val_auc, val_acc = test_model(val_loader, model)\n",
    "                print('{}/{}, Step:{}/{}, TrainLoss:{:.6f}, ValAUC:{} ValAcc:{}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(train_loader), loss, val_auc, val_acc))\n",
    "        val_auc, val_acc = test_model(val_loader, model)\n",
    "        train_auc, train_acc = test_model(train_loader, model)\n",
    "        print('Epoch: [{}/{}], trainAUC: {}, trainAcc: {}'.format(epoch+1, num_epochs, train_auc, train_acc))\n",
    "        print('Epoch: [{}/{}], ValAUC: {}, ValAcc: {}'.format(epoch+1, num_epochs, val_auc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide which tag to predict for trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.sum()/data_cuisine_tags.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tag: tag_cuisine_american, which 27.3525% are 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary and indexing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_cuisine_indian            0.023525  85% auc\n",
    "tag_cuisine_nordic            0.000399\n",
    "tag_cuisine_european          0.012360\n",
    "tag_cuisine_asian             0.182217  98% auc\n",
    "tag_cuisine_mexican           0.013557\n",
    "tag_cuisine_latin-american    0.094896  90% auc\n",
    "tag_cuisine_french            0.077352  72% auc\n",
    "tag_cuisine_italian           0.233254  80% auc\n",
    "tag_cuisine_african           0.003987\n",
    "tag_cuisine_mediterranean     0.076555  88% auc\n",
    "tag_cuisine_american          0.273525  80% auc\n",
    "tag_cuisine_middle-eastern    0.046252  87% auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
