{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from embedding import load_emb_vectors, build_emb_weight\n",
    "from loadData import create_dataset_obj, collate_func\n",
    "from model import create_emb_layer, two_stage_RNN, test_model\n",
    "from preprocess import tokenize_dataset, all_tokens_list, build_vocab, token2index_dataset \n",
    "from importlib import reload\n",
    "# reload(loadData)\n",
    "# from loadData import create_dataset_obj, collate_func\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../../data/glove.6B.50d.txt'\n",
    "words_emb_dict = load_emb_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']\n",
    "steps_aug = ['step_one_sp', 'step_two_sp', 'step_three_sp',\n",
    "             'step_four_sp', 'step_five_sp', 'step_six_sp']\n",
    "tags = ['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "        'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "        'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "        'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "        'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "        'tag_cuisine_middle-eastern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['step_one', 'step_two', 'step_three', 'step_four', 'step_five',\n",
      "       'step_six', 'step_one_sp', 'step_two_sp', 'step_three_sp',\n",
      "       'step_four_sp', 'step_five_sp', 'step_six_sp', 'tag_cuisine_indian',\n",
      "       'tag_cuisine_nordic', 'tag_cuisine_european', 'tag_cuisine_asian',\n",
      "       'tag_cuisine_mexican', 'tag_cuisine_latin-american',\n",
      "       'tag_cuisine_french', 'tag_cuisine_italian', 'tag_cuisine_african',\n",
      "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
      "       'tag_cuisine_middle-eastern'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_with_aug = pd.read_csv('../data/recipe_data_with_aug.csv', index_col=0)\n",
    "data_with_aug_tags = data_with_aug[steps+steps_aug+tags]\n",
    "print(data_with_aug_tags.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing original instruction data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one has been tokenized.\n",
      "step_two has been tokenized.\n",
      "step_three has been tokenized.\n",
      "step_four has been tokenized.\n",
      "step_five has been tokenized.\n",
      "step_six has been tokenized.\n",
      "Processing augmented instruction data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one_sp has been tokenized.\n",
      "step_two_sp has been tokenized.\n",
      "step_three_sp has been tokenized.\n",
      "step_four_sp has been tokenized.\n",
      "step_five_sp has been tokenized.\n",
      "step_six_sp has been tokenized.\n"
     ]
    }
   ],
   "source": [
    "print('Processing original instruction data')\n",
    "# tokenize each steps on original datasets\n",
    "steps_token = []\n",
    "for step in steps:\n",
    "    steps_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')\n",
    "\n",
    "# tokenize each steps on augmented datasets\n",
    "print('Processing augmented instruction data')\n",
    "steps_aug_token = []\n",
    "for step in steps_aug:\n",
    "    steps_aug_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags = data_with_aug_tags[steps_token+steps_aug_token+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['step_one_token', 'step_two_token', 'step_three_token',\n",
       "       'step_four_token', 'step_five_token', 'step_six_token',\n",
       "       'step_one_sp_token', 'step_two_sp_token', 'step_three_sp_token',\n",
       "       'step_four_sp_token', 'step_five_sp_token', 'step_six_sp_token',\n",
       "       'tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
       "       'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
       "       'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
       "       'tag_cuisine_italian', 'tag_cuisine_african',\n",
       "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
       "       'tag_cuisine_middle-eastern'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_aug_tags.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data, test_data = train_test_split(data_with_aug_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "test_data = test_data[steps_token+tags]\n",
    "#train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2ori_colname = dict(zip(steps_aug_token+tags, steps_token+tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_predicted = ['tag_cuisine_american', 'tag_cuisine_italian', 'tag_cuisine_asian', \n",
    "                 'tag_cuisine_latin-american','tag_cuisine_mediterranean']\n",
    "\n",
    "test_targets = []\n",
    "for row in test_data[tags_predicted].iterrows():\n",
    "    test_targets.append(list(row[1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_types = {\n",
    "    'rnn': nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "    }\n",
    "\n",
    "params = dict(\n",
    "    rnn1_type = 'gru',\n",
    "    rnn2_type = 'gru',\n",
    "    bi = False,\n",
    "    tags_predicted = tags_predicted,\n",
    "    \n",
    "    hidden_dim1 = 30,\n",
    "    hidden_dim2 = 30,\n",
    "    num_classes = 1,\n",
    "    \n",
    "    multi_task_train = 'mean_loss', #{'mean_loss', 'random_selection'}\n",
    "    num_epochs = 20,\n",
    "    batch_size = 50,\n",
    "    learning_rate = 0.01,\n",
    "    step_max_descent = 3,\n",
    "    \n",
    "    add_data_aug = True,\n",
    "    cuda_on = True \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== This is the Kfold 1 =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/pandas-0.22.0-py3.6-linux-x86_64.egg/pandas/core/frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-571d08d5e884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                            collate_func)\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mval_auc_kf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-6c727beed297>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, emb_weight, train_loader, val_loader, test_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mweights_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n\u001b[0;32m---> 21\u001b[0;31m                           cuda_on, num_tasks, num_classes)\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of train parameters'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/CapstonePlated/Plated_Recipe_Tags_Predict/src/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, cuda_on, weights_matrix, num_classes)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_emb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# module for steps in the fisrt stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/tx443/CapstonePlated/Plated_Recipe_Tags_Predict/src/model.py\u001b[0m in \u001b[0;36mcreate_emb_layer\u001b[0;34m(weights_matrix, trainable)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_emb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0memb_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0memb_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights_matrix\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "k = 1 \n",
    "val_auc_kf = []\n",
    "for train_index, val_index in kf.split(train_val_data):\n",
    "    print('===================== This is the Kfold {} ====================='.format(k))\n",
    "    k += 1\n",
    "    val_data = train_val_data[steps_token+tags].iloc[val_index]\n",
    "    train_data = train_val_data.iloc[train_index]\n",
    "    \n",
    "    if params['add_data_aug']:\n",
    "        ##### add augmentation to training set by index #####\n",
    "        train_org = train_data[steps_token+tags]\n",
    "        train_aug = train_data[steps_aug_token+tags]\n",
    "        train_aug.rename(index=str, columns=aug2ori_colname, inplace=True)\n",
    "        train_data = pd.concat([train_org, train_aug], axis=0, ignore_index=False)\n",
    "        ##### add augmentation to training set by index #####\n",
    "    else:\n",
    "        train_data = train_data[steps_token+tags]\n",
    "    \n",
    "    train_targets = []\n",
    "    for row in train_data[tags_predicted].iterrows():\n",
    "        train_targets.append(list(row[1].values))\n",
    "    val_targets = []\n",
    "    for row in val_data[tags_predicted].iterrows():\n",
    "        val_targets.append(list(row[1].values))\n",
    "    \n",
    "    train_X = train_data[steps_token]\n",
    "    val_X = val_data[steps_token]\n",
    "    test_X = test_data[steps_token]\n",
    "    all_train_tokens = all_tokens_list(train_X)\n",
    "    max_vocab_size = len(list(set(all_train_tokens)))\n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    emb_weight = build_emb_weight(words_emb_dict, id2token)\n",
    "    train_data_indices = token2index_dataset(train_X, token2id)\n",
    "    val_data_indices = token2index_dataset(val_X, token2id)\n",
    "    test_data_indices = token2index_dataset(test_X, token2id)\n",
    "\n",
    "    # batchify datasets: \n",
    "    batch_size = params['batch_size']\n",
    "    max_sent_len = np.array([94, 86, 87, 90, 98, 91])\n",
    "    train_loader, val_loader, test_loader = create_dataset_obj(train_data_indices, val_data_indices,\n",
    "                                                           test_data_indices, train_targets,\n",
    "                                                           val_targets, test_targets,\n",
    "                                                           batch_size, max_sent_len, \n",
    "                                                           collate_func)\n",
    "    \n",
    "    val_auc, val_acc = train_model(params, emb_weight, train_loader, val_loader, test_loader)\n",
    "    val_auc_kf.append(val_auc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "def train_model(params, emb_weight, train_loader, val_loader, test_loader):\n",
    "    rnn1_type = params['rnn1_type'] \n",
    "    rnn_1 = rnn_types[rnn1_type]\n",
    "    rnn2_type = params['rnn2_type']\n",
    "    rnn_2 = rnn_types[rnn2_type]\n",
    "    bi = params['bi']\n",
    "    tags_predicted = params['tags_predicted']\n",
    "    num_tasks = len(tags_predicted)\n",
    "\n",
    "    hidden_dim1 = params['hidden_dim1']\n",
    "    hidden_dim2 = params['hidden_dim2']\n",
    "    \n",
    "    multi_task_train = params['multi_task_train'] \n",
    "    num_classes = params['num_classes']\n",
    "    batch_size = params['batch_size']\n",
    "    cuda_on = params['cuda_on']\n",
    "\n",
    "    weights_matrix = torch.from_numpy(emb_weight)\n",
    "    model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n",
    "                          cuda_on, num_tasks, num_classes)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print('The number of train parameters', sum([np.prod(p.size()) for p in model_parameters]))\n",
    "    model = model.to(device)\n",
    "\n",
    "    #parameter for training\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs'] # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    #pos_weight=torch.Tensor([40,]).cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss() #torch.nn.BCELoss(); torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (steps_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "            for step_id in range(6):\n",
    "                lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "                steps_batch[step_id] = steps_batch[step_id].to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(steps_batch, lengths_batch)\n",
    "            if multi_task_train == 'mean_loss':\n",
    "                loss_list = []\n",
    "                for task_id in range(num_tasks):\n",
    "                    loss_list.append(criterion(logits[task_id], \n",
    "                                          labels_batch[task_id].view(-1,1).float().to(device)))\n",
    "                loss= torch.mean(torch.stack(loss_list))\n",
    "            elif multi_task_train == 'random_selection':\n",
    "                task_id = np.random.randint(0, num_tasks)\n",
    "                loss = criterion(logits[task_id], labels_batch[task_id].view(-1,1).float().to(device))\n",
    "            else:\n",
    "                print('multi-task-train-method Error')\n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i % 10 == 0:\n",
    "                # validate\n",
    "#                 print('---------------------')\n",
    "#                 for p in model.parameters():\n",
    "#                     if p.requires_grad:\n",
    "#                         print(p.name, p.size(), p.requires_grad, torch.mean(torch.abs(p.data)), torch.mean(torch.abs(p.grad)))\n",
    "#                         break\n",
    "                val_auc, val_acc = test_model(val_loader, model)\n",
    "                print('{}/{}, Step:{}/{}, TrainLoss:{:.6f}, ValAUC:{} ValAcc:{}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(train_loader), loss, val_auc, val_acc))\n",
    "        val_auc, val_acc = test_model(val_loader, model)\n",
    "        train_auc, train_acc = test_model(train_loader, model)\n",
    "        print('Epoch: [{}/{}], trainAUC: {}, trainAcc: {}'.format(epoch+1, num_epochs, train_auc, train_acc))\n",
    "        print('Epoch: [{}/{}], ValAUC: {}, ValAcc: {}'.format(epoch+1, num_epochs, val_auc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide which tag to predict for trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.sum()/data_cuisine_tags.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tag: tag_cuisine_american, which 27.3525% are 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary and indexing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_cuisine_indian            0.023525  85% auc\n",
    "tag_cuisine_nordic            0.000399\n",
    "tag_cuisine_european          0.012360\n",
    "tag_cuisine_asian             0.182217  98% auc\n",
    "tag_cuisine_mexican           0.013557\n",
    "tag_cuisine_latin-american    0.094896  90% auc\n",
    "tag_cuisine_french            0.077352  72% auc\n",
    "tag_cuisine_italian           0.233254  80% auc\n",
    "tag_cuisine_african           0.003987\n",
    "tag_cuisine_mediterranean     0.076555  88% auc\n",
    "tag_cuisine_american          0.273525  80% auc\n",
    "tag_cuisine_middle-eastern    0.046252  87% auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
