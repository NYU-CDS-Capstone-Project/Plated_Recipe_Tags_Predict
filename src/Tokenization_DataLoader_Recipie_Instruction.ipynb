{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "#BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is the output of \"Consolidated Data Cleaning\"\n",
    "data_all = pd.read_csv('cleaned_recipe_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'external_id', 'title', 'subtitle', 'carbs', 'fat',\n",
       "       'protein', 'chef_id', 'calories', 'cooking_tips', 'story',\n",
       "       'ingredients_display', 'step_one', 'step_two', 'step_three',\n",
       "       'step_four', 'step_five', 'step_six', 'recipe_tags',\n",
       "       'tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
       "       'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
       "       'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
       "       'tag_cuisine_italian', 'tag_cuisine_african',\n",
       "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
       "       'tag_cuisine_middle-eastern'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intruction = data_all[['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cuisine_tags = data_all[['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "       'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "       'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "       'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "       'tag_cuisine_middle-eastern']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preheat oven to 425Â°F. Pat chicken dry with paper towel and season all over with .5 teaspoon salt and black pepper as desired. Heat 1 tablespoon olive oil in a medium pan over medium-high heat. When oil is shimmering, add chicken and sear until cooked through and no longer pink, about 6 minutes per side. Transfer chicken to a plate or cutting board, and set aside to rest. Wipe pan clean and reserve for cooking salsa.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_intruction.step_one[1083]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and remove punctuation\n",
    "def tokenizer(sent):\n",
    "    #print(sent)\n",
    "    if pd.isnull(sent):\n",
    "        words = []\n",
    "    else:\n",
    "        tokens = word_tokenize(sent)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(step_n):\n",
    "    \"\"\"returns tokenization for each step, training set tokenizatoin\"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in step_n:\n",
    "        tokens = tokenizer(sample)\n",
    "        token_dataset.append(tokens)\n",
    "    return token_dataset\n",
    "\n",
    "def all_tokens_list(train_data):\n",
    "    \"\"\"returns all tokens of instruction (all steps) for creating vocabulary\"\"\"\n",
    "    all_tokens = []\n",
    "    for columns in train_data.columns:\n",
    "        for sample in train_data[columns]:\n",
    "            all_tokens += sample[:]\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one has been tokenized.\n",
      "step_two has been tokenized.\n",
      "step_three has been tokenized.\n",
      "step_four has been tokenized.\n",
      "step_five has been tokenized.\n",
      "step_six has been tokenized.\n"
     ]
    }
   ],
   "source": [
    "# tokenize each steps\n",
    "data_instruction_tokenized = pd.DataFrame()\n",
    "for steps in data_intruction.columns:\n",
    "    data_instruction_tokenized[steps] = tokenize_dataset(data_intruction[steps])\n",
    "    print(steps, 'has been tokenized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step_one</th>\n",
       "      <th>step_two</th>\n",
       "      <th>step_three</th>\n",
       "      <th>step_four</th>\n",
       "      <th>step_five</th>\n",
       "      <th>step_six</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[preheat, oven, to, place, butter, in, a, smal...</td>\n",
       "      <td>[on, a, baking, sheet, toss, carrots, green, b...</td>\n",
       "      <td>[while, vegetables, roast, mince, garlic, and,...</td>\n",
       "      <td>[pat, steaks, dry, with, paper, towel, and, se...</td>\n",
       "      <td>[once, roasted, remove, vegetables, from, oven...</td>\n",
       "      <td>[once, steaks, have, rested, find, the, direct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[in, a, small, pot, combine, rice, cups, water...</td>\n",
       "      <td>[rinse, bell, pepper, and, halve, lengthwise, ...</td>\n",
       "      <td>[pat, steaks, dry, with, paper, towel, and, se...</td>\n",
       "      <td>[while, steaks, sear, heat, teaspoons, canola,...</td>\n",
       "      <td>[return, pan, from, steaks, to, medium, heat, ...</td>\n",
       "      <td>[once, rested, cut, steaks, against, the, grai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[in, a, small, pot, combine, rice, cups, water...</td>\n",
       "      <td>[while, rice, cooks, halve, bok, choy, lengthw...</td>\n",
       "      <td>[heat, sesame, oil, in, a, medium, nonstick, p...</td>\n",
       "      <td>[add, cooked, rice, and, tablespoon, canola, o...</td>\n",
       "      <td>[pat, pork, chops, dry, with, a, paper, towel,...</td>\n",
       "      <td>[once, rested, cut, pork, into, slices, then, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[preheat, oven, to, in, a, small, pot, combine...</td>\n",
       "      <td>[rinse, all, produce, halve, cucumber, lengthw...</td>\n",
       "      <td>[on, half, of, baking, sheet, toss, chickpeas,...</td>\n",
       "      <td>[while, chickpeas, and, tomatoes, roast, place...</td>\n",
       "      <td>[while, feta, bakes, in, a, large, bowl, whisk...</td>\n",
       "      <td>[divide, quinoa, between, serving, bowls, then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[preheat, oven, to, pat, chicken, dry, with, p...</td>\n",
       "      <td>[while, chicken, cooks, halve, lime, cut, half...</td>\n",
       "      <td>[return, pan, from, chicken, to, mediumhigh, h...</td>\n",
       "      <td>[stack, tortillas, wrap, in, foil, and, place,...</td>\n",
       "      <td>[stir, chipotle, paste, and, teaspoon, salt, i...</td>\n",
       "      <td>[divide, warmed, tortillas, between, serving, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            step_one  \\\n",
       "0  [preheat, oven, to, place, butter, in, a, smal...   \n",
       "1  [in, a, small, pot, combine, rice, cups, water...   \n",
       "2  [in, a, small, pot, combine, rice, cups, water...   \n",
       "3  [preheat, oven, to, in, a, small, pot, combine...   \n",
       "4  [preheat, oven, to, pat, chicken, dry, with, p...   \n",
       "\n",
       "                                            step_two  \\\n",
       "0  [on, a, baking, sheet, toss, carrots, green, b...   \n",
       "1  [rinse, bell, pepper, and, halve, lengthwise, ...   \n",
       "2  [while, rice, cooks, halve, bok, choy, lengthw...   \n",
       "3  [rinse, all, produce, halve, cucumber, lengthw...   \n",
       "4  [while, chicken, cooks, halve, lime, cut, half...   \n",
       "\n",
       "                                          step_three  \\\n",
       "0  [while, vegetables, roast, mince, garlic, and,...   \n",
       "1  [pat, steaks, dry, with, paper, towel, and, se...   \n",
       "2  [heat, sesame, oil, in, a, medium, nonstick, p...   \n",
       "3  [on, half, of, baking, sheet, toss, chickpeas,...   \n",
       "4  [return, pan, from, chicken, to, mediumhigh, h...   \n",
       "\n",
       "                                           step_four  \\\n",
       "0  [pat, steaks, dry, with, paper, towel, and, se...   \n",
       "1  [while, steaks, sear, heat, teaspoons, canola,...   \n",
       "2  [add, cooked, rice, and, tablespoon, canola, o...   \n",
       "3  [while, chickpeas, and, tomatoes, roast, place...   \n",
       "4  [stack, tortillas, wrap, in, foil, and, place,...   \n",
       "\n",
       "                                           step_five  \\\n",
       "0  [once, roasted, remove, vegetables, from, oven...   \n",
       "1  [return, pan, from, steaks, to, medium, heat, ...   \n",
       "2  [pat, pork, chops, dry, with, a, paper, towel,...   \n",
       "3  [while, feta, bakes, in, a, large, bowl, whisk...   \n",
       "4  [stir, chipotle, paste, and, teaspoon, salt, i...   \n",
       "\n",
       "                                            step_six  \n",
       "0  [once, steaks, have, rested, find, the, direct...  \n",
       "1  [once, rested, cut, steaks, against, the, grai...  \n",
       "2  [once, rested, cut, pork, into, slices, then, ...  \n",
       "3  [divide, quinoa, between, serving, bowls, then...  \n",
       "4  [divide, warmed, tortillas, between, serving, ...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_instruction_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_instruction_tokenized.shape[0] == data_cuisine_tags.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, test_data, y_train, test_tags = train_test_split(data_instruction_tokenized, data_cuisine_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All tokens from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form all tokens list\n",
    "all_train_tokens = all_tokens_list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide which tag to predict for trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.sum()/data_cuisine_tags.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tag: tag_cuisine_american, which 27.3525% are 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build vocabulary and indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3489"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_counter.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = len(list(set(all_train_tokens)))):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 49 ; token while\n",
      "Token while; token id 49\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = len(list(set(all_train_tokens))))\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct data strcuture for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 2031\n",
      "Val dataset size is 226\n",
      "Test dataset size is 251\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    \"\"\"returns [[[step1 indices],[step2 indices],...,[step6 indices]],[],[],...]\"\"\"\n",
    "    recipie_indices_data = []\n",
    "    for recipie in tokens_data.iterrows():\n",
    "        step_indices_data = []\n",
    "        for step in recipie[1]:\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in step]\n",
    "            step_indices_data.append(index_list)\n",
    "        recipie_indices_data.append(step_indices_data)\n",
    "    return recipie_indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data)\n",
    "val_data_indices = token2index_dataset(val_data)\n",
    "test_data_indices = token2index_dataset(test_data)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "class IntructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, tags_list):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data_list: list of recipie tokens \n",
    "        @param target_list: list of single tag, i.e. 'tag_cuisine_american'\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.tags_list = tags_list\n",
    "        assert (len(self.data_list) == len(self.tags_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call recipie[i]\n",
    "        \"\"\"\n",
    "        recipie = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        step1_idx = recipie[0][:MAX_SENTENCE_LENGTH]\n",
    "        step2_idx = recipie[1][:MAX_SENTENCE_LENGTH]\n",
    "        step3_idx = recipie[2][:MAX_SENTENCE_LENGTH]\n",
    "        step4_idx = recipie[3][:MAX_SENTENCE_LENGTH]       \n",
    "        step5_idx = recipie[4][:MAX_SENTENCE_LENGTH]\n",
    "        step6_idx = recipie[5][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.tags_list[key]\n",
    "        return [[step1_idx, step2_idx, step3_idx, step4_idx, step5_idx, step6_idx], \n",
    "                [len(step1_idx),len(step2_idx), len(step3_idx),len(step4_idx), len(step5_idx),len(step6_idx)], \n",
    "                label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[-1])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_list = []\n",
    "        for i, step in enumerate(datum[0]):\n",
    "            padded_vec = np.pad(np.array(step), \n",
    "                                    pad_width=((0, MAX_SENTENCE_LENGTH-datum[1][i])), \n",
    "                                    mode=\"constant\", constant_values=0)\n",
    "            padded_list.append(padded_vec)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_dataset = IntructionDataset(train_data_indices, list(train_tags['tag_cuisine_american']))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_dataset = IntructionDataset(val_data_indices, list(val_tags['tag_cuisine_american']))\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IntructionDataset(test_data_indices, list(test_tags['tag_cuisine_american']))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   7,  297,  105,   17,   74,   20,   14,    2,  120,   15,    3,   23,\n",
      "            5,  572,    2,  780,    2,   39,    3,   52,   85,   59,   17, 1359,\n",
      "           80,  550,  952,  245,   43,  525,  100,  550,    5,  360,   69,  346,\n",
      "          587,   34,  780,  140,    2,  447,   53,  155,   31,   77,  385,  360,\n",
      "         2509,    5,  296,  655,    2,   34, 1359,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  55,  152, 3088,  177,    3,  114,   96,   90,    5,  996,  181,  510,\n",
      "          121,   11,  465,  746,    2,  131,    6,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [ 128,  597,    2,   39,    6,  240,    7, 1382,    3,   96,    5,  296,\n",
      "            2,   90,    5,  719,  597,    2,  240,   77,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  77, 1684, 1417,    5,  177,    2,  248,   34,  323,  131,   26,  127,\n",
      "         2510, 1065,  652,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  85,  477,  142,   80,   96,    2,   90,    5,  290,   77,    5,   38,\n",
      "          256,  276,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  47, 1370,  148,   26, 2059,   85,  478,  142,   80,   96,   90,    5,\n",
      "           93,   24,    5, 1370,  121,   11,   13,   53,  137,    5,  223,    2,\n",
      "           77,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   7,   95,    2,   59,   17,  434,    3,   13,    5,  854,    2,   39,\n",
      "            3,   52,   98,    2,    7,   14,    2,   15,   19,  115,   85,   80,\n",
      "          226,  167,    5,   34,  434,    2,  210,  127,  764,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   7,  152,   95,    2,  149,  139,    3,   23,    5,  134,  390,    2,\n",
      "           66,    3,  103,  210,    5,  996,  514,  184,   28,   90,    2, 2214,\n",
      "           58,  652, 2215,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  49,  436,  619,  877,   34,  101,  588,   64,    6,    4,   83,  907,\n",
      "           23,    2,  866,    6, 1272,    9,  657,  416,   39,    6,  845, 1017,\n",
      "            2,  232,   53,    3,   52,   85,  460,  715,   80,  114,   96,   90,\n",
      "            5,  436,  121,   11,  845,  557,    2,  137,    5,  327,  460,  131,\n",
      "            6,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [  85, 1503,   70,    2,  177,  142,   80,   96,  877, 1503,   70,   62,\n",
      "          155,   31,   90,  422,    5,  716,  542,    2,   77,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([[63, 45, 73, 42, 27, 55],\n",
      "        [39, 32, 68, 60, 38, 19],\n",
      "        [33, 37, 35, 28, 62, 20],\n",
      "        [36, 52, 57, 63, 37, 15],\n",
      "        [57, 40, 26, 44, 43, 14],\n",
      "        [61, 30, 66, 55, 53, 25],\n",
      "        [41, 28, 65, 39, 50, 33],\n",
      "        [51, 59, 60, 44, 50, 27],\n",
      "        [38, 59, 63, 70, 63, 49],\n",
      "        [36, 63, 13, 46, 49, 21]])\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (data)\n",
    "    print(lengths)\n",
    "    print (labels)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
