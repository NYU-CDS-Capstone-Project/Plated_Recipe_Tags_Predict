{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "# get_ipython().run_line_magic('autoreload', '2')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from embedding import load_emb_vectors, build_emb_weight\n",
    "from loadData import create_dataset_obj, collate_func\n",
    "from model import create_emb_layer, two_stage_RNN, test_model\n",
    "from preprocess import tokenize_dataset, all_tokens_list, build_vocab, token2index_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "def train_model(params, emb_weight, train_loader, val_loader, test_loader, device):\n",
    "    tags_predicted = params['tags_predicted']\n",
    "    num_tasks = len(tags_predicted)\n",
    "    rnn1_type = params['rnn1_type'] \n",
    "    rnn_1 = rnn_types[rnn1_type]\n",
    "    rnn2_type = params['rnn2_type']\n",
    "    rnn_2 = rnn_types[rnn2_type]\n",
    "    bi = params['bi']\n",
    "    hidden_dim1 = params['hidden_dim1']\n",
    "    hidden_dim2 = params['hidden_dim2']\n",
    "    \n",
    "    multi_task_train = params['multi_task_train'] \n",
    "    num_classes = params['num_classes']\n",
    "    batch_size = params['batch_size']\n",
    "    cuda_on = params['cuda_on']\n",
    "\n",
    "    weights_matrix = torch.from_numpy(emb_weight)\n",
    "    model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n",
    "                          cuda_on, weights_matrix, num_tasks, num_classes)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print('The number of train parameters', sum([np.prod(p.size()) for p in model_parameters]))\n",
    "    model = model.to(device)\n",
    "\n",
    "    #parameter for training\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs'] # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    #pos_weight=torch.Tensor([40,]).cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss() #torch.nn.BCELoss(); torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_AUC_list = []\n",
    "    train_ACC_list = []\n",
    "    val_AUC_list = []\n",
    "    val_ACC_list = []\n",
    "    max_val_auc = 0\n",
    "    step_max_descent = params['step_max_descent']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (steps_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "            for step_id in range(6):\n",
    "                lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "                steps_batch[step_id] = steps_batch[step_id].to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(steps_batch, lengths_batch)\n",
    "            if num_tasks == 1:\n",
    "                task_id = 0\n",
    "            else:\n",
    "                print('Task number is greater than 1')\n",
    "            loss = criterion(logits[task_id], labels_batch[task_id].view(-1,1).float().to(device))\n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 10 iterations\n",
    "            if i % 10 == 0:\n",
    "                val_auc, val_acc = test_model(val_loader, model)\n",
    "                val_ACC_list.append(val_acc[task_id])\n",
    "                val_AUC_list.append(val_auc[task_id])\n",
    "                print('{}/{}, Step:{}/{}, TrainLoss:{:.6f}, ValAUC:{} ValAcc:{}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(train_loader), loss, val_auc, val_acc))\n",
    "                \n",
    "                # train_auc, train_acc = test_model(train_loader, model)\n",
    "                # train_AUC_list.append(train_auc)\n",
    "                # train_ACC_list.append(train_acc)\n",
    "                \n",
    "                # early stop\n",
    "                if max_val_auc < val_auc[task_id]:\n",
    "                    max_val_auc = val_auc[task_id]\n",
    "                    step_num_descent = 0\n",
    "                else:\n",
    "                    step_num_descent += 1\n",
    "                if step_max_descent == step_num_descent:\n",
    "                    print('early stop!')\n",
    "                    break\n",
    "        val_auc, val_acc = test_model(val_loader, model)\n",
    "        train_auc, train_acc = test_model(train_loader, model)\n",
    "        print('Epoch: [{}/{}], trainAUC: {}, trainAcc: {}'.format(epoch+1, num_epochs, train_auc, train_acc))\n",
    "        print('Epoch: [{}/{}], ValAUC: {}, ValAcc: {}'.format(epoch+1, num_epochs, val_auc, val_acc))\n",
    "        if step_max_descent == step_num_descent:\n",
    "            break\n",
    "    val_auc_mean = np.mean(val_AUC_list[-step_max_descent*2-1:])\n",
    "    val_acc_mean = np.mean(val_ACC_list[-step_max_descent*2-1:])\n",
    "    return val_auc_mean, val_acc_mean\n",
    "\n",
    "# main \n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# get device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "data_path='/scratch/tx443/CapstonePlated/Plated_Recipe_Tags_Predict/data/'\n",
    "\n",
    "# ### Get pre-trained embeddings\n",
    "# # encode the pretrained embedding to text file\n",
    "# model = KeyedVectors.load_word2vec_format('/home/hb1500/Plated/vocab.bin', binary=True)\n",
    "# model.save_word2vec_format('pretrained_embd.txt', binary=False)\n",
    "\n",
    "fname = '/scratch/tx443/CapstonePlated/data/glove.6B.50d.txt'\n",
    "words_emb_dict = load_emb_vectors(fname)\n",
    "\n",
    "# ### Load Cleaned Data \n",
    "\n",
    "steps = ['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']\n",
    "steps_aug = ['step_one_sp', 'step_two_sp', 'step_three_sp',\n",
    "             'step_four_sp', 'step_five_sp', 'step_six_sp']\n",
    "tags = ['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "        'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "        'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "        'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "        'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "        'tag_cuisine_middle-eastern']\n",
    "\n",
    "data_with_aug = pd.read_csv(data_path+'recipe_data_with_aug.csv', index_col=0)\n",
    "data_with_aug_tags = data_with_aug[steps+steps_aug+tags]\n",
    "print('column names of augmented data: ', data_with_aug_tags.columns)\n",
    "\n",
    "\n",
    "# ### Tokenization\n",
    "print('Processing original instruction data')\n",
    "# tokenize each steps on original datasets\n",
    "steps_token = []\n",
    "for step in steps:\n",
    "    steps_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')\n",
    "\n",
    "# tokenize each steps on augmented datasets\n",
    "print('Processing augmented instruction data')\n",
    "steps_aug_token = []\n",
    "for step in steps_aug:\n",
    "    steps_aug_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')\n",
    "\n",
    "data_with_aug_tags = data_with_aug_tags[steps_token+steps_aug_token+tags]\n",
    "print('column names of augmented data(after tokenize): ', data_with_aug_tags.columns)\n",
    "\n",
    "\n",
    "# ### Split train and test sets\n",
    "\n",
    "train_val_data, test_data = train_test_split(data_with_aug_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "test_data = test_data[steps_token+tags]\n",
    "#train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)\n",
    "\n",
    "# map colnames from autmented to original \n",
    "aug2ori_colname = dict(zip(steps_aug_token+tags, steps_token+tags))\n",
    "\n",
    "\n",
    "# Cross validation for train and validation \n",
    "# tags \n",
    "tags_predicted = ['tag_cuisine_american']\n",
    "test_targets = []\n",
    "for row in test_data[tags_predicted].iterrows():\n",
    "    test_targets.append(list(row[1].values))\n",
    "\n",
    "\n",
    "# parameters\n",
    "rnn_types = {\n",
    "    'rnn': nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "}\n",
    "\n",
    "params = dict(\n",
    "    rnn1_type = 'gru',\n",
    "    rnn2_type = 'gru',\n",
    "    bi = True,\n",
    "    tags_predicted = tags_predicted,\n",
    "    \n",
    "    hidden_dim1 = 30,\n",
    "    hidden_dim2 = 30,\n",
    "    num_classes = 1,\n",
    "    \n",
    "    multi_task_train = None, #{'mean_loss', 'random_selection'}\n",
    "    num_epochs = 10,\n",
    "    batch_size = 50,\n",
    "    learning_rate = 0.01,\n",
    "    step_max_descent = 3,\n",
    "    \n",
    "    add_data_aug = True,\n",
    "    cuda_on = True\n",
    "    )\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "k = 1 \n",
    "val_auc_kf = []\n",
    "for train_index, val_index in kf.split(train_val_data):\n",
    "    print('===================== This is the Kfold {} ====================='.format(k))\n",
    "    k += 1\n",
    "    val_data = train_val_data[steps_token+tags].iloc[val_index]\n",
    "    train_data = train_val_data.iloc[train_index]\n",
    "    \n",
    "    if params['add_data_aug']:\n",
    "        ##### add augmentation to training set by index #####\n",
    "        train_org = train_data[steps_token+tags]\n",
    "        train_aug = train_data[steps_aug_token+tags]\n",
    "        train_aug.rename(index=str, columns=aug2ori_colname, inplace=True)\n",
    "        train_data = pd.concat([train_org, train_aug], axis=0, ignore_index=False)\n",
    "        ##### add augmentation to training set by index #####\n",
    "    else:\n",
    "        train_data = train_data[steps_token+tags]\n",
    "    \n",
    "    train_targets = []\n",
    "    for row in train_data[tags_predicted].iterrows():\n",
    "        train_targets.append(list(row[1].values))\n",
    "    val_targets = []\n",
    "    for row in val_data[tags_predicted].iterrows():\n",
    "        val_targets.append(list(row[1].values))\n",
    "    \n",
    "    train_X = train_data[steps_token]\n",
    "    val_X = val_data[steps_token]\n",
    "    test_X = test_data[steps_token]\n",
    "    all_train_tokens = all_tokens_list(train_X)\n",
    "    max_vocab_size = len(list(set(all_train_tokens)))\n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    emb_weight = build_emb_weight(words_emb_dict, id2token)\n",
    "    train_data_indices = token2index_dataset(train_X, token2id)\n",
    "    val_data_indices = token2index_dataset(val_X, token2id)\n",
    "    test_data_indices = token2index_dataset(test_X, token2id)\n",
    "\n",
    "    # batchify datasets: \n",
    "    batch_size = params['batch_size']\n",
    "    max_sent_len = np.array([94, 86, 87, 90, 98, 91])\n",
    "    train_loader, val_loader, test_loader = create_dataset_obj(train_data_indices, val_data_indices,\n",
    "                                                           test_data_indices, train_targets,\n",
    "                                                           val_targets, test_targets,\n",
    "                                                           batch_size, max_sent_len, \n",
    "                                                           collate_func)\n",
    "    \n",
    "    val_auc, val_acc = train_model(params, emb_weight, train_loader, val_loader, test_loader, device)\n",
    "    val_auc_kf.append(val_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
