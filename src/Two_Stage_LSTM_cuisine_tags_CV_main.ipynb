{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from embedding import load_emb_vectors, build_emb_weight\n",
    "from loadData import create_dataset_obj, collate_func\n",
    "from model import create_emb_layer, two_stage_RNN, test_model\n",
    "from preprocess import tokenize_dataset, all_tokens_list, build_vocab, token2index_dataset \n",
    "from importlib import reload\n",
    "# reload(loadData)\n",
    "# from loadData import create_dataset_obj, collate_func\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the pretrained embedding to text file\n",
    "model = KeyedVectors.load_word2vec_format('/home/hb1500/Plated/vocab.bin', binary=True)\n",
    "model.save_word2vec_format('pretrained_embd.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../../data/glove.6B.50d.txt'\n",
    "words_emb_dict = load_emb_vectors(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']\n",
    "steps_aug = ['step_one_sp', 'step_two_sp', 'step_three_sp',\n",
    "             'step_four_sp', 'step_five_sp', 'step_six_sp']\n",
    "tags = ['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "        'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "        'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "        'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "        'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "        'tag_cuisine_middle-eastern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug = pd.read_csv('../data/recipe_data_with_aug.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags = data_with_aug[steps+steps_aug+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['step_one', 'step_two', 'step_three', 'step_four', 'step_five',\n",
      "       'step_six', 'step_one_sp', 'step_two_sp', 'step_three_sp',\n",
      "       'step_four_sp', 'step_five_sp', 'step_six_sp', 'tag_cuisine_indian',\n",
      "       'tag_cuisine_nordic', 'tag_cuisine_european', 'tag_cuisine_asian',\n",
      "       'tag_cuisine_mexican', 'tag_cuisine_latin-american',\n",
      "       'tag_cuisine_french', 'tag_cuisine_italian', 'tag_cuisine_african',\n",
      "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
      "       'tag_cuisine_middle-eastern'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_with_aug_tags.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing original instruction data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one has been tokenized.\n",
      "step_two has been tokenized.\n",
      "step_three has been tokenized.\n",
      "step_four has been tokenized.\n",
      "step_five has been tokenized.\n",
      "step_six has been tokenized.\n",
      "Processing augmented instruction data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one_sp has been tokenized.\n",
      "step_two_sp has been tokenized.\n",
      "step_three_sp has been tokenized.\n",
      "step_four_sp has been tokenized.\n",
      "step_five_sp has been tokenized.\n",
      "step_six_sp has been tokenized.\n"
     ]
    }
   ],
   "source": [
    "print('Processing original instruction data')\n",
    "# tokenize each steps on original datasets\n",
    "steps_token = []\n",
    "for step in steps:\n",
    "    steps_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')\n",
    "\n",
    "# tokenize each steps on augmented datasets\n",
    "print('Processing augmented instruction data')\n",
    "steps_aug_token = []\n",
    "for step in steps_aug:\n",
    "    steps_aug_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags = data_with_aug_tags[steps_token+steps_aug_token+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['step_one_token', 'step_two_token', 'step_three_token',\n",
       "       'step_four_token', 'step_five_token', 'step_six_token',\n",
       "       'step_one_sp_token', 'step_two_sp_token', 'step_three_sp_token',\n",
       "       'step_four_sp_token', 'step_five_sp_token', 'step_six_sp_token',\n",
       "       'tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
       "       'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
       "       'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
       "       'tag_cuisine_italian', 'tag_cuisine_african',\n",
       "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
       "       'tag_cuisine_middle-eastern'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_aug_tags.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_data = train_test_split(data_with_aug_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "test_data = test_data[steps_token+tags]\n",
    "#train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2ori_colname = dict(zip(steps_aug_token+tags, steps_token+tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_types = {\n",
    "    'rnn': nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "}\n",
    "\n",
    "params = dict(\n",
    "    rnn1_type = 'gru',\n",
    "    rnn2_type = 'gru',\n",
    "    bi = True,\n",
    "    hidden_dim1 = 30,\n",
    "    hidden_dim2 = 30,\n",
    "    num_classes = 1,\n",
    "    \n",
    "    num_epochs = 10,\n",
    "    batch_size = 50,\n",
    "    learning_rate = 0.01,\n",
    "    step_max_descent = 3,\n",
    "    \n",
    "    add_data_aug = True,\n",
    "    cuda_on = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = 'tag_cuisine_american' \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== This is the Kfold 1 =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/pandas-0.22.0-py3.6-linux-x86_64.egg/pandas/core/frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train parameters 23071\n",
      "1/10, Step:1/73, TrainLoss:0.709951, ValAUC:0.550818 ValAcc:0.754425\n",
      "1/10, Step:11/73, TrainLoss:0.588349, ValAUC:0.572693 ValAcc:0.754425\n",
      "1/10, Step:21/73, TrainLoss:0.445122, ValAUC:0.606536 ValAcc:0.754425\n",
      "1/10, Step:31/73, TrainLoss:0.526425, ValAUC:0.603736 ValAcc:0.754425\n",
      "1/10, Step:41/73, TrainLoss:0.562818, ValAUC:0.595229 ValAcc:0.754425\n",
      "1/10, Step:51/73, TrainLoss:0.711131, ValAUC:0.636601 ValAcc:0.753299\n",
      "1/10, Step:61/73, TrainLoss:0.667172, ValAUC:0.636601 ValAcc:0.754425\n",
      "1/10, Step:71/73, TrainLoss:0.693478, ValAUC:0.644369 ValAcc:0.754425\n",
      "Epoch: [1/10], trainAUC: 0.705179, trainAcc: 0.719114\n",
      "Epoch: [1/10], ValAUC: 0.644369, ValAcc: 0.754425\n",
      "2/10, Step:1/73, TrainLoss:0.780731, ValAUC:0.650577 ValAcc:0.754425\n",
      "2/10, Step:11/73, TrainLoss:0.535162, ValAUC:0.686904 ValAcc:0.753299\n",
      "2/10, Step:21/73, TrainLoss:0.450683, ValAUC:0.760508 ValAcc:0.752173\n",
      "2/10, Step:31/73, TrainLoss:0.488147, ValAUC:0.783916 ValAcc:0.746544\n",
      "2/10, Step:41/73, TrainLoss:0.434917, ValAUC:0.779504 ValAcc:0.727406\n",
      "2/10, Step:51/73, TrainLoss:0.562068, ValAUC:0.786135 ValAcc:0.686878\n",
      "2/10, Step:61/73, TrainLoss:0.406175, ValAUC:0.793189 ValAcc:0.725155\n",
      "2/10, Step:71/73, TrainLoss:0.455657, ValAUC:0.785554 ValAcc:0.709394\n",
      "Epoch: [2/10], trainAUC: 0.870233, trainAcc: 0.658053\n",
      "Epoch: [2/10], ValAUC: 0.785554, ValAcc: 0.709394\n",
      "3/10, Step:1/73, TrainLoss:0.496567, ValAUC:0.786531 ValAcc:0.742041\n",
      "3/10, Step:11/73, TrainLoss:0.235284, ValAUC:0.797997 ValAcc:0.701513\n",
      "3/10, Step:21/73, TrainLoss:0.354892, ValAUC:0.817812 ValAcc:0.669992\n",
      "3/10, Step:31/73, TrainLoss:0.384187, ValAUC:0.813532 ValAcc:0.663237\n",
      "3/10, Step:41/73, TrainLoss:0.403977, ValAUC:0.813743 ValAcc:0.689130\n",
      "3/10, Step:51/73, TrainLoss:0.394261, ValAUC:0.816967 ValAcc:0.695885\n",
      "early stop!\n",
      "Epoch: [3/10], trainAUC: 0.916466, trainAcc: 0.645186\n",
      "Epoch: [3/10], ValAUC: 0.816967, ValAcc: 0.695885\n",
      "===================== This is the Kfold 2 =====================\n",
      "The number of train parameters 23071\n",
      "1/10, Step:1/73, TrainLoss:0.678325, ValAUC:0.595753 ValAcc:0.705752\n",
      "1/10, Step:11/73, TrainLoss:0.670794, ValAUC:0.570721 ValAcc:0.705752\n",
      "1/10, Step:21/73, TrainLoss:0.508842, ValAUC:0.618686 ValAcc:0.705752\n",
      "1/10, Step:31/73, TrainLoss:0.503605, ValAUC:0.649751 ValAcc:0.705752\n",
      "1/10, Step:41/73, TrainLoss:0.620189, ValAUC:0.688312 ValAcc:0.705752\n",
      "1/10, Step:51/73, TrainLoss:0.550578, ValAUC:0.719942 ValAcc:0.703931\n",
      "1/10, Step:61/73, TrainLoss:0.511150, ValAUC:0.772574 ValAcc:0.626547\n",
      "1/10, Step:71/73, TrainLoss:0.655175, ValAUC:0.785420 ValAcc:0.682082\n",
      "Epoch: [1/10], trainAUC: 0.844093, trainAcc: 0.706314\n",
      "Epoch: [1/10], ValAUC: 0.785420, ValAcc: 0.682082\n",
      "2/10, Step:1/73, TrainLoss:0.515292, ValAUC:0.787612 ValAcc:0.613801\n",
      "2/10, Step:11/73, TrainLoss:0.390695, ValAUC:0.792915 ValAcc:0.581026\n",
      "2/10, Step:21/73, TrainLoss:0.561493, ValAUC:0.792184 ValAcc:0.571922\n",
      "2/10, Step:31/73, TrainLoss:0.341334, ValAUC:0.791241 ValAcc:0.674798\n",
      "2/10, Step:41/73, TrainLoss:0.379911, ValAUC:0.807340 ValAcc:0.640203\n",
      "2/10, Step:51/73, TrainLoss:0.180668, ValAUC:0.819973 ValAcc:0.652949\n",
      "2/10, Step:61/73, TrainLoss:0.525243, ValAUC:0.791713 ValAcc:0.618353\n",
      "2/10, Step:71/73, TrainLoss:0.362142, ValAUC:0.795649 ValAcc:0.644755\n",
      "Epoch: [2/10], trainAUC: 0.905667, trainAcc: 0.663513\n",
      "Epoch: [2/10], ValAUC: 0.795649, ValAcc: 0.644755\n",
      "3/10, Step:1/73, TrainLoss:0.351027, ValAUC:0.800245 ValAcc:0.578295\n",
      "early stop!\n",
      "Epoch: [3/10], trainAUC: 0.906663, trainAcc: 0.612768\n",
      "Epoch: [3/10], ValAUC: 0.800245, ValAcc: 0.578295\n",
      "===================== This is the Kfold 3 =====================\n",
      "The number of train parameters 23071\n",
      "1/10, Step:1/73, TrainLoss:0.681677, ValAUC:0.446648 ValAcc:0.700665\n",
      "1/10, Step:11/73, TrainLoss:0.612293, ValAUC:0.537881 ValAcc:0.700665\n",
      "1/10, Step:21/73, TrainLoss:0.549059, ValAUC:0.546718 ValAcc:0.700665\n",
      "1/10, Step:31/73, TrainLoss:0.535268, ValAUC:0.602414 ValAcc:0.700665\n",
      "1/10, Step:41/73, TrainLoss:0.538301, ValAUC:0.624308 ValAcc:0.700665\n",
      "1/10, Step:51/73, TrainLoss:0.552767, ValAUC:0.661814 ValAcc:0.700665\n",
      "1/10, Step:61/73, TrainLoss:0.451473, ValAUC:0.695265 ValAcc:0.700665\n",
      "1/10, Step:71/73, TrainLoss:0.488618, ValAUC:0.738490 ValAcc:0.700665\n",
      "Epoch: [1/10], trainAUC: 0.762520, trainAcc: 0.732558\n",
      "Epoch: [1/10], ValAUC: 0.738490, ValAcc: 0.700665\n",
      "2/10, Step:1/73, TrainLoss:0.650401, ValAUC:0.737529 ValAcc:0.700665\n",
      "2/10, Step:11/73, TrainLoss:0.568480, ValAUC:0.743694 ValAcc:0.700665\n",
      "2/10, Step:21/73, TrainLoss:0.443316, ValAUC:0.767370 ValAcc:0.691767\n",
      "2/10, Step:31/73, TrainLoss:0.382802, ValAUC:0.780427 ValAcc:0.692656\n",
      "2/10, Step:41/73, TrainLoss:0.405454, ValAUC:0.789311 ValAcc:0.670410\n",
      "2/10, Step:51/73, TrainLoss:0.574435, ValAUC:0.793741 ValAcc:0.673079\n",
      "2/10, Step:61/73, TrainLoss:0.574154, ValAUC:0.804313 ValAcc:0.685537\n",
      "2/10, Step:71/73, TrainLoss:0.499139, ValAUC:0.818143 ValAcc:0.657952\n",
      "Epoch: [2/10], trainAUC: 0.888708, trainAcc: 0.682080\n",
      "Epoch: [2/10], ValAUC: 0.818143, ValAcc: 0.657952\n",
      "3/10, Step:1/73, TrainLoss:0.324134, ValAUC:0.815495 ValAcc:0.668630\n",
      "3/10, Step:11/73, TrainLoss:0.479606, ValAUC:0.821683 ValAcc:0.633925\n",
      "3/10, Step:21/73, TrainLoss:0.359042, ValAUC:0.829419 ValAcc:0.626806\n",
      "3/10, Step:31/73, TrainLoss:0.299132, ValAUC:0.828645 ValAcc:0.621467\n",
      "3/10, Step:41/73, TrainLoss:0.281890, ValAUC:0.844210 ValAcc:0.610789\n",
      "3/10, Step:51/73, TrainLoss:0.319857, ValAUC:0.841139 ValAcc:0.623247\n",
      "3/10, Step:61/73, TrainLoss:0.218971, ValAUC:0.824426 ValAcc:0.627696\n",
      "3/10, Step:71/73, TrainLoss:0.229874, ValAUC:0.839944 ValAcc:0.614348\n",
      "early stop!\n",
      "Epoch: [3/10], trainAUC: 0.960972, trainAcc: 0.631345\n",
      "Epoch: [3/10], ValAUC: 0.839944, ValAcc: 0.614348\n",
      "===================== This is the Kfold 4 =====================\n",
      "The number of train parameters 23071\n",
      "1/10, Step:1/73, TrainLoss:0.711548, ValAUC:0.475006 ValAcc:0.711752\n",
      "1/10, Step:11/73, TrainLoss:0.612083, ValAUC:0.527534 ValAcc:0.711752\n",
      "1/10, Step:21/73, TrainLoss:0.632995, ValAUC:0.527151 ValAcc:0.711752\n",
      "1/10, Step:31/73, TrainLoss:0.634851, ValAUC:0.543326 ValAcc:0.711752\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "val_auc_kf = []\n",
    "for train_index, val_index in kf.split(train):\n",
    "    print('===================== This is the Kfold {} ====================='.format(k))\n",
    "    k += 1\n",
    "    val_data = train[steps_token+tags].iloc[val_index]\n",
    "    train_data = train.iloc[train_index]\n",
    "    \n",
    "    if params['add_data_aug']:\n",
    "        ##### add augmentation to training set by index #####\n",
    "        train_org = train_data[steps_token+tags]\n",
    "        train_aug = train_data[steps_aug_token+tags]\n",
    "        train_aug.rename(index=str, columns=aug2ori_colname, inplace=True)\n",
    "        # concatenate dfs\n",
    "        train_data = pd.concat([train_org, train_aug], axis=0, ignore_index=False)\n",
    "        ##### add augmentation to training set by index #####\n",
    "    else:\n",
    "        train_data = train_data[steps_token+tags]\n",
    "    \n",
    "    #print(len(train_data), len(train_data.dropna()))\n",
    "    #look up\n",
    "    train_targets = list(train_data[predicted_tags])\n",
    "    val_targets = list(val_data[predicted_tags])\n",
    "    test_targets = list(test_data[predicted_tags])\n",
    "    \n",
    "    train_X = train_data[steps_token]\n",
    "    val_X = val_data[steps_token]\n",
    "    test_X = test_data[steps_token]\n",
    "    all_train_tokens = all_tokens_list(train_X)\n",
    "    max_vocab_size = len(list(set(all_train_tokens)))\n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    emb_weight = build_emb_weight(words_emb_dict, id2token)\n",
    "    train_data_indices = token2index_dataset(train_X, token2id)\n",
    "    val_data_indices = token2index_dataset(val_X, token2id)\n",
    "    test_data_indices = token2index_dataset(test_X, token2id)\n",
    "\n",
    "    # batchify datasets: \n",
    "    batch_size = params['batch_size']\n",
    "    max_sent_len = np.array([94, 86, 87, 90, 98, 91])\n",
    "    train_loader, val_loader, test_loader = create_dataset_obj(train_data_indices, val_data_indices,\n",
    "                                                           test_data_indices, train_targets,\n",
    "                                                           val_targets, test_targets,\n",
    "                                                           batch_size, max_sent_len, \n",
    "                                                           collate_func)\n",
    "    \n",
    "    val_auc, val_acc = model_train(params, emb_weight, train_loader, val_loader, test_loader)\n",
    "    val_auc_kf.append(val_auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(params, emb_weight, train_loader, val_loader, test_loader):\n",
    "    rnn1_type = params['rnn1_type'] \n",
    "    rnn_1 = rnn_types[rnn1_type]\n",
    "    rnn2_type = params['rnn2_type']\n",
    "    rnn_2 = rnn_types[rnn2_type]\n",
    "    bi = params['bi']\n",
    "    hidden_dim1 = params['hidden_dim1']\n",
    "    hidden_dim2 = params['hidden_dim2']\n",
    "    num_classes = params['num_classes']\n",
    "    batch_size = params['batch_size']\n",
    "    cuda_on = params['cuda_on']\n",
    "\n",
    "    weights_matrix = torch.from_numpy(emb_weight)\n",
    "    model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n",
    "                          cuda_on, weights_matrix, num_classes)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print('The number of train parameters', sum([np.prod(p.size()) for p in model_parameters]))\n",
    "    model = model.to(device)\n",
    "\n",
    "    #parameter for training\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs'] # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss() #torch.nn.BCELoss(); torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUC_list = []\n",
    "    val_AUC_list = []\n",
    "    train_ACC_list = []\n",
    "    val_ACC_list = []\n",
    "    max_val_auc = 0\n",
    "    step_max_descent = params['step_max_descent']\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (steps_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "            for step_id in range(6):\n",
    "                lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "                steps_batch[step_id] = steps_batch[step_id].to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(steps_batch, lengths_batch)\n",
    "            loss = criterion(outputs, labels_batch.view(-1,1).float().to(device)) \n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 10 step\n",
    "            if i % 10 == 0:\n",
    "                val_auc, val_acc = test_model(val_loader, model)\n",
    "                print('{}/{}, Step:{}/{}, TrainLoss:{:.6f}, ValAUC:{:.6f} ValAcc:{:.6f}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(train_loader), loss, val_auc, val_acc))\n",
    "                train_auc, train_acc = test_model(train_loader, model)\n",
    "                train_AUC_list.append(train_auc)\n",
    "                val_AUC_list.append(val_auc)\n",
    "                train_ACC_list.append(train_acc)\n",
    "                val_ACC_list.append(val_acc)\n",
    "                \n",
    "                # early stop\n",
    "                if max_val_auc < val_auc:\n",
    "                    max_val_auc = val_auc\n",
    "                    step_num_descent = 0\n",
    "                else:\n",
    "                    step_num_descent += 1\n",
    "                if step_max_descent == step_num_descent:\n",
    "                    print('early stop!')\n",
    "                    break\n",
    "        print('Epoch: [{}/{}], trainAUC: {:.6f}, trainAcc: {:.6f}'.format(epoch+1, num_epochs, train_auc, train_acc))\n",
    "        print('Epoch: [{}/{}], ValAUC: {:.6f}, ValAcc: {:.6f}'.format(epoch+1, num_epochs, val_auc, val_acc))\n",
    "        if step_max_descent == step_num_descent:\n",
    "            break\n",
    "    #return train_loss_list, train_AUC_list, val_AUC_list, train_ACC_list, val_ACC_list\n",
    "    val_auc_mean = np.mean(val_AUC_list[-step_max_descent*2+1:])\n",
    "    val_acc_mean = np.mean(val_ACC_list[-step_max_descent*2+1:])\n",
    "    return val_auc_mean, val_acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All tokens from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form all tokens list\n",
    "all_train_tokens = all_tokens_list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide which tag to predict for trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.iloc[:,1:].sum()/data_cuisine_tags.iloc[:,1:].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tag: tag_cuisine_american, which 27.3525% are 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary and indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3157"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter(all_train_tokens)\n",
    "# token_counter.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weight = build_emb_weight(words_emb_dict, id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050015827793605569"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.sum(emb_weight,1)==0)/emb_weight.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct data strcuture for datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_cuisine_indian            0.023525  85% auc\n",
    "tag_cuisine_nordic            0.000399\n",
    "tag_cuisine_european          0.012360\n",
    "tag_cuisine_asian             0.182217  98% auc\n",
    "tag_cuisine_mexican           0.013557\n",
    "tag_cuisine_latin-american    0.094896  90% auc\n",
    "tag_cuisine_french            0.077352  72% auc\n",
    "tag_cuisine_italian           0.233254  80% auc\n",
    "tag_cuisine_african           0.003987\n",
    "tag_cuisine_mediterranean     0.076555  88% auc\n",
    "tag_cuisine_american          0.273525  80% auc\n",
    "tag_cuisine_middle-eastern    0.046252  87% auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "logits_all = []\n",
    "labels_all = []\n",
    "model.eval()\n",
    "for steps_batch, lengths_batch, labels_batch in test_loader:\n",
    "    for step_id in range(6):\n",
    "        lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "        steps_batch[step_id] = steps_batch[step_id].to(devi) \n",
    "    logits = model(steps_batch, lengths_batch)\n",
    "    logits_all.extend(list(logits.cpu().detach().numpy()))\n",
    "    labels_all.extend(list(labels_batch.numpy()))\n",
    "logits_all = np.array(logits_all)\n",
    "labels_all = np.array(labels_all)\n",
    "auc = roc_auc_score(labels_all, logits_all)\n",
    "predicts = (logits_all > 0.5).astype(int)\n",
    "acc = np.mean(predicts==labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82168113146898103"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b342ac7c588>]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b342a7a5160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_all, logits_all, pos_label=1)\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 50])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([90, 50])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([90, 60])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
