{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tx443/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import string\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the pretrained embedding to text file\n",
    "model = KeyedVectors.load_word2vec_format('/home/hb1500/Plated/vocab.bin', binary=True)\n",
    "model.save_word2vec_format('pretrained_embd.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "# There are three types of embeddings: \n",
    "# pretrained_embd (from Recipe101); pretrained_embd (Recipe101 + Plated); Glove.6B.50d\n",
    "def load_emb_vectors(fname):\n",
    "    data = {}\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            data[word] = embedding\n",
    "    return data\n",
    "#fname = 'pretrained_embd.txt'\n",
    "#fname = '/Users/hetianbai/Desktop/DS-GA 1011/Labs/lab5/glove.6B/glove.6B.50d.txt'\n",
    "fname = '../../data/glove.6B.50d.txt'\n",
    "words_emb_dict = load_emb_vectors(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['step_one','step_two', 'step_three', 'step_four', 'step_five', 'step_six']\n",
    "steps_aug = ['step_one_sp', 'step_two_sp', 'step_three_sp',\n",
    "             'step_four_sp', 'step_five_sp', 'step_six_sp']\n",
    "tags = ['tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
    "        'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
    "        'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
    "        'tag_cuisine_italian', 'tag_cuisine_african',\n",
    "        'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
    "        'tag_cuisine_middle-eastern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the data is the output of \"Consolidated Data Cleaning\"\n",
    "# data_all = pd.read_csv('../../data/cleaned_recipe_data.csv', index_col=0)\n",
    "# # augmentated data saved to: https://drive.google.com/open?id=10Y3wExYdavqalI17d7KBI2RdKjRu4UP3\n",
    "# data_all_aug = pd.read_csv('../../data/augmented_instruction.csv', index_col=0)\n",
    "# data_all_aug.set_index('external_id', inplace=True)\n",
    "# #assert (data_all.index.values.tolist() == data_all_aug.index.values.tolist())\n",
    "\n",
    "# data_all_aug = data_all_aug[steps_aug]\n",
    "# data_with_aug = pd.merge(data_all, data_all_aug, how='outer', left_index=True, right_index=True)\n",
    "# data_with_aug.to_csv('../data/recipe_data_with_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug = pd.read_csv('../data/recipe_data_with_aug.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags = data_with_aug[steps+steps_aug+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['step_one', 'step_two', 'step_three', 'step_four', 'step_five',\n",
      "       'step_six', 'step_one_sp', 'step_two_sp', 'step_three_sp',\n",
      "       'step_four_sp', 'step_five_sp', 'step_six_sp', 'tag_cuisine_indian',\n",
      "       'tag_cuisine_nordic', 'tag_cuisine_european', 'tag_cuisine_asian',\n",
      "       'tag_cuisine_mexican', 'tag_cuisine_latin-american',\n",
      "       'tag_cuisine_french', 'tag_cuisine_italian', 'tag_cuisine_african',\n",
      "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
      "       'tag_cuisine_middle-eastern'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_with_aug_tags.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and remove punctuation\n",
    "def tokenizer(sent):\n",
    "    #print(sent)\n",
    "    if pd.isnull(sent):\n",
    "        words = []\n",
    "    else:\n",
    "        table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        sent = sent.translate(table)\n",
    "        tokens = word_tokenize(sent)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        #table = str.maketrans('', '', string.punctuation)\n",
    "        #stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "        #re.findall(r'\\d+', 'sdfa')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(step_n):\n",
    "    \"\"\"returns tokenization for each step, training set tokenizatoin\"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in step_n:\n",
    "        tokens = tokenizer(sample)\n",
    "        token_dataset.append(tokens)\n",
    "    return token_dataset\n",
    "\n",
    "def all_tokens_list(train_data):\n",
    "    \"\"\"returns all tokens of instruction (all steps) for creating vocabulary\"\"\"\n",
    "    all_tokens = []\n",
    "    for columns in train_data.columns:\n",
    "        for sample in train_data[columns]:\n",
    "            all_tokens += sample[:] \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing original instruction data\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_two has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_three has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_four has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_five has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_six has been tokenized.\n",
      "Processing augmented instruction data\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tx443/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_one_sp has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_two_sp has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_three_sp has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_four_sp has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_five_sp has been tokenized.\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "step_six_sp has been tokenized.\n"
     ]
    }
   ],
   "source": [
    "print('Processing original instruction data')\n",
    "# tokenize each steps on original datasets\n",
    "steps_token = []\n",
    "for step in steps:\n",
    "    steps_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')\n",
    "\n",
    "# tokenize each steps on augmented datasets\n",
    "print('Processing augmented instruction data')\n",
    "steps_aug_token = []\n",
    "for step in steps_aug:\n",
    "    steps_aug_token.append(step+'_token')\n",
    "    data_with_aug_tags[step+'_token'] = tokenize_dataset(data_with_aug_tags[step])\n",
    "    print(step, 'has been tokenized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_aug_tags = data_with_aug_tags[steps_token+steps_aug_token+tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['step_one_token', 'step_two_token', 'step_three_token',\n",
       "       'step_four_token', 'step_five_token', 'step_six_token',\n",
       "       'step_one_sp_token', 'step_two_sp_token', 'step_three_sp_token',\n",
       "       'step_four_sp_token', 'step_five_sp_token', 'step_six_sp_token',\n",
       "       'tag_cuisine_indian', 'tag_cuisine_nordic', 'tag_cuisine_european',\n",
       "       'tag_cuisine_asian', 'tag_cuisine_mexican',\n",
       "       'tag_cuisine_latin-american', 'tag_cuisine_french',\n",
       "       'tag_cuisine_italian', 'tag_cuisine_african',\n",
       "       'tag_cuisine_mediterranean', 'tag_cuisine_american',\n",
       "       'tag_cuisine_middle-eastern'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_aug_tags.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_data = train_test_split(data_with_aug_tags, test_size=0.1, random_state=RANDOM_STATE)\n",
    "test_data = test_data[steps_token+tags]\n",
    "#train_data, val_data, train_tags, val_tags = train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug2ori_colname = dict(zip(steps_aug_token+tags, steps_token+tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_five_sp_token': 'step_five_token',\n",
       " 'step_four_sp_token': 'step_four_token',\n",
       " 'step_one_sp_token': 'step_one_token',\n",
       " 'step_six_sp_token': 'step_six_token',\n",
       " 'step_three_sp_token': 'step_three_token',\n",
       " 'step_two_sp_token': 'step_two_token',\n",
       " 'tag_cuisine_african': 'tag_cuisine_african',\n",
       " 'tag_cuisine_american': 'tag_cuisine_american',\n",
       " 'tag_cuisine_asian': 'tag_cuisine_asian',\n",
       " 'tag_cuisine_european': 'tag_cuisine_european',\n",
       " 'tag_cuisine_french': 'tag_cuisine_french',\n",
       " 'tag_cuisine_indian': 'tag_cuisine_indian',\n",
       " 'tag_cuisine_italian': 'tag_cuisine_italian',\n",
       " 'tag_cuisine_latin-american': 'tag_cuisine_latin-american',\n",
       " 'tag_cuisine_mediterranean': 'tag_cuisine_mediterranean',\n",
       " 'tag_cuisine_mexican': 'tag_cuisine_mexican',\n",
       " 'tag_cuisine_middle-eastern': 'tag_cuisine_middle-eastern',\n",
       " 'tag_cuisine_nordic': 'tag_cuisine_nordic'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug2ori_colname "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for train and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_types = {\n",
    "    'rnn': nn.RNN,\n",
    "    'lstm': nn.LSTM,\n",
    "    'gru': nn.GRU\n",
    "}\n",
    "\n",
    "params = dict(\n",
    "    rnn1_type = 'gru',\n",
    "    rnn2_type = 'gru',\n",
    "    bi = True,\n",
    "    hidden_dim1 = 30,\n",
    "    hidden_dim2 = 30,\n",
    "    num_classes = 1,\n",
    "    \n",
    "    num_epochs = 5,\n",
    "    batch_size = 50,\n",
    "    learning_rate = 0.01,\n",
    "    add_data_aug = True,\n",
    "    cuda_on = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== This is the Kfold 1 =====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/pandas-0.22.0-py3.6-linux-x86_64.egg/pandas/core/frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train parameters 23071\n",
      "1/5, Step:1/73, TrainLoss:0.673671, ValAUC:0.516710 ValAcc:0.754425\n",
      "1/5, Step:11/73, TrainLoss:0.512991, ValAUC:0.536419 ValAcc:0.754425\n",
      "1/5, Step:21/73, TrainLoss:0.590027, ValAUC:0.531796 ValAcc:0.754425\n",
      "1/5, Step:31/73, TrainLoss:0.628712, ValAUC:0.569258 ValAcc:0.754425\n",
      "1/5, Step:41/73, TrainLoss:0.592697, ValAUC:0.609839 ValAcc:0.754425\n",
      "1/5, Step:51/73, TrainLoss:0.601926, ValAUC:0.651291 ValAcc:0.754425\n",
      "1/5, Step:61/73, TrainLoss:0.494686, ValAUC:0.675121 ValAcc:0.754425\n",
      "1/5, Step:71/73, TrainLoss:0.570093, ValAUC:0.723865 ValAcc:0.751047\n",
      "Epoch: [1/5], trainAUC: 0.773962, trainAcc: 0.678326\n",
      "Epoch: [1/5], ValAUC: 0.730364, ValAcc: 0.720652\n",
      "2/5, Step:1/73, TrainLoss:0.532694, ValAUC:0.738950 ValAcc:0.719526\n",
      "2/5, Step:11/73, TrainLoss:0.564985, ValAUC:0.784127 ValAcc:0.681250\n",
      "2/5, Step:21/73, TrainLoss:0.413808, ValAUC:0.803413 ValAcc:0.734161\n",
      "2/5, Step:31/73, TrainLoss:0.509308, ValAUC:0.808909 ValAcc:0.691381\n",
      "2/5, Step:41/73, TrainLoss:0.425131, ValAUC:0.833505 ValAcc:0.718400\n",
      "2/5, Step:51/73, TrainLoss:0.438645, ValAUC:0.842276 ValAcc:0.686878\n",
      "2/5, Step:61/73, TrainLoss:0.350439, ValAUC:0.837574 ValAcc:0.725155\n",
      "2/5, Step:71/73, TrainLoss:0.396487, ValAUC:0.844073 ValAcc:0.672243\n",
      "Epoch: [2/5], trainAUC: 0.903822, trainAcc: 0.656839\n",
      "Epoch: [2/5], ValAUC: 0.845526, ValAcc: 0.702639\n",
      "3/5, Step:1/73, TrainLoss:0.380117, ValAUC:0.845896 ValAcc:0.712771\n",
      "3/5, Step:11/73, TrainLoss:0.304328, ValAUC:0.846900 ValAcc:0.693633\n",
      "3/5, Step:21/73, TrainLoss:0.498030, ValAUC:0.859264 ValAcc:0.665489\n",
      "3/5, Step:31/73, TrainLoss:0.494550, ValAUC:0.852342 ValAcc:0.618206\n",
      "3/5, Step:41/73, TrainLoss:0.309071, ValAUC:0.833716 ValAcc:0.659860\n",
      "3/5, Step:51/73, TrainLoss:0.522486, ValAUC:0.823941 ValAcc:0.712771\n",
      "3/5, Step:61/73, TrainLoss:0.202187, ValAUC:0.831048 ValAcc:0.709394\n",
      "3/5, Step:71/73, TrainLoss:0.253240, ValAUC:0.834720 ValAcc:0.690256\n",
      "Epoch: [3/5], trainAUC: 0.959745, trainAcc: 0.657325\n",
      "Epoch: [3/5], ValAUC: 0.830969, ValAcc: 0.706017\n",
      "4/5, Step:1/73, TrainLoss:0.170967, ValAUC:0.832633 ValAcc:0.703765\n",
      "4/5, Step:11/73, TrainLoss:0.382461, ValAUC:0.841431 ValAcc:0.706017\n",
      "4/5, Step:21/73, TrainLoss:0.243263, ValAUC:0.839555 ValAcc:0.698136\n",
      "4/5, Step:31/73, TrainLoss:0.219703, ValAUC:0.837309 ValAcc:0.658734\n",
      "4/5, Step:41/73, TrainLoss:0.127511, ValAUC:0.819635 ValAcc:0.673369\n",
      "4/5, Step:51/73, TrainLoss:0.311707, ValAUC:0.823598 ValAcc:0.648602\n",
      "4/5, Step:61/73, TrainLoss:0.159054, ValAUC:0.837521 ValAcc:0.659860\n",
      "4/5, Step:71/73, TrainLoss:0.151628, ValAUC:0.837759 ValAcc:0.677872\n",
      "Epoch: [4/5], trainAUC: 0.984463, trainAcc: 0.602941\n",
      "Epoch: [4/5], ValAUC: 0.837125, ValAcc: 0.648602\n",
      "5/5, Step:1/73, TrainLoss:0.063969, ValAUC:0.836992 ValAcc:0.635093\n",
      "5/5, Step:11/73, TrainLoss:0.090355, ValAUC:0.826293 ValAcc:0.646351\n",
      "5/5, Step:21/73, TrainLoss:0.073184, ValAUC:0.823545 ValAcc:0.656482\n",
      "5/5, Step:31/73, TrainLoss:0.106519, ValAUC:0.828512 ValAcc:0.656482\n",
      "5/5, Step:41/73, TrainLoss:0.147858, ValAUC:0.816015 ValAcc:0.650854\n",
      "5/5, Step:51/73, TrainLoss:0.150181, ValAUC:0.823281 ValAcc:0.638470\n",
      "5/5, Step:61/73, TrainLoss:0.104722, ValAUC:0.818367 ValAcc:0.629464\n",
      "5/5, Step:71/73, TrainLoss:0.074915, ValAUC:0.828459 ValAcc:0.663237\n",
      "Epoch: [5/5], trainAUC: 0.994146, trainAcc: 0.603426\n",
      "Epoch: [5/5], ValAUC: 0.828987, ValAcc: 0.656482\n",
      "===================== This is the Kfold 2 =====================\n",
      "The number of train parameters 23071\n",
      "1/5, Step:1/73, TrainLoss:0.727976, ValAUC:0.531760 ValAcc:0.705752\n",
      "1/5, Step:11/73, TrainLoss:0.581880, ValAUC:0.578900 ValAcc:0.705752\n",
      "1/5, Step:21/73, TrainLoss:0.546844, ValAUC:0.577321 ValAcc:0.705752\n",
      "1/5, Step:31/73, TrainLoss:0.553187, ValAUC:0.606642 ValAcc:0.705752\n",
      "1/5, Step:41/73, TrainLoss:0.538934, ValAUC:0.633276 ValAcc:0.705752\n",
      "1/5, Step:51/73, TrainLoss:0.611658, ValAUC:0.677305 ValAcc:0.705752\n",
      "1/5, Step:61/73, TrainLoss:0.678307, ValAUC:0.711575 ValAcc:0.703931\n",
      "1/5, Step:71/73, TrainLoss:0.660666, ValAUC:0.743017 ValAcc:0.704842\n",
      "Epoch: [1/5], trainAUC: 0.808532, trainAcc: 0.725151\n",
      "Epoch: [1/5], ValAUC: 0.738940, ValAcc: 0.700290\n",
      "2/5, Step:1/73, TrainLoss:0.518030, ValAUC:0.738869 ValAcc:0.702111\n",
      "2/5, Step:11/73, TrainLoss:0.375875, ValAUC:0.759139 ValAcc:0.682992\n",
      "2/5, Step:21/73, TrainLoss:0.487226, ValAUC:0.781460 ValAcc:0.668425\n",
      "2/5, Step:31/73, TrainLoss:0.440698, ValAUC:0.786598 ValAcc:0.624726\n",
      "2/5, Step:41/73, TrainLoss:0.550579, ValAUC:0.788059 ValAcc:0.575564\n",
      "2/5, Step:51/73, TrainLoss:0.322551, ValAUC:0.793952 ValAcc:0.635651\n",
      "2/5, Step:61/73, TrainLoss:0.429520, ValAUC:0.795295 ValAcc:0.659321\n",
      "2/5, Step:71/73, TrainLoss:0.523404, ValAUC:0.798525 ValAcc:0.661142\n",
      "Epoch: [2/5], trainAUC: 0.903296, trainAcc: 0.617894\n",
      "Epoch: [2/5], ValAUC: 0.806185, ValAcc: 0.591951\n",
      "3/5, Step:1/73, TrainLoss:0.371923, ValAUC:0.806043 ValAcc:0.584668\n",
      "3/5, Step:11/73, TrainLoss:0.388546, ValAUC:0.791713 ValAcc:0.573743\n",
      "3/5, Step:21/73, TrainLoss:0.208581, ValAUC:0.803733 ValAcc:0.674798\n",
      "3/5, Step:31/73, TrainLoss:0.338175, ValAUC:0.809767 ValAcc:0.622905\n",
      "3/5, Step:41/73, TrainLoss:0.261678, ValAUC:0.807952 ValAcc:0.614711\n",
      "3/5, Step:51/73, TrainLoss:0.197303, ValAUC:0.791006 ValAcc:0.621084\n",
      "3/5, Step:61/73, TrainLoss:0.280029, ValAUC:0.797275 ValAcc:0.618353\n",
      "3/5, Step:71/73, TrainLoss:0.335737, ValAUC:0.812195 ValAcc:0.609249\n",
      "Epoch: [3/5], trainAUC: 0.962196, trainAcc: 0.640191\n",
      "Epoch: [3/5], ValAUC: 0.805478, ValAcc: 0.638382\n",
      "4/5, Step:1/73, TrainLoss:0.218101, ValAUC:0.801683 ValAcc:0.651128\n",
      "4/5, Step:11/73, TrainLoss:0.309406, ValAUC:0.802036 ValAcc:0.633830\n",
      "4/5, Step:21/73, TrainLoss:0.201540, ValAUC:0.811111 ValAcc:0.584668\n",
      "4/5, Step:31/73, TrainLoss:0.211624, ValAUC:0.800245 ValAcc:0.588310\n",
      "4/5, Step:41/73, TrainLoss:0.193939, ValAUC:0.792420 ValAcc:0.622905\n",
      "4/5, Step:51/73, TrainLoss:0.114636, ValAUC:0.808966 ValAcc:0.631099\n",
      "4/5, Step:61/73, TrainLoss:0.257809, ValAUC:0.823980 ValAcc:0.631099\n",
      "4/5, Step:71/73, TrainLoss:0.138873, ValAUC:0.815896 ValAcc:0.622905\n",
      "Epoch: [4/5], trainAUC: 0.983249, trainAcc: 0.646726\n",
      "Epoch: [4/5], ValAUC: 0.811017, ValAcc: 0.647486\n",
      "5/5, Step:1/73, TrainLoss:0.211830, ValAUC:0.811912 ValAcc:0.642934\n",
      "5/5, Step:11/73, TrainLoss:0.125450, ValAUC:0.805525 ValAcc:0.622905\n",
      "5/5, Step:21/73, TrainLoss:0.147182, ValAUC:0.801989 ValAcc:0.628368\n",
      "5/5, Step:31/73, TrainLoss:0.162700, ValAUC:0.796026 ValAcc:0.618353\n",
      "5/5, Step:41/73, TrainLoss:0.090438, ValAUC:0.786056 ValAcc:0.611980\n",
      "5/5, Step:51/73, TrainLoss:0.215060, ValAUC:0.792255 ValAcc:0.585578\n",
      "5/5, Step:61/73, TrainLoss:0.042829, ValAUC:0.807882 ValAcc:0.569191\n",
      "5/5, Step:71/73, TrainLoss:0.190914, ValAUC:0.825866 ValAcc:0.587399\n",
      "Epoch: [5/5], trainAUC: 0.990107, trainAcc: 0.603798\n",
      "Epoch: [5/5], ValAUC: 0.829872, ValAcc: 0.580116\n",
      "===================== This is the Kfold 3 =====================\n",
      "The number of train parameters 23071\n",
      "1/5, Step:1/73, TrainLoss:0.674342, ValAUC:0.530638 ValAcc:0.700665\n",
      "1/5, Step:11/73, TrainLoss:0.587310, ValAUC:0.609423 ValAcc:0.700665\n",
      "1/5, Step:21/73, TrainLoss:0.698945, ValAUC:0.628270 ValAcc:0.700665\n",
      "1/5, Step:31/73, TrainLoss:0.707249, ValAUC:0.654102 ValAcc:0.700665\n",
      "1/5, Step:41/73, TrainLoss:0.423141, ValAUC:0.687693 ValAcc:0.700665\n",
      "1/5, Step:51/73, TrainLoss:0.523146, ValAUC:0.736756 ValAcc:0.700665\n",
      "1/5, Step:61/73, TrainLoss:0.707669, ValAUC:0.692077 ValAcc:0.700665\n",
      "1/5, Step:71/73, TrainLoss:0.505600, ValAUC:0.715377 ValAcc:0.700665\n",
      "Epoch: [1/5], trainAUC: 0.735533, trainAcc: 0.732558\n",
      "Epoch: [1/5], ValAUC: 0.712518, ValAcc: 0.700665\n",
      "2/5, Step:1/73, TrainLoss:0.570560, ValAUC:0.712635 ValAcc:0.700665\n",
      "2/5, Step:11/73, TrainLoss:0.490712, ValAUC:0.753047 ValAcc:0.698885\n",
      "2/5, Step:21/73, TrainLoss:0.571919, ValAUC:0.793694 ValAcc:0.641934\n",
      "2/5, Step:31/73, TrainLoss:0.450659, ValAUC:0.815706 ValAcc:0.647273\n",
      "2/5, Step:41/73, TrainLoss:0.583994, ValAUC:0.815002 ValAcc:0.633925\n",
      "2/5, Step:51/73, TrainLoss:0.587347, ValAUC:0.831786 ValAcc:0.619687\n",
      "2/5, Step:61/73, TrainLoss:0.525757, ValAUC:0.838233 ValAcc:0.658841\n",
      "2/5, Step:71/73, TrainLoss:0.442561, ValAUC:0.844538 ValAcc:0.648163\n",
      "Epoch: [2/5], trainAUC: 0.889699, trainAcc: 0.671393\n",
      "Epoch: [2/5], ValAUC: 0.844749, ValAcc: 0.646383\n",
      "3/5, Step:1/73, TrainLoss:0.475157, ValAUC:0.846812 ValAcc:0.626806\n",
      "3/5, Step:11/73, TrainLoss:0.370872, ValAUC:0.836592 ValAcc:0.651722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5, Step:21/73, TrainLoss:0.325463, ValAUC:0.839920 ValAcc:0.623247\n",
      "3/5, Step:31/73, TrainLoss:0.550491, ValAUC:0.851758 ValAcc:0.596551\n",
      "3/5, Step:41/73, TrainLoss:0.403525, ValAUC:0.853704 ValAcc:0.614348\n",
      "3/5, Step:51/73, TrainLoss:0.192399, ValAUC:0.862447 ValAcc:0.622357\n",
      "3/5, Step:61/73, TrainLoss:0.259391, ValAUC:0.865213 ValAcc:0.591211\n",
      "3/5, Step:71/73, TrainLoss:0.457422, ValAUC:0.864627 ValAcc:0.609899\n",
      "Epoch: [3/5], trainAUC: 0.947369, trainAcc: 0.653880\n",
      "Epoch: [3/5], ValAUC: 0.861627, ValAcc: 0.639264\n",
      "4/5, Step:1/73, TrainLoss:0.163323, ValAUC:0.859095 ValAcc:0.645493\n",
      "4/5, Step:11/73, TrainLoss:0.207560, ValAUC:0.841139 ValAcc:0.634815\n",
      "4/5, Step:21/73, TrainLoss:0.343780, ValAUC:0.840436 ValAcc:0.635705\n",
      "4/5, Step:31/73, TrainLoss:0.366606, ValAUC:0.852532 ValAcc:0.608119\n",
      "4/5, Step:41/73, TrainLoss:0.310932, ValAUC:0.853422 ValAcc:0.584982\n",
      "4/5, Step:51/73, TrainLoss:0.174066, ValAUC:0.865635 ValAcc:0.572524\n",
      "4/5, Step:61/73, TrainLoss:0.318437, ValAUC:0.865752 ValAcc:0.593881\n",
      "4/5, Step:71/73, TrainLoss:0.140181, ValAUC:0.847562 ValAcc:0.596551\n",
      "Epoch: [4/5], trainAUC: 0.980663, trainAcc: 0.619112\n",
      "Epoch: [4/5], ValAUC: 0.847867, ValAcc: 0.595661\n",
      "5/5, Step:1/73, TrainLoss:0.184304, ValAUC:0.848429 ValAcc:0.604559\n",
      "5/5, Step:11/73, TrainLoss:0.158388, ValAUC:0.854219 ValAcc:0.595661\n",
      "5/5, Step:21/73, TrainLoss:0.224769, ValAUC:0.842686 ValAcc:0.608119\n",
      "5/5, Step:31/73, TrainLoss:0.175302, ValAUC:0.838233 ValAcc:0.599220\n",
      "5/5, Step:41/73, TrainLoss:0.254501, ValAUC:0.843601 ValAcc:0.593881\n",
      "5/5, Step:51/73, TrainLoss:0.092301, ValAUC:0.845101 ValAcc:0.581423\n",
      "5/5, Step:61/73, TrainLoss:0.072150, ValAUC:0.829934 ValAcc:0.622357\n",
      "5/5, Step:71/73, TrainLoss:0.169175, ValAUC:0.840436 ValAcc:0.610789\n",
      "Epoch: [5/5], trainAUC: 0.994157, trainAcc: 0.608424\n",
      "Epoch: [5/5], ValAUC: 0.841092, ValAcc: 0.593881\n",
      "===================== This is the Kfold 4 =====================\n",
      "The number of train parameters 23071\n",
      "1/5, Step:1/73, TrainLoss:0.685465, ValAUC:0.493937 ValAcc:0.711752\n",
      "1/5, Step:11/73, TrainLoss:0.537106, ValAUC:0.527654 ValAcc:0.711752\n",
      "1/5, Step:21/73, TrainLoss:0.583628, ValAUC:0.565277 ValAcc:0.711752\n",
      "1/5, Step:31/73, TrainLoss:0.596813, ValAUC:0.608196 ValAcc:0.711752\n",
      "1/5, Step:41/73, TrainLoss:0.453324, ValAUC:0.652408 ValAcc:0.711752\n",
      "1/5, Step:51/73, TrainLoss:0.583989, ValAUC:0.703523 ValAcc:0.705178\n",
      "1/5, Step:61/73, TrainLoss:0.476647, ValAUC:0.755404 ValAcc:0.646958\n",
      "1/5, Step:71/73, TrainLoss:0.556555, ValAUC:0.750707 ValAcc:0.650715\n",
      "Epoch: [1/5], trainAUC: 0.815940, trainAcc: 0.689964\n",
      "Epoch: [1/5], ValAUC: 0.750300, ValAcc: 0.676068\n",
      "2/5, Step:1/73, TrainLoss:0.519719, ValAUC:0.752121 ValAcc:0.677007\n",
      "2/5, Step:11/73, TrainLoss:0.411105, ValAUC:0.770836 ValAcc:0.627239\n",
      "2/5, Step:21/73, TrainLoss:0.390655, ValAUC:0.782003 ValAcc:0.679825\n",
      "2/5, Step:31/73, TrainLoss:0.359751, ValAUC:0.806830 ValAcc:0.668556\n",
      "2/5, Step:41/73, TrainLoss:0.414173, ValAUC:0.810568 ValAcc:0.634751\n",
      "2/5, Step:51/73, TrainLoss:0.361339, ValAUC:0.809538 ValAcc:0.681703\n",
      "2/5, Step:61/73, TrainLoss:0.478650, ValAUC:0.796765 ValAcc:0.702361\n",
      "2/5, Step:71/73, TrainLoss:0.372972, ValAUC:0.823053 ValAcc:0.640385\n",
      "Epoch: [2/5], trainAUC: 0.909881, trainAcc: 0.691873\n",
      "Epoch: [2/5], ValAUC: 0.817541, ValAcc: 0.683581\n",
      "3/5, Step:1/73, TrainLoss:0.484635, ValAUC:0.814162 ValAcc:0.691093\n",
      "3/5, Step:11/73, TrainLoss:0.246969, ValAUC:0.818380 ValAcc:0.653532\n",
      "3/5, Step:21/73, TrainLoss:0.300192, ValAUC:0.818404 ValAcc:0.629117\n",
      "3/5, Step:31/73, TrainLoss:0.168200, ValAUC:0.821399 ValAcc:0.622544\n",
      "3/5, Step:41/73, TrainLoss:0.364814, ValAUC:0.824395 ValAcc:0.604702\n",
      "3/5, Step:51/73, TrainLoss:0.291120, ValAUC:0.830961 ValAcc:0.649776\n",
      "3/5, Step:61/73, TrainLoss:0.271760, ValAUC:0.811958 ValAcc:0.653532\n",
      "3/5, Step:71/73, TrainLoss:0.337558, ValAUC:0.829188 ValAcc:0.623483\n",
      "Epoch: [3/5], trainAUC: 0.955493, trainAcc: 0.633853\n",
      "Epoch: [3/5], ValAUC: 0.831225, ValAcc: 0.630995\n",
      "4/5, Step:1/73, TrainLoss:0.270380, ValAUC:0.830745 ValAcc:0.646019\n",
      "4/5, Step:11/73, TrainLoss:0.322301, ValAUC:0.822286 ValAcc:0.642263\n",
      "4/5, Step:21/73, TrainLoss:0.178209, ValAUC:0.824515 ValAcc:0.624422\n",
      "4/5, Step:31/73, TrainLoss:0.171890, ValAUC:0.816822 ValAcc:0.608458\n",
      "4/5, Step:41/73, TrainLoss:0.297341, ValAUC:0.829164 ValAcc:0.593434\n",
      "4/5, Step:51/73, TrainLoss:0.312882, ValAUC:0.834244 ValAcc:0.594373\n",
      "4/5, Step:61/73, TrainLoss:0.250183, ValAUC:0.816439 ValAcc:0.635690\n",
      "4/5, Step:71/73, TrainLoss:0.168604, ValAUC:0.813444 ValAcc:0.646958\n",
      "Epoch: [4/5], trainAUC: 0.978657, trainAcc: 0.615785\n",
      "Epoch: [4/5], ValAUC: 0.818740, ValAcc: 0.615970\n",
      "5/5, Step:1/73, TrainLoss:0.152398, ValAUC:0.814714 ValAcc:0.624422\n",
      "5/5, Step:11/73, TrainLoss:0.177505, ValAUC:0.817949 ValAcc:0.611275\n",
      "5/5, Step:21/73, TrainLoss:0.128267, ValAUC:0.824059 ValAcc:0.650715\n",
      "5/5, Step:31/73, TrainLoss:0.204063, ValAUC:0.830889 ValAcc:0.635690\n",
      "5/5, Step:41/73, TrainLoss:0.182820, ValAUC:0.846298 ValAcc:0.615031\n",
      "5/5, Step:51/73, TrainLoss:0.224980, ValAUC:0.825401 ValAcc:0.597190\n",
      "5/5, Step:61/73, TrainLoss:0.154121, ValAUC:0.809274 ValAcc:0.620666\n",
      "5/5, Step:71/73, TrainLoss:0.053608, ValAUC:0.814881 ValAcc:0.624422\n",
      "Epoch: [5/5], trainAUC: 0.988268, trainAcc: 0.620875\n",
      "Epoch: [5/5], ValAUC: 0.815169, ValAcc: 0.623483\n",
      "===================== This is the Kfold 5 =====================\n",
      "The number of train parameters 23071\n",
      "1/5, Step:1/73, TrainLoss:0.705644, ValAUC:0.521755 ValAcc:0.758315\n",
      "1/5, Step:11/73, TrainLoss:0.600963, ValAUC:0.636086 ValAcc:0.758315\n",
      "1/5, Step:21/73, TrainLoss:0.641473, ValAUC:0.656661 ValAcc:0.758315\n",
      "1/5, Step:31/73, TrainLoss:0.745221, ValAUC:0.678792 ValAcc:0.758315\n",
      "1/5, Step:41/73, TrainLoss:0.647533, ValAUC:0.682762 ValAcc:0.758315\n",
      "1/5, Step:51/73, TrainLoss:0.603384, ValAUC:0.688208 ValAcc:0.754878\n",
      "1/5, Step:61/73, TrainLoss:0.513111, ValAUC:0.726541 ValAcc:0.713640\n",
      "1/5, Step:71/73, TrainLoss:0.455717, ValAUC:0.755298 ValAcc:0.758315\n",
      "Epoch: [1/5], trainAUC: 0.789155, trainAcc: 0.717920\n",
      "Epoch: [1/5], ValAUC: 0.753072, ValAcc: 0.758315\n",
      "2/5, Step:1/73, TrainLoss:0.596518, ValAUC:0.750577 ValAcc:0.758315\n",
      "2/5, Step:11/73, TrainLoss:0.369709, ValAUC:0.736198 ValAcc:0.758315\n",
      "2/5, Step:21/73, TrainLoss:0.505077, ValAUC:0.772440 ValAcc:0.746860\n",
      "2/5, Step:31/73, TrainLoss:0.384515, ValAUC:0.787167 ValAcc:0.735404\n",
      "2/5, Step:41/73, TrainLoss:0.409249, ValAUC:0.815736 ValAcc:0.691875\n",
      "2/5, Step:51/73, TrainLoss:0.421486, ValAUC:0.823864 ValAcc:0.696457\n",
      "2/5, Step:61/73, TrainLoss:0.428462, ValAUC:0.825581 ValAcc:0.710203\n",
      "2/5, Step:71/73, TrainLoss:0.285608, ValAUC:0.827485 ValAcc:0.690729\n",
      "Epoch: [2/5], trainAUC: 0.900324, trainAcc: 0.684459\n",
      "Epoch: [2/5], ValAUC: 0.833360, ValAcc: 0.731968\n",
      "3/5, Step:1/73, TrainLoss:0.399805, ValAUC:0.833655 ValAcc:0.741132\n",
      "3/5, Step:11/73, TrainLoss:0.383838, ValAUC:0.821664 ValAcc:0.720513\n",
      "3/5, Step:21/73, TrainLoss:0.388647, ValAUC:0.844090 ValAcc:0.634599\n",
      "3/5, Step:31/73, TrainLoss:0.305959, ValAUC:0.842347 ValAcc:0.662091\n",
      "3/5, Step:41/73, TrainLoss:0.286589, ValAUC:0.829202 ValAcc:0.664382\n",
      "3/5, Step:51/73, TrainLoss:0.391488, ValAUC:0.824266 ValAcc:0.688438\n",
      "3/5, Step:61/73, TrainLoss:0.379214, ValAUC:0.832207 ValAcc:0.664382\n",
      "3/5, Step:71/73, TrainLoss:0.526142, ValAUC:0.842910 ValAcc:0.703330\n",
      "Epoch: [3/5], trainAUC: 0.948115, trainAcc: 0.647615\n",
      "Epoch: [3/5], ValAUC: 0.831858, ValAcc: 0.701039\n",
      "4/5, Step:1/73, TrainLoss:0.207975, ValAUC:0.827190 ValAcc:0.688438\n",
      "4/5, Step:11/73, TrainLoss:0.249980, ValAUC:0.839557 ValAcc:0.671255\n",
      "4/5, Step:21/73, TrainLoss:0.122260, ValAUC:0.842695 ValAcc:0.655218\n",
      "4/5, Step:31/73, TrainLoss:0.274890, ValAUC:0.836150 ValAcc:0.636890\n",
      "4/5, Step:41/73, TrainLoss:0.089110, ValAUC:0.845405 ValAcc:0.650636\n",
      "4/5, Step:51/73, TrainLoss:0.175303, ValAUC:0.841032 ValAcc:0.648345\n",
      "4/5, Step:61/73, TrainLoss:0.173544, ValAUC:0.830678 ValAcc:0.636890\n",
      "4/5, Step:71/73, TrainLoss:0.124678, ValAUC:0.839959 ValAcc:0.672401\n",
      "Epoch: [4/5], trainAUC: 0.969560, trainAcc: 0.622973\n",
      "Epoch: [4/5], ValAUC: 0.832690, ValAcc: 0.672401\n",
      "5/5, Step:1/73, TrainLoss:0.225583, ValAUC:0.831160 ValAcc:0.670110\n",
      "5/5, Step:11/73, TrainLoss:0.119697, ValAUC:0.801894 ValAcc:0.648345\n",
      "5/5, Step:21/73, TrainLoss:0.073518, ValAUC:0.836043 ValAcc:0.641472\n",
      "5/5, Step:31/73, TrainLoss:0.359392, ValAUC:0.832421 ValAcc:0.610543\n",
      "5/5, Step:41/73, TrainLoss:0.095514, ValAUC:0.815843 ValAcc:0.646054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5, Step:51/73, TrainLoss:0.144132, ValAUC:0.831804 ValAcc:0.634599\n",
      "5/5, Step:61/73, TrainLoss:0.094019, ValAUC:0.834004 ValAcc:0.659800\n",
      "5/5, Step:71/73, TrainLoss:0.055982, ValAUC:0.827003 ValAcc:0.656364\n",
      "Epoch: [5/5], trainAUC: 0.993137, trainAcc: 0.603041\n",
      "Epoch: [5/5], ValAUC: 0.825098, ValAcc: 0.657509\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "k = 1\n",
    "predicted_tags = 'tag_cuisine_american' \n",
    "for train_index, val_index in kf.split(train):\n",
    "    print('===================== This is the Kfold {} ====================='.format(k))\n",
    "    k += 1\n",
    "    val_data = train[steps_token+tags].iloc[val_index]\n",
    "    train_data = train.iloc[train_index]\n",
    "    #print(len(train_data), len(train_data.dropna()))\n",
    "    \n",
    "    if params['add_data_aug']:\n",
    "        ##### add augmentation to training set by index #####\n",
    "        train_org = train_data[steps_token+tags]\n",
    "        train_aug = train_data[steps_aug_token+tags]\n",
    "        train_aug.rename(index=str, columns=aug2ori_colname, inplace=True)\n",
    "        # concatenate dfs\n",
    "        train_data = pd.concat([train_org, train_aug], axis=0, ignore_index=False)\n",
    "        ##### add augmentation to training set by index #####\n",
    "    else:\n",
    "        train_data = train_data[steps_token+tags]\n",
    "    \n",
    "    #print(len(train_data), len(train_data.dropna()))\n",
    "    #look up\n",
    "    train_targets = list(train_data[predicted_tags])\n",
    "    val_targets = list(val_data[predicted_tags])\n",
    "    test_targets = list(test_data[predicted_tags])\n",
    "    \n",
    "    train_X = train_data[steps_token]\n",
    "    val_X = val_data[steps_token]\n",
    "    test_X = test_data[steps_token]\n",
    "    all_train_tokens = all_tokens_list(train_X)\n",
    "    max_vocab_size = len(list(set(all_train_tokens)))\n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    emb_weight = build_emb_weight(words_emb_dict, id2token)\n",
    "    train_data_indices = token2index_dataset(train_X, token2id)\n",
    "    val_data_indices = token2index_dataset(val_X, token2id)\n",
    "    test_data_indices = token2index_dataset(test_X, token2id)\n",
    "\n",
    "    # batchify datasets: \n",
    "    batch_size = params['batch_size']\n",
    "    max_sent_len = np.array([94, 86, 87, 90, 98, 91])\n",
    "    train_loader, val_loader, test_loader = create_dataset_obj(train_data_indices, val_data_indices,\n",
    "                                                           test_data_indices, train_targets,\n",
    "                                                           val_targets, test_targets,\n",
    "                                                           batch_size, max_sent_len, \n",
    "                                                           collate_func)\n",
    "    \n",
    "    model_train(params, emb_weight, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(params, emb_weight, train_loader, val_loader, test_loader):\n",
    "    rnn1_type = params['rnn1_type'] \n",
    "    rnn_1 = rnn_types[rnn1_type]\n",
    "    rnn2_type = params['rnn2_type']\n",
    "    rnn_2 = rnn_types[rnn2_type]\n",
    "    bi = params['bi']\n",
    "    hidden_dim1 = params['hidden_dim1']\n",
    "    hidden_dim2 = params['hidden_dim2']\n",
    "    num_classes = params['num_classes']\n",
    "    batch_size = params['batch_size']\n",
    "    cuda_on = params['cuda_on']\n",
    "\n",
    "    weights_matrix = torch.from_numpy(emb_weight)\n",
    "    model = two_stage_RNN(rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, \n",
    "                          cuda_on, weights_matrix, num_classes)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print('The number of train parameters', sum([np.prod(p.size()) for p in model_parameters]))\n",
    "    model = model.to(device)\n",
    "\n",
    "    #parameter for training\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs'] # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss() #torch.nn.BCELoss(); torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUC_list = []\n",
    "    val_AUC_list = []\n",
    "    train_ACC_list = []\n",
    "    val_ACC_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (steps_batch, lengths_batch, labels_batch) in enumerate(train_loader):\n",
    "            for step_id in range(6):\n",
    "                lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "                steps_batch[step_id] = steps_batch[step_id].to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(steps_batch, lengths_batch)\n",
    "            loss = criterion(outputs, labels_batch.view(-1,1).float().to(device)) \n",
    "            train_loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 10 iterations\n",
    "            if i % 10 == 0:\n",
    "                val_auc, val_acc = test_model(val_loader, model)\n",
    "                print('{}/{}, Step:{}/{}, TrainLoss:{:.6f}, ValAUC:{:.6f} ValAcc:{:.6f}'.format(\n",
    "                    epoch+1, num_epochs, i+1, len(train_loader), loss, val_auc, val_acc))\n",
    "        val_auc, val_acc = test_model(val_loader, model)\n",
    "        train_auc, train_acc = test_model(train_loader, model)\n",
    "        train_AUC_list.append(train_auc)\n",
    "        val_AUC_list.append(val_auc)\n",
    "        train_ACC_list.append(train_acc)\n",
    "        val_ACC_list.append(val_acc)\n",
    "        print('Epoch: [{}/{}], trainAUC: {:.6f}, trainAcc: {:.6f}'.format(epoch+1, num_epochs, train_auc, train_acc))\n",
    "        print('Epoch: [{}/{}], ValAUC: {:.6f}, ValAcc: {:.6f}'.format(epoch+1, num_epochs, val_auc, val_acc))\n",
    "    return train_loss_list, train_AUC_list, val_AUC_list, train_ACC_list, val_ACC_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All tokens from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form all tokens list\n",
    "all_train_tokens = all_tokens_list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide which tag to predict for trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_cuisine_indian            0.023525\n",
       "tag_cuisine_nordic            0.000399\n",
       "tag_cuisine_european          0.012360\n",
       "tag_cuisine_asian             0.182217\n",
       "tag_cuisine_mexican           0.013557\n",
       "tag_cuisine_latin-american    0.094896\n",
       "tag_cuisine_french            0.077352\n",
       "tag_cuisine_italian           0.233254\n",
       "tag_cuisine_african           0.003987\n",
       "tag_cuisine_mediterranean     0.076555\n",
       "tag_cuisine_american          0.273525\n",
       "tag_cuisine_middle-eastern    0.046252\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cuisine_tags.iloc[:,1:].sum()/data_cuisine_tags.iloc[:,1:].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose tag: tag_cuisine_american, which 27.3525% are 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary and indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3157"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter(all_train_tokens)\n",
    "# token_counter.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index 0 for unk and 1 for pad\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2614 ; token switch\n",
      "Token switch; token id 2614\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = len(list(set(all_train_tokens)))\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print(\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print(\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emb_weight(words_emb_dict, id2token):\n",
    "    vocab_size = len(id2token)\n",
    "    emb_dim = len(words_emb_dict['a'])\n",
    "    emb_weight = np.zeros([vocab_size, emb_dim])\n",
    "    for i in range(2,vocab_size):\n",
    "        emb = words_emb_dict.get(id2token[i], None)\n",
    "        if emb is not None:\n",
    "            emb_weight[i] = emb\n",
    "    return emb_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weight = build_emb_weight(words_emb_dict, id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050015827793605569"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.sum(emb_weight,1)==0)/emb_weight.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct data strcuture for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    \"\"\"returns [[[step1 indices],[step2 indices],...,[step6 indices]],[],[],...]\"\"\"\n",
    "    recipie_indices_data = []\n",
    "    UNK_IDX = 1\n",
    "    for recipie in tokens_data.iterrows():\n",
    "        step_indices_data = []\n",
    "        for step in recipie[1]:\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in step]\n",
    "            step_indices_data.append(index_list)\n",
    "        recipie_indices_data.append(step_indices_data)\n",
    "    return recipie_indices_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntructionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, tags_list, max_sent_len):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data_list: list of recipie tokens \n",
    "        @param target_list: list of single tag, i.e. 'tag_cuisine_american'\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.tags_list = tags_list\n",
    "        assert (len(self.data_list) == len(self.tags_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call recipie[i]\n",
    "        \"\"\"\n",
    "        recipie = self.data_list[key]\n",
    "        step1_idx = recipie[0][:max_sent_len[0]]\n",
    "        step2_idx = recipie[1][:max_sent_len[1]]\n",
    "        step3_idx = recipie[2][:max_sent_len[2]]\n",
    "        step4_idx = recipie[3][:max_sent_len[3]]       \n",
    "        step5_idx = recipie[4][:max_sent_len[4]]\n",
    "        step6_idx = recipie[5][:max_sent_len[5]]\n",
    "        label = self.tags_list[key]\n",
    "        return [[step1_idx, step2_idx, step3_idx, step4_idx, step5_idx, step6_idx], \n",
    "                [len(step1_idx),len(step2_idx), len(step3_idx),len(step4_idx), len(step5_idx),len(step6_idx)], \n",
    "                label]\n",
    "\n",
    "def collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    steps_dict = defaultdict(list)\n",
    "    label_list = []\n",
    "    length_dict = defaultdict(list)\n",
    "    max_sent_len = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[-1])\n",
    "        for i in range(6):\n",
    "            length_dict[i].append(datum[1][i])\n",
    "    # padding\n",
    "    for i in range(6):\n",
    "        max_sent_len.append(max(length_dict[i]))\n",
    "    \n",
    "    for datum in batch:\n",
    "        for i, step in enumerate(datum[0]):\n",
    "            padded_vec = np.pad(np.array(step), \n",
    "                                pad_width=((0, max_sent_len[i]-datum[1][i])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "            steps_dict[i].append(padded_vec)\n",
    "    \n",
    "    for key in length_dict.keys():\n",
    "        length_dict[key] = torch.LongTensor(length_dict[key])\n",
    "        steps_dict[key] = torch.from_numpy(np.array(steps_dict[key]).astype(np.int)) \n",
    "        \n",
    "    return [steps_dict, length_dict, torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "def create_dataset_obj(train,val,test,train_targets,val_targets,test_targets,\n",
    "                       BATCH_SIZE,max_sent_len,collate_func):\n",
    "    collate_func=partial(collate_func)\n",
    "    train_dataset = IntructionDataset(train, train_targets, max_sent_len)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset = IntructionDataset(val, val_targets, max_sent_len)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=False)\n",
    "\n",
    "    test_dataset = IntructionDataset(test, test_targets, max_sent_len)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=collate_func,\n",
    "                                               shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, trainable=False):\n",
    "    vocab_size, emb_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if trainable == False:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, vocab_size, emb_dim\n",
    "\n",
    "class two_stage_RNN(nn.Module):\n",
    "    def __init__(self, rnn_1, hidden_dim1, bi, rnn_2, hidden_dim2, batch_size, cuda_on, \n",
    "                 weights_matrix, num_classes):\n",
    "        \n",
    "        super(two_stage_RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.bi = bi\n",
    "\n",
    "        self.embedding, vocab_size, emb_dim = create_emb_layer(weights_matrix, trainable=False)\n",
    "        \n",
    "        # module for steps in the fisrt stage\n",
    "#         self.hidden_stage1, self.hidden_stage2 = self.init_hidden(batch_size, cuda_on)\n",
    "        rnn_common = rnn_1(emb_dim, hidden_dim1, num_layers=1, \n",
    "                           batch_first=True, bidirectional=bi)\n",
    "        self.rnn_each_step = nn.ModuleList([])\n",
    "        for i in range(6):\n",
    "            self.rnn_each_step.append(rnn_common)\n",
    "        \n",
    "        # module for the second stage\n",
    "        if self.bi:\n",
    "            self.steps_rnn = rnn_2(hidden_dim1*2, hidden_dim2, num_layers=1, batch_first=False)\n",
    "        else:\n",
    "            self.steps_rnn = rnn_2(hidden_dim1, hidden_dim2, num_layers=1, batch_first=False)\n",
    "        # module for interaction\n",
    "        self.linear = nn.Linear(hidden_dim2, num_classes)\n",
    "        \n",
    "    def forward(self, steps, lengths):\n",
    "        # first stage\n",
    "        output_each_step = []\n",
    "        for i in range(6):\n",
    "            rnn_input = steps[i]\n",
    "            emb = self.embedding(rnn_input) # embedding\n",
    "\n",
    "            output, _ = self.rnn_each_step[i](emb) #, self.hidden_stage1[str(i)]\n",
    "            if self.bi:\n",
    "                output_size = output.size()\n",
    "                output = output.view(output_size[0], output_size[1], 2, self.hidden_dim1)\n",
    "            if self.bi:\n",
    "                output_each_step.append(torch.cat((output[:,-1,0,:],output[:,0,1,:]),1))\n",
    "            else:\n",
    "                output_each_step.append(output[:,-1,:])\n",
    "        \n",
    "        #second stage\n",
    "        output1 = torch.stack(output_each_step, 0)\n",
    "        output, _ = self.steps_rnn(output1) #, self.hidden_stage2\n",
    "        logits = self.linear(output[-1,:,:])\n",
    "        #logits = torch.sigmoid(logits)\n",
    "        return logits\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    logits_all = []\n",
    "    labels_all = []\n",
    "    model.eval()\n",
    "    for steps_batch, lengths_batch, labels_batch in loader:\n",
    "        for step_id in range(6):\n",
    "            lengths_batch[step_id] = lengths_batch[step_id].cuda()\n",
    "            steps_batch[step_id] = steps_batch[step_id].cuda() \n",
    "        logits = model(steps_batch, lengths_batch)\n",
    "        logits_all.extend(list(logits.cpu().detach().numpy()))\n",
    "        labels_all.extend(list(labels_batch.numpy()))\n",
    "    logits_all = np.array(logits_all)\n",
    "    labels_all = np.array(labels_all)\n",
    "    auc = roc_auc_score(labels_all, logits_all)\n",
    "    predicts = (logits_all > 0.5).astype(int)\n",
    "    acc = np.mean(predicts==labels_all)\n",
    "    return auc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_cuisine_indian            0.023525  85% auc\n",
    "tag_cuisine_nordic            0.000399\n",
    "tag_cuisine_european          0.012360\n",
    "tag_cuisine_asian             0.182217  98% auc\n",
    "tag_cuisine_mexican           0.013557\n",
    "tag_cuisine_latin-american    0.094896  90% auc\n",
    "tag_cuisine_french            0.077352  72% auc\n",
    "tag_cuisine_italian           0.233254  80% auc\n",
    "tag_cuisine_african           0.003987\n",
    "tag_cuisine_mediterranean     0.076555  88% auc\n",
    "tag_cuisine_american          0.273525  80% auc\n",
    "tag_cuisine_middle-eastern    0.046252  87% auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "logits_all = []\n",
    "labels_all = []\n",
    "model.eval()\n",
    "for steps_batch, lengths_batch, labels_batch in test_loader:\n",
    "    for step_id in range(6):\n",
    "        lengths_batch[step_id] = lengths_batch[step_id].to(device)\n",
    "        steps_batch[step_id] = steps_batch[step_id].to(devi) \n",
    "    logits = model(steps_batch, lengths_batch)\n",
    "    logits_all.extend(list(logits.cpu().detach().numpy()))\n",
    "    labels_all.extend(list(labels_batch.numpy()))\n",
    "logits_all = np.array(logits_all)\n",
    "labels_all = np.array(labels_all)\n",
    "auc = roc_auc_score(labels_all, logits_all)\n",
    "predicts = (logits_all > 0.5).astype(int)\n",
    "acc = np.mean(predicts==labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82168113146898103"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b342ac7c588>]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b342a7a5160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels_all, logits_all, pos_label=1)\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 50])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([90, 50])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([90, 60])\n",
      "torch.Size([90, 30])\n",
      "torch.Size([90])\n",
      "torch.Size([90])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
